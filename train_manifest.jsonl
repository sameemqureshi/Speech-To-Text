{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.1 Biological Neuron.wav", "duration": 377.7, "text": "prof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 one\npartialbrief history of deep learning\nhello everyone welcome to lecture one of csseven fifteen which is the course on deep learning\nin  today\u2019s lecture is going to be a bit non technical we are not going to cover any\ntechnical concepts  or  we only going to talk about a brief or partial history of deep\nlearning\nso we hear the terms artificial neural networks artificial neurons quite often these days\nand i just wanted you take you through the journey of where does all these originate\nfrom and this history contains several spans across several fields not just computer\nscience we will start with biology then talk about something in physics then eventually\ncome to computer science and so on so with that let us start\nrefer slide time zerofiftysix\nso just some acknowledgments and disclaimers i have taken lot of this material from\nthe first people which i have mentioned on the bullet and there might still be some errors\nbecause its dates as back as one thousand eight hundred and seventyone so maybe i have got some of the facts wrong so feel\nfree to contact me if you think some of these portions need to be corrected and it would\nbe good if you could provide me appropriate references for these corrections\ndeep learning\nso let us start with the first chapter which is on biological neurons as i said its spans\nseveral fields will start with biology\nrefer slide time onetwentyfour\nand we will first talk about the brain and neurons within the brain so  way back in\none thousand eight hundred and seventyone one thousand eight hundred and seventythree around that time joseph von gerlach actually proposed that the nervous\nsystem our nervous system is a single continuous network as opposed to a network of\nmany discrete cells\nso his idea was that this is one gigantic cell sitting in our nervous system and it is not a\nnetwork of discrete cells and this theory was known as the reticular theory\nrefer slide time onefiftyfive\nand around the same time there was the some breakthrough or some progress in staining\ntechniques where camillo golgi discovered that a chemical reaction that would allow\nyou to examine the neurons or the nervous tissue\nso he was looking at this nervous tissue using some staining technique and by looking at\nwhat you see in this figure on the right hand side the yellow figure even he concluded\nthat this is just once single cell and not a network of discrete cells so he was again a\nproponent of reticular theory so this is about camillo golgi\nrefer slide time twothirtyfive\n\nand then interestingly santiago cajal he used the same technique which golgi proposed\nand he studied the same tissue and he came up with the conclusion that this is not a\nsingle cell this is actually a collection of various discrete cells which together forms a\nnetwork so it is a network of things as opposed to a single cell there so that is what\nhis theory was and this was eventually came to be known as the neuron doctrine\nrefer slide time threeten\nalthough this was not a consolidated in the form of a doctrine by cajal that was done by\nthis gentleman so he coined the term neuron  so now today when you think about art\nhere  about  artificial  neural  networks  or  artificial  neurons  the  term  neuron  actually\noriginated way back in one thousand eight hundred and ninetyone and this gentleman was responsible for coining that\nand he was also responsible for consolidating the neuron doctrine  so already as you\nsaw on the previous slide cajal had proposed it but then over the years many people\nbought  this  idea  and  this  guy  was  responsible  for  consolidating  that  into  a  neuron\ndoctrine interestingly he is not only responsible for coining the term neuron he is also\nresponsible for coining the term chromosome  so two very important terms were coined\nby this one person\nso now here is a question so around one thousand nine hundred and six when it was time to give the nobel prize in\nmedicine what do you think which of these two proponents say there are two theories\none is reticular theory which is a single cell and then there is this neuron doctrine which\nis a collection of cells or collection of neurons that a nervous system is a collection of\nneurons so what do you think which of these two guys who are proponents of these two\ndifferent theories who would have got the actual nobel prize for medicine\nrefer slide time fourtwentyseven\nso interestingly it was given to both of them  so till one thousand nine hundred and six in fact way later till one thousand nine hundred and fifty\nalso this debate was not completely set settled and then the committee said both of these\nare interesting pieces of work we yet do not know what really actual what the situation\nis actually but these conflicting ideas have a place together and so the nobel prize was\nactually given to both of them and this led to a history of a some kind of controversies\nbetween these two scientists and so on\nrefer slide time fourfiftyeight\nand this debate surprisingly was settled way later in one thousand nine hundred and fifty and not by progress in biology\nas such but by progress in a different field\nso this was with the advent of electron microscopy  so now it was able to see this at a\nmuch better scale and by looking at this under a microscope it was found that actually\nthere is a gap between these neurons and hence it is not a one single cell it is actually a\ncollection or a network of cells with a clear gap between them or some connections\nbetween them which are now known as synapses so this was when the debate was\nsettled\nso now why am i talking about biology why am i telling you about biological neuron\nand so on so this is what we need to understand so there has always been interested in\nunderstanding how the human brain works from a biological perspective at least and\naround this time the debate was more or less settled that we have this our brain is a\ncollection of many neurons and they interact with each other to help us do a lot of\ncomplex processing that we do on a daily basis right from getting up in the morning and\ndeciding what do we want to do today taking decisions performing computations and\nvarious complex things that our brain is capable of doing\nnow the interest is in seeing if we understand how the brain works can we make an\nartificial model for that so can we come up with something which can simulate how our\nbrain works and what is that model and how do we make a computer do that or how do\nwe make a machine do that so that is why i started from biological neurons to take the\ninspiration from biology"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.2 From Spring to Winter of AI.wav", "duration": 783.26, "text": "prof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 one\nchapter two from spring to winter of ai\nwe will start talking about artificial intelligence and this is titled as from the spring to\nthe winter of ai so i am going to talk about when was this boom in ai started or\nwhen is that people started thinking and talking about ai seriously and what eventually\nhappened to the initial boom and so\nrefer slide time zerothirtyfour\nso  let  us start  with  one thousand nine hundred and fortythree  whereas  i  saying  that  there  was  a  lot  of  interest  in\nunderstanding how does a human brain work and then come up with a computational\nor  a  mathematical  model  of  that  so  mcculloch  and  pitts  one  of  them  was  a\nneuroscientist and the other one was a logician no computer scientists or anything at that\npoint of time \nand they came up with this extremely simplified model that just as a brain takes a input\nfrom lot of factors so now suppose you want to decide whether you want to go out for a\nmovie or not so you would probably think about do you really have any exams coming\nup that could be our factor xone you could think about is a weather good to go out is it\ndeep learning\nraining would it be difficult to go out at this point would there be a lot of traffic is it a\nvery popular movie and hence tickets may not be available and so on \nso being kind of presses all this information you might also look at things like the\nreviews of the movie or the imdb rating of the movie and so on and based on all these\ncomplex inputs it applies some function and then takes a decision yes or no that i want\nto probably go for a movie \nso this is an overly simplified model of how the brain works is and what this model\nsays is that you take inputs from various sources and based on that you come up with the\nbinary decision right so this is what they proposed in one thousand nine hundred and fortythree so now we have come to\nan artificial neuron so this is not a biological neuron this is how you would implement\nit as a machine right so that was in one thousand nine hundred and fortythree \nrefer slide time twoone\nthen  along  and  then  this  kind  of led  to a  lot  of  boom  in our  interest  in artificial\nintelligence and so on and i guess around one thousand nine hundred and fiftysix in a conference the term artificial\nintelligence  was  a  formally  coined  and  within  a  one  or  two  years  from  there  frank\nrosenberg  came  up  with  this  perceptron  model  of  doing  computations  or  what\nperceptron model of what an artificial neuron could be\nand we will talk about this in detail later on the course and not tell you what these things\nare as of now just think of the a new model was proposed and this is what he had to say\nabout this model right so he said that the perceptron may eventually be able to learn\nmake decisions and translate languages do you find anything odd about this statement\nyeah so learn and make decisions make sense but why translate languages why is so\nspecific why such a specific interest in languages\nso that you have to connect back to history so this is also the period of the cold war\nand there was always always a lot of interest there was lot of research and translation\nwas actually fuelled by the world war and evens that happened after that  where these\ncountries which were at loggerheads with each other \nthey wanted to understand what the other country is doing but they did not speak each\nother\u2019s language that is why there was a lot of interest from espionage point of view or\nfrom spying and so on to be able to translate languages and hence that specific require\nand lot of this research would have been funded from agencies which are interested in\nthese things right and the defence or war or something \nrefer slide time threethirtysix\nso and this work was largely done for the navy and this is an this is an extract from the\narticle written in new york times way back in one thousand nine hundred and fiftyseven or fiftyeight where it was mentioned that\nthe embryo often this perceptron is an embryo of an electronic computer that the navy\nexpects will be able to walk talk see write reproduce itself and be conscious of it is\nexistence \nso i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand nine hundred and fiftyseven fiftyeight why i am\nthat is why i like the history part of it so recently there is a lot of boom or a lot of hype\naround ai that ai will take over a lot of things will take our jobs it might eventually we\nmight be colonized by ai agents and so on \nso i just want to emphasize that i do not know whether that will happen or not but this\nis not something new we have been talking about the promise of ai as far back since\none thousand nine hundred and fiftyseven one thousand nine hundred and fiftyeight right this not something new that people are talking about now it is always\nbeen there and to what extent this promise will be fulfilled is yet to be seen \nand of course as compared to one thousand nine hundred and fiftysevenfiftyeight we have made a lot of progress in other fields\nwhich have enabled ai to be much more successful than it was earlier for example we\nhave much better compute power now we have lots of data  now  and all thanks to the\ninternet and other things that you can actually crawl tons and tons of data and then try to\nlearn something from a data or try to make the machine learn something from it\nso we have made a lot of progress in other aspects where which ai is now at a position\nwhere it can really make a difference but just wanted to say that these are not things\nwhich i have not been said in the past it has always been the it  has always been\nconsidered to be very promising and perhaps a bit hyped also so that is about one thousand nine hundred and fiftysevenfiftyeight \nrefer slide time fivetwentyfive\n\nthen now what we talk about what is all the for the past eight to ten years at least when\nwe talk about ai talking about deep learning and that is what this course  is about\nlargely about deep learning i am not saying that other and what deep learning is largely\nabout if i want to tell you in a very layman nutshell term is it is about a large number\nof artificial neurons connected to each other in layers and functioning towards achieving\ncertain goal\nso this is like a schematic of what a deep neural network or a feed forward neural\nnetwork would look like now this is again not something new which is up in the last eight\nto ten years although people have started discussing it a lot in the last eight to ten years\nlook at it way back in  one thousand nine hundred and sixtyfivesixtyeight opposed something which looked very much like a\nmodern deep neural network or a modern feed forward neural network \nand in many circles he is considered to be one of the founding fathers of modern deep\nlearning\nrefer slide time sixtwentysix\nso that is about that refer slide time sixthirty right from one thousand nine hundred and fortythree to one thousand nine hundred and sixtyeight it was mainly\nabout the springtime  for ai and what i mean  by that  that everyone was showing\ninterest in that the government was funding a lot of research in ai and people really\nthought that ai could deliver a lot of things on a lot of fronts refer slide time sixfortysix for\nvarious applications health care defence and so on \nand then around one thousand nine hundred and sixtynine an interesting paper came out by these two gentlemen minsky\nand papert which essentially outlined some limitations of the perceptron model and we\nwill talk about these limitations later on in the course in the second or third lecture but\nfor now i will not get into a details of that but what it is said that it is possible that a\nperceptron cannot handle some very simple functions also \nso you are trying to make the perceptron learn some very complex functions because\nthe way we decide how to watch a movie is a very complex function of the inputs that\nwe considered but even a simple function like xor or is something which a perceptron\ncannot be used to model that is what this paper essentially showed and this led to severe\ncriticism for ai and then people started losing interest in ai and lot of government\nfunding actually subsided after one thousand nine hundred and sixtynine \nrefer slide time seventhirtyeight\nall the way to one thousand nine hundred and eightysix actually this was the ai winter of connectionism so there was very\nlittle interest in connectionist ai so there are two types of ai one is symbolic ai and the\nother is connectionist ai so whatever we are going to study in this course about neural\nnetworks and all that probably falls in connectionist ai paradigm and there was no\ninterest in this and people i mean hard to get funding and so on for these seventeen to eighteen years \nrefer slide time eighttwo\nand that was largely triggered by this study that was done by minsky and papert and\ninterestingly they were also often misquoted and what they had actually said in that\npapers so they had said a single perceptron cannot do it they in fact said that a multi\nlayer network of perceptrons can do it  but  no one focused on the second part that a\nmultilayer network of perceptron people started  pushing the idea that a perceptron\ncannot do it and hence we should not be investigating it and so on right so that is\nwhat happened for a long time and this known as the winter the first winter \nrefer slide time eightthirtytwo\n\nthen around one thousand nine hundred and eightysix actually came this algorithm which is known as back propagation\nagain this is an algorithm which we are going to cover in a lot of detail in the course in\nthe fourth or fiveth lecture and this algorithm actually enables to train a deep neural network\nright so deep network of neurons is something that you can train using this algorithm \nnow this algorithm was actually popularized by at rumelhart and others in one thousand nine hundred and eightysix but it\nis not completely discovered by them this was also around in various other fields so it\nwas there  in  i think in systems analysis or something like that it was being used for\nother purposes in a different context and so on and rumelhart other and others in one thousand nine hundred and eightysix\nwere the first to kind of popularize it in the context of deep neural networks \nand this was a very important discovery because even today all the neural network so\nmost of them are trained using back propagation right and of course there have been\nseveral other advances but the core remains the same that you use back propagation to\ntrain a deep neural network right so something this was discovered almost thirty years\nback is still primarily used for training deep neural networks that is why this was a very\nimportant paper or breakthrough at that time \n refer slide time ninefiftyeight\nand  around  the  same  time  so  again  interestingly so  back  propagation  is  used  in\nconjunction with something known as gradient descent which was again discovered\nway back in one thousand eight hundred and fortyseven by cauchy and he was interested in using this to compute the orbit of\nheavenly bodies\nthat is something that people care about at that time today of course we use it for\nvarious  other  purposes  one  of  them  being  discovering  cats  and  videos  or  even  for\nmedical imaging or for describing whether certain have of cancer is being depicted in a\nxray or things like that there is a lot of other purposes for which deep neural networks\nenhance and hence back propagation gradient descent and other things are being used\nfor it but again these are not very modern discoveries these are dated way back thirty years\nand even gradient descent is almost  one hundred and fifty years and so on so that is  what i wanted to\nemphasize \nrefer slide time tenfortythree\nand around the same time in one thousand nine hundred and ninety or one thousand nine hundred and eightynine there is this another interesting theorem\nwhich was proved which is known as the universal approximation theorem and this is\nagain something that we will cover in the course in the third lecture or something like\nwhere we will talk about the power of a deep neural network\nso again the importance of this or why this theorem was important will become clear\nlater and when we cover it in detail but for now it is important to understand that what\nthis theorem said is that if you have a deep neural network you could basically model all\ntypes of functions continuous functions to any desired precision \nso what it means in very layman terms is that if the way you make decisions using a\nbunch of inputs is a very very complex function of the input then you can have a neural\nnetwork which will be able to learn this function right in many laymen terms that is\nwhat it means \nand if i have to hype it up a bit or i have to say it in a very enthused and excited manner\ni would say that basically it says that deed neural networks can be used for solving all\nkinds of machine learning problems and that is roughly what it says but with a pinch of\nsalt and a lot of caveats but that is what it means at least in the context of this course\nso  this  is  all  around  one thousand nine hundred and eightynine  and  despite  this  happening  some  important  discoveries\ntowards  the  late  end  of  eighty\u2019s  which  was  back  propagation  universal  approximation\ntheorem people were still not being able to use deep neural networks for really solving\nlarge practical problems and a few challenges there was of course the compute power at\nthat time was not at a level where it could support deep neural networks \nwe do not have enough data for training deep neural networks and also in terms of\ntechniques while back propagation is a sound technique it is to fail when you have\nreally deep neural network so when people try it training a very deep neural network\nthey found that the training does not really converge the system does not really learn\nanything and so on and there were certain issues with using back propagation off the\nshelf at that time because of which it was not very successful\nso again despite these slight boom around eightysix to ninety where some important discoveries\nwere made and even follow up in ninetytwo ninetythree and so on there is still  not a real big hype\naround deep neural networks or artificial neural networks and at time again a slump a\nslow winter right up till two thousand and six"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.3 The Deep Revival.wav", "duration": 437.02, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture  one\nchapter three the deep revival\nwhen this deep revival happened so in two thousand and six a very important study was or a very\nimportant contribution was made by hinton and salakhutdinov \nrefer slide time zerofifteen\n\nsorry if i have not pronounced it properly and they found that a method for training very\ndeep neural network effectively now again the details of these are not important  we\nwill be doing that in the course at some point but what is the important take away here is\nthat while from one thousand nine hundred and eightynine to two thousand and six we knew that there is an algorithm for training deep neural\nnetworks and they can potentially be used for solving a wide range of problems because\nthat  is what the  universal approximation  theorem said  but the problem was that  in\npractice we were not being able to use it for much \nit was not easy to train these networks but now with this technique there was revived\ninterest  and  hope that  now  actually  can  train  very deep  neural  networks for  lot  of\npractical problems this sparked off the interest again and then people started looking at\nall such of thing right that even this particular study which was done in two thousand and six  will\nactually be very simple to something done way back in ninetyoneninetythree and which again showed\nthat you can train a very deep neural network but again due to several factors may be at\nthat time due to the computational requirements or the data requirements or whatever i\nam not too sure about that it did not become so popular then but by two thousand and six probably the\nstage was much better for these kind of networks or techniques to succeed so then it\nbecame popular in two thousand and six \nrefer slide time onefortyone\n\nthen  this  two thousand and six  to  two thousand and nine  people  started  gaining  more  and  more  insights  into  the\neffectiveness of this discovery made by  hinton and others which is unsupervised pre\ntraining  right  that  is  what  i  spoke  about  on  the  previous  slide  unsupervised  pre\ntraining \nand they started getting more and more insights into how you can make deep neural\nnetworks really work so they came up with various techniques some of which we are\ngoing to study in this course so this was about how do you initialise the network better\nwhat  is  the  better  optimization  algorithm  to  use  what  is  the  better  regularization\nalgorithm to use and so on so there were many things which were started coming out at\nthis period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep\nneural networks of artificial neural networks started becoming popular \nthat is when people realised that all this all the negative things that were tied to it that\nyou are not able to train it well and so on have slowly  people have started finding\nsolutions to get by those and maybe we should start again focusing on the potential of\ndeep neural networks and see if they can be used for large scale practical application  so\nthis two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a\nlot of work to popularize  deep neural  networks and get rid of some of the problems\nwhich existed in training them \nrefer slide time twofiftyeight\nnow from  two thousand and nine onwards there  was this series of success is which kind of caught\neveryone which made everyone to stand up and take notice  right  that this is really\nworking for a lot of practical applications starting with handwriting recognition  so\naround two thousand and nine these guys won handwriting recognition competition in arabic and they did\nway better than the competitor systems using a deep neural network and then this was a\nsuccess\nrefer slide time threetwentythree\n\nso this was an handwriting recognition and then there was speech so this shown that\nvarious  existing  systems  the  error  rate  of  these  system  could  be  seriously  be\nsignificantly  reduced  by  using  deep  neural  networks  or  plugging  in  a  deep  neural\nnetwork component to existing systems right so this was handwriting and then speech\nrefer slide time threefortyfive\n\nthen again some kind of pattern recognition which was on handwritten digit recognition\nfor mnist this is a very popular data set which had been around since ninetyeight and a new\nrecord was set on this data so this is the highest accuracy that was achieved on this data\nset around that time in two thousand and ten sorry and this is also the time when gpus entered the same\nso before that all of the stuff was being done on cpus  but  then people realised that\nvery deep neural networks require a lot of computation and lot of this computation can\nhappen very quickly on gpus as opposed to cpus \nso people started using gpus for training and that drastically reduced the training and\ninference  time  so  that  was  again  something  which  sparked  a  lot  of  interest  right\nbecause even though these were successful they were taking a lot of time to train but\nnow the gpus could even take care of that and this success continued \nrefer slide time fourthirtyeight\n\nso  people  started  gaining  or  getting  success  in  other  fields  like  visual  pattern\nrecognition so this was a competition on recognising traffic sign boards and here again\na deep neural network did way better than its other competitors \nrefer slide time fourfiftythree\n\nand then the most popular or one thing which made neural networks really popular was\nthis image net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when\nthis  alexnet  was  one  of  the  participating  systems  in  this  competition  most  of  the\nsystems were non neural network based systems and this competition  was basically\nabout classifying a given image into one of thousand classes\nso this could be an image of a bird or a dog or a human or car truck and so on say you\nhave to identify the right class of the main object in the image  so in two thousand and twelve this alexnet\nwhich was a deep neural network or a convolutional neural network based system was\nable to actually outperform all the other systems by a margin of sixtyseven percent so the error\nfor this system was sixteen percent and this is a deep neural network because it had eight layers \nthe next year this was improved further and something known as zf network propose\nwhich was again eight layers but it did better than alexnet  the next year even a deeper\nnetwork with nineteen layers was proposed which did significantly better than alexnet then\ngoogle entered the scene and they proposed something which is twentytwo layers and again\nreduced the error then microsoft joined in and they proposed something which had one hundred and fiftytwo\nlayers and the error that you see here is actually better than what humans do \nso even if a human was asked to label this image because of certain law certain noise in\nthe image and so on even a human is bound to make more errors than threesix per cent  that\nmeans even if you show hundred images to humans he or she is bound to may go wrong\nor more than three or four of these images right there is this system was able to get an\nerror of threesix per cent over the large test set \nso this two thousand and twelve to two thousand and sixteen period were there was this continuous success on the image net\nchallenge  as  well  as  successes  in  other  fields  like  natural  language  processing\nhandwriting recognition speech and so on so this is the period where now everyone\nstarted talking about deep learning and lot of company started investing in it  a lot of\ntraditional systems which were not deep neural network based was now started people\nstarted converting them to deep neural network based system \nso translation system speed systems image classification object detection and so on\nthere  were  lot  of  success  in  all  these  fields  using  deep  neural  networks  and  this\nparticular thing that we are talking about which is image net and the success in this was\ndriven by something known as convolutional neural networks"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.4 From Cats to Convolutional Neural Networks.wav", "duration": 171.8, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 one\nchapter four from cats to convolutional neural networks\ni will talk about the history of convolutional neural networks and i call this part of\nhistory as cats and it will become obvious why i call it so\nrefer slide time zerotwentyone\nso around one thousand nine hundred and fiftynine hubel and wiesel did this famous experiment they are still i think you\ncould see some videos of it on youtube where there is this cat and there was a screen in\nfront of it and on the screen there were these lines being displayed at different locations\nand in different orientations so  slanted horizontal vertical and so on and there are\nsome electrodes fitted to the cat and they were measuring trying to measure that which\nparts of brain actually respond to different visual stimuli\nlet us say if you show it stimulus at a certain location does the different part of the brain\nfire and so on so and one of the things of outcomes of the study was that that different\nneurons in brain fire to only different types of stimuli it is not that all neurons in brain\nalways fire to any kind of visual stimuli that you give to them \nrefer slide time oneeighteen\nso this is essentially roughly the idea behind convolutional neural networks starting\nfrom something known as neocognitron which was proposed way back in one thousand nine hundred and eighty you\ncould think of it as a very primitive convolutional neural network i am sure that most of\nyou have now read about or heard about convolutional neural networks  but something\nvery similar to it was proposed way back in one thousand nine hundred and eighty\nrefer slide time onethirtysix\nand what we know as the modern convolutional neural networks maybe i think yan li\nkun is someone who proposed them way back in one thousand nine hundred and eightynine and he was interested in using\nthem for the task of handwritten digit recognition and this was again in the context of\npostal delivery services so lot of pin codes get written or phone numbers get written on\nthe postcards and there was a requirement to read them automatically  so that they can\nbe the letters or postcards can be separated into different categories according to the\npostcard according to the postal code and so on right so or the pin code\nso that is where this interest was there and one thousand nine hundred and eightynine was when this convolutional neural\nnetworks were first proposed or used for this task\nrefer slide time twonineteen\nand then over the years several improvements were done to that and in one thousand nine hundred and ninetyeight this now\nhow  famous  data  set  the  mnist  data  set  which  is  used  for  teaching  deep  neural\nnetworks courses or even for initial experiments with various neural network based\nnetworks this is one of the popular data sets which is used in this field and this was\nagain released way back in one thousand nine hundred and ninetyeight and even today even for my course i use it for various\nassignments and so on\nrefer slide time twofortynine\nso it is interesting that an algorithm which was inspired by an experiment on cats is\ntoday used to detect cats in videos of course among other various other things is just i\nam just jokingly saying this"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.5 Faster, higher, stronger.wav", "duration": 127.52, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 one\nchapter five faster higher stronger\nso this is what the progression was right that in two thousand and six people started or the study by\nhinton and others led to the survival and then people started realizing the deep neural\nnetworks and actually we use for lot of practical applications and actually beat a lot of\nexisting systems\nbut there are still some problems and we still need to make the system more robust\nfaster and even scale higher accuracies and so on\nrefer slide time zerofortyone\nso in parallelly while there was lot of success happening from two thousand and twelve to  two thousand and sixteen  or even\ntwo thousand and ten to two thousand and sixteen in parallel there will also a lot of research to find better optimization\nalgorithms which could lead to better convergence better accuracies\nand again some of the older ideas which were proposed way back in one thousand nine hundred and eightythree now this is\nagain something that we will do in the course  so most of the things that i am talking\nabout we are going to cover in the course so we are going to talk about the imagenet\nchallenge we are going to talk about all those networks the winning networks that i had\nlisted there alex net zf net google net and so on\nwe are going to talk about nesterov gradient descent which is listed on the slide and\nmany other better optimization methods which were proposed starting from two thousand and eleven so\nthere was this parallel resource happening while people were getting a lot of success\nusing traditional neural networks they are also interested in making them better and\nrobust and lead for lead to faster convergence and better accuracies and so on\nso this led to a lot of interest in coming up with better optimization algorithms and\nthere was a series of these proposed starting from two thousand and eleven  so adagrad is again something\nthat we will do in the course rms prop adam eve and many more so many new\nalgorithms i have been proposed and in parallel a lot of other regularization techniques\nor  weight  initialization  strategies  have  also  been  proposed  for  example  batch\nnormalization or xavier initialization and so on  so  these are all things which were\naimed at making neural networks perform even better or faster and even reach better\nsolutions or better accuracies and so on this all that we are going to see in the course at\nsome point or the other"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.6 The Curious Case of Sequences.wav", "duration": 355.24, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nlecture  one\nchapter six the curious case of sequences\nso i  was talking about successes in image speech pattern recognition even natural\nlanguage processing and so on so one interesting thing here is about sequences so i\nwill talk about sequences now \nrefer slide time zerotwentyseven\nsequences are everywhere when you are dealing with data  so  you have time series\nwhich is like say the stock market trends or any other kind of a series time series then\nyou have speech which is again a series of phonemes or you have music you have text\nwhich is a series of words you could even have videos which are the series of images\nright one frame each image each frame can be considered to be an image and so on\nso in speech data one peculiar characteristic of speech data is that every unit in the\nsequence interacts with other units so words on their own may not mean much  but\nwhen you put them together into a sentence they all interact with each other and give\nmeaning to the sentence right and the same can be said about music or speech or any\nkind of sequence data so all these elements of the sequence actually interact with each\nother \nso there was a need for models to capture this interaction and this is very important for\nnatural  language  processing  because  in  natural  language  processing  you  deal  with\nsequence of words or all your texts or sentences or documents or all sequences of words\nso that is very important and the same in the case of speech also \nso if you take up any deep learning paper nowadays it is very likely that you will come\nacross the term recurrent neural network or lstms which are long short term memory\ncells and so on\nrefer slide time onefortyseven\n\nso this is also something which was proposed way back in one thousand nine hundred and eightysix\nrefer slide time onefortynine\n\nso a recurrent neural network is something which allows you to capture the interactions\nbetween the elements of your sequence i had said at a very layman level but of course\nyou are going to see this in much more detail in the course and this was also not\nsomething new even though you hear about it a lot in the past  three  to four years the first\nrecurrent neural network and what you see here is exactly a very similar to what we are\ngoing to cover in the course was proposed way back in jordan by jordan in one thousand nine hundred and eightysix \nrefer slide time twotwentythree\n\nits variant was proposed by elmen in one thousand nine hundred and ninetyso this is again not a very new idea this\nhas existed for some time but now there are various factors because of which it has been\npossible to now start using them for a lot of practical applications  as i said one you\nhave a lot of compute time and the other you have a lot of data and the third is now the\ntraining has stabilized a lot because of these advances which i was talking about in terms\nof better optimization algorithms better regularization better weight initialization and so\non \nso it has become very easy to train these networks for real world problems at a large\nscale so that is why they have become very popular and hear about them on a regular\nbasis but it is again something which was done way back \nrefer slide time threefour\nso from one thousand nine hundred and ninetynine to one thousand nine hundred and ninetyfour actually people also looking at various problems will be training\nneural networks and recurrent neural networks and so that this problem which is known\nas exploding and the vanishing gradient problem which is again something that we will\nsee in the course in reasonable detail  we have this problem and it is very difficult to\ntrain  recurrent  neural  networks  for  longer  sequences  so  if  you  have  a  very  long\nsequence or a time series you cannot really train a recurrent neural network to learn\nsomething from that \nrefer slide time threethirtyfour\n\nand to overcome these problems around one thousand nine hundred and ninetyseven  long short term memory cells were\nproposed and this is again something that we will cover in the course and this is now\nalmost de facto standard used for training for a lot of nlp work lstm are used as one\nof  the  building  blocks  and  another  variants  of  lstms  which  are  known  as  gated\nrecurrent units and some other variants \nso  this  is  also  not  something  new  even  though  they  have  become  very  popular\nnowadays like almost any article that you pick about to talk about any article on deep\nlearning that pick about to talk about recurrent neural networks or lstms or gated\nrecurrent units this is not something which is new \nrefer slide time fourtwentythree\n\nlstms had come way back in one thousand nine hundred and ninetyseven  but again due to various compute and other issues\nwhich i said at that time it is not  so  easy to use them  but  by two thousand and fourteen  because of these\nparallel progresses which i mentioned in terms of optimization regularization and so on\npeople are now able to use rnns lstms for large scale sequence to sequence problems\nand in particular a very important discovery at this time are very important model which\nwas proposed at this time which is attention mechanism which is used in a lot of deep\nneural  networks nowadays which enabled  to  deal with  a lot  of sequence prediction\nproblems \nfor example translation where you have given one sequence in one language and you\nwant to generate the equivalent sequence in another language so this is known as a\nsequence to sequence translation problem  so  for that people proposed a sequence to\nsequence attention network and this was one of the key discoveries which then led to a\nlot of adaptation of or  adoption of deep neural networks for nlp \na lot of research in nlp happened which was then driven by deep neutral networks so\na lot of existing algorithms which are non neural network based algorithms which are\ntraditionally used for nlp was slowly replaced by these deep neural network based\nalgorithms ok\nrefer slide time fivethirtythree\nand  again  this  idea  of  attention  itself  is  something  that  was  explored  earlier  also\nsomewhere around one thousand nine hundred and ninetyone or so and it was something known as reinforcement learning\nwhich was used for learning this attention mechanism what attention basically tells you\nis that if you have a large sequence and if you want to do something with this sequence\nwhat are the important entities of this sequence or elements of this sequence that you\nneed to focus on so this is again something that we will look at in detail in the course"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.7 Beating humans at their own games (literally).wav", "duration": 83.0, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nlecture  one\nbeating humans at their own game literally\nnow since i mentioned rl so we will go on to the next chapter which was now\nbecoming much more ambitious with what you can do with deep learning and people\nstarted beating humans at their own game quite literally\nrefer slide time zerotwentysix\nso there was this starting with atari games in two thousand and fifteen where resources from deep mind\nshow that you could train a deep neural network to play atari games and do much better\nthan what humans do  so that is something that they were able to show on atari games\nand then people started looking at other game\nrefer slide time zerofortysix\nso then there was this go and this popular tournament and which alphago which is\ndeep reinforcement learning based agent was actually able to beat the reigning champion\nat that time one of the best players of go at that time\nrefer slide time onesix\nthen  even  at  poker  were  something  known  as  deepstack  which  is  again  a  deep\nreinforcement learning based agent which is able to beat eleven professional poker players at\nthis game\nrefer slide time oneseventeen\n\nthen other games like defense of the ancients since on which is a much more complex\nstrategy based game where again deep reinforcement learning based agents have shown a\nlot of success in beating top professional players on this game"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.8 The Madness (2013-).wav", "duration": 254.48, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 one\nchapter eight the madness two thousand and thirteen \nso this was all happening where deep learning now started showing a lot of promise in a  \nlot of fields nlp vision speech and again this deep reinforcement learning and so on  \nwhich led to this complete madness starting from two thousand and thirteen\nrefer slide time zerotwentyeight\nwell almost for every application the traditional methods were then overwritten or kind\nof beaten by deep neural network based system so something like language modelling\nwhich has been around since probably one thousand nine hundred and fiftys or so\n now  the  reining  algorithm  or  the  better  algorithm  for language  modelling  is now\nsomething which is based on deep neural networks\nrefer slide time zerofifty\nthen similarly for speech recognition lot of work a lot of probabilistic lot of work\nbased on probabilistic models was done in this or in the speech area or the speech\nliterature for the past thirty forty years and now all of that has been overcome by deep neural\nnetwork based solutions\nrefer slide time oneeight\nsame for machine translation  a lot of interest in this field  a lot of companies now have\ntheir  machine  translation  systems  based on deep neural networks as opposed to the\nearlier phrase based statistical machine translations or the probabilistic models which\nwere used earlier\nrefer slide time onetwentythree\nsimilarly for conversation modelling dialogue a lot of new work started in dialogue post\na deep learning era where people now realize that if you have a lot of sequences of\nconversations you could actually try to train a deep neural network to learn from this\nsequence and have conversations with humans of course you are nowhere close to\nhuman level conversations we are very very far off from them but in limited domains\nthese bots are showing some success now\nrefer slide time onefifty\nsame for question answering where you are given a question and you want to answer it\neither from a knowledge graph or from a document or from a image and so on\nrefer slide time onefiftyseven\nand in the field of computer vision things like object detection most of the reigning\nsystems  or  the  best  performing  systems  nowadays  are  deep  neural  network  based\nsystems a lot of advances are being made on these systems over in the last few years\nrefer slide time twoseventeen\nsame for visual tracking where you want to track the same person in a video or image\ncaptioning where you want to generate captions for images  for example people upload\na lot of images on facebook\nrefer slide time twotwentyone\nand if you want to automatically caption them or imagine you are on a reselling site\nright something like olx where you upload your furniture  and you do not provide a\ndescription from that  but can the machine already automatically generate a description\nfor it so it is easier for the human to read what that product is and so on\nrefer slide time twofortyfive\nso similarly video captioning i given a video anyone to caption the main activity which\nis happening in that video all of these problems are being solved using deep learning\nbased  solutions  using  a  combination  of  something  known as  feed  forward  neural\nnetworks or convolutional neural networks or recurrent neural networks and so on\nrefer slide time threethree\nvisual question answering you are given an image and a question and you want to\nanswer that question\nrefer slide time threeeight\nvideo question answering answering questions from videos\nrefer slide time threeeleven\nvideo summarizations if you are given a large video and you want to generate a trailer a\nsort of a trailer for that video contains which kind is the most important frame for that\nvideo even these systems are based on deep learning\nrefer slide time threetwentytwo\nthen this was all about classification recognition and so on  but  now people started\ngetting more ambitious that can we humans are very good at creativity  so can we use\nmachines  to be creative  right to generate images  so now  if i have seen a lot of\ncelebrity faces  can i generate new celebrity faces or if i have seen a lot of bedroom\nimages\nand i am if a fireman architect  now can i generate new bedroom images can i can we\ntrain a machine to generate new bed bedroom images so a lot of phenomenal progress\nor work has happened in this field in the last four five years starting with things like\ngenerative adversarial networks variational autoencoders and so on\nand  people  are  now  starting  to  seriously  invest  into  creativity  that  how  to  make\nmachines creative again we are far off from where the desired output but there is still\nsignificant progress happening in this field generating audio\nrefer slide time fourfifteen\nso that was about generating images you can generate music also\nrefer slide time fourtwentyone\nand this is again about generating images and so on"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 1.9 (Need for) Sanity.wav", "duration": 245.16, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture  one\nchapter nine need for sanity\nso lot of fields have adopted deep learning now and lot of state of the art ai systems are  \nbased on deep neural  networks but now what i s needed is after all thi s madness were \ndeep learning has taken over a lot of research areas can we now bring in some sanity to \nthe proceeding so this is really a need for sanity\nrefer slide time zerothirtytwo\nand why i say that is that because there is this paradox of deep learning  so there is this\ninteresting question that why does deep learning works so well despite having a high\ncapacity\nso the deep neural networks have a very high capacity which means that susceptible to\nover fitting so most of you would have done some course on machine learning  so\nthere you know that over fitting is bad because you are just memorizing the training data\nand then you might not be able to do so well and at tested and over fitting happens when\nyour  model  has  a  high  capacity so  even  though  deep  neural  networks  have  high\ncapacity why are they doing so well  we will focus on this high capacity but when we\ntalk about the universal approximation theorem and give some arguments for why deep\nneural networks have such a high capacity\nthe other thing is they have this numerical instability right so we spoke about these\nvanishing and exploding gradients  and again we will talk about  this later on in the\ncourse so despite this training difficulties why is it that deep neural networks performs\nso well and of course they have this sharp minima which is again it could lead to over\nfitting so if you look at there is an  optimization problem it is not a  neat convex\noptimization problem so it is a non convex optimization problem so why does it still\ndo so well\nso it is also not very robust so here is an example on the right hand side the figure that\nyou show so the first figure is actually of a panda and the machine is able to detect this\npanda with some fiftyseven percent confidence  right we have trained a machine for a lot of\nanimal images we have shown it a lot of animal images at test time we show at this\nimage the first image that you see on the right hand side and is able to classify this is a\npanda with fiftyseven percent confidence but now what i do is i add some very random noise\nso that second image that you see with some very random pixels if i add it to this image\ni will get a new image\nso every pixel in this image is added to this new noise image and i get the image which\nis see on the third the third image that you see right to you and me or to any average\nhuman this still looks like a panda  there is hardly any difference between this image\nand the original image but now if you pass this to the machine all of a sudden instead of\nrecognizing this is a panda it starts to recognize it as a gibbon and that too with ninetynine\npercent confidence so why is it that they are not very robust and despite this not being\nvery robust why are deep neural networks  so  successful so  people are interested in\nthese questions and people have started asking these questions\nthere are no clear answers yet but slowly and steadily there is an increasing emphasis\non explainability and theoretical justifications  so it is not enough to say that your deep\nneural network works and gives you ninetynine percent accuracy  it is  also good to have an\nexplanation for why that happens is it that some components of the networks are really\nable to discriminate between certain patterns and so on so what is going on inside the\nnetwork which is actually making it work so  well right and hopefully this will bring in\nsome sanity to the proceedings\nso instead of just saying that i apply deep learning to problem x and got  ninety percent\nsuccess we will also make some kind of more sane arguments just to why this works and\nwhat is the further promise of this and thinks like that so  that is roughly a  quick\nhistorical recap of where deep learning started and where it is today starting all the way\nback from advances in biology in one thousand eight hundred and seventyone to recent advances till two thousand and seventeen and so on deep\nlearning right and here are few url\nrefer slide time threefiftyfive\nso you could take a look at this for a lot of interesting applications of recurrent neural\nnetworks\nrefer slide time fourzero\n\nbunch of startups which have come up in this space is working on very varied and\ninteresting problems and here are all the references that i have used for this particular\npresentation\nrefer slide time foursix\nso that is where we end lecture one and i will see you again soon for lecture two"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.1 Motivation from Biological Neurons.wav", "duration": 411.64, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nlecture \u2013 two\nmcculloch pitts neuron threshold logic perceptrons perceptron learning\nalgorithm and convergence multilayer perceptrons mlps representation\npower of mlps\nso welcome to lecture two of cs seven thousand and fifteen which is the course on deep learning  so we will\ntalk about mcculloch pitts neuron thresholding logic perceptrons and a learning\nalgorithm for perceptrons and talk about the convergence of this algorithm and then we\nwill talk about multilayer network of perceptrons and finally the representation power\nof perceptrons\nso  let us start module one which is on biological neurons  so  remember during the\nhistory we had started all the way back  in the one thousand eight hundred and eightys when we spoke about biological\nneurons so  we will just start there spend a few minutes on it and then go on to the\ncomputational models which is mcculloch pitts neuron\nrefer slide time zerofortynine\nso now this is a course on deep learning  so we are going to talk about deep neural\nnetworks now the most fundamental unit of a deep neural network is something known\nas an artificial neuron\nand the question is why is it called a neuron  where does the inspiration come from so\nwe already know that the inspiration comes from biology and more specifically it comes\nfrom the brain because we saw that way back in the one thousand eight hundred and ninetys this term neuron was coined\nfor neural processing units or the cells in our brain\nso now before we move on to the computational neurons or the artificial neurons  we\nwill just see the biological neurons in a bit more detail and then we will move on from\nthere\nrefer slide time onetwentynine\nso this is what a typical biological neuron looks like  so  here actually there are two\nneurons this portion is called the dendrite  so it is used to receive inputs from all the\nother neurons\nso that is the place where the input comes in  then remember we said that in one thousand nine hundred and fiftys we\ndiscovered that these neurons are actually discrete cells and there is something which\nconnects them so that connection is called a synapse and it decides the strength of the\nconnection between these two neurons so there is an input there is some strength to the\nconnection\nthen once this neuron receives inputs from various other neurons it starts processing it\nso that is the central processing unit which is called the soma  and once it is done this\nprocessing it will it is ready to send its output to other set of neurons  so that output is\ncarried on by the axon  so we have inputs we have some weights attached to the input\nwe have some processing and then an output  so that is what a typical biological neuron\nlooks like\nrefer slide time twothirtyone\nand let us see a very cartoonish illustration of how this works right how the neuron\nworks so our sense organs interact with the outside world and then they pass on this\ninformation to the neuron and then the neuron decides whether i need to take some\naction in this case the action could be whether i it should laugh or not right whether the\ninput is really funny enough to evoke laughter  so  if that happens this is known as\nsomething that the neuron has fired\nrefer slide time twofiftyseven\nnow of course in reality it is not just a single neuron which does all this  there is a\nmassively parallel interconnected network of neurons so you see a massive network\nhere now the neurons in the lower level site so these neurons they actually interact\nwith the sensory organs they do some processing based on the inputs  so  they decide\nwhether i should fire or not\nand if they fire they transmit this information to the next set of neurons  and this process\ncontinues till the information is relayed all the way back and then finally you decide\nwhether you need to take any action or not again in which this case it should be laughter\nso  that is how it works and when i say massively parallel interconnected network i\nreally mean it because there are ten raise to eleven which is roughly one hundred billion neurons\nin the brain\nrefer slide time threefortyfour\nnow this massively parallel network also ensures that there is some division of work\nnow what do you mean by that is not that every neuron is responsible for taking care of\nwhether i should laugh or not or not every neuron is responsible for processing visual\ndata some neurons may possess visual data some neurons may possess speeds data and\nso on so there is this division of work every neuron has a certain role to play so for\nexample in this cartoonish example that we took\nrefer slide time fourseven\n\nso there might be this one neuron which fires if the visuals are funny right whatever you\nare seeing is funny  there will be one neuron which finds sheldons speech to be funny\nthe way he speaks  so  that might be funny and there might be another neuron which\nactually  finds  the  dialogue  content  to  be  funny and  now  all  of  this  pass  on  the\ninformation to the next level and this guy would fire if at least two of these three inputs are\nfunny so that means i have some threshold based on which i decide whether to react or\nnot if it is really funny then only i laugh it otherwise i will not laugh\nrefer slide time fourfortytwo\nso the neurons in the brain as was obvious in the previous slide are arranged in a\nhierarchy and i will take a more realistic example where we look at the visual cortex\nso  is  this  is  the  portion  of  the  brain  which  is  responsible  for  processing  visual\ninformation right so as you see here you have our retina from where the information\nstarts flowing and it goes through various levels\nso you see you follow the arrows and you will see there are several levels there is one\nlevel here then another here another here and so on right  so it is again as i was trying\nto illustrate in that cartoon the information is relayed through multiple layers and then it\ngoes all the way back to the spinal cord which decides that in this case i need to move\nthe muscle right\nso  that  is  what  is  being  decided  here  right  so  the  information  flows  through  a\nhierarchy of layers and in this particular case i am going to focus on these three circled\nlayers which are vone vtwo and ait right so these actually form a hierarchy and let us see\nwhat this hierarchy does right\nso at layer one you detect edges and corners so i am looking at you all i just see some\ndots and some shapes so that is what layer one recognizes i just recognize some edges and\nsome dots and so on\nrefer slide time fiveforty\nnow layer two tries to group all of these together and come up with some meaningful\nfeature groups right so it realizes oh these two edges actually form the nose these two\ndots actually form the eyes and these two edges actually form the mouth right so that is\nslightly higher level of processing that it is doing and then layer three further collects all this\nand leads to higher level objects right\nso now it is realizing all these things put together is actually a human face right  so you\nadd edges and circles or dots  then you had some feature groups and then the feature\ngroups combine into objects right so that is how this hierarchy processes\nrefer slide time sixthirty\nso here is a disclaimer i understand very little about how the human brain works right\nand what you saw is a very oversimplified explanation of how the brain works right\nwhat i told you is there is an input a layer of networks which does a network which has\nmany layers which does some processing and then you have an output right that is the\nvery simplistic view that i gave you  this is an oversimplified version but this version\nsuffices for everything that we need for this course right this is not a biology or a neural\nprocessing course right so it is enough for this course  so that is where we will end\nmodule one"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.2 McCulloch Pitts Neuron, Thresholding Logic.wav", "duration": 778.7, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nmodule twotwo\nlecture  two\nmcculloch pitts neuron\nlet us start with module two which is about mcculloch pitts neuron \nrefer slide time zeroseventeen\n\nso as we are done this during the history lecture way back in one thousand nine hundred and fortythree mcculloch and pitts\nthey proposed highly simplified computational model of the brain so now let us see\nwhat\u2019s the motivation we know that our brain is capable of very complex processing\nit\u2019s capable of taking a lot of inputs from various sources and then help us taking various\ndecisions and actions now what if you want a computer to do this  we want a module\nwhich is very similar to how the brain works or at least how we think the brain works\nwhich takes a lot of inputs and then does some processing and helps us take a decision \nso what they proposed is this model which will take a lot of inputs and these inputs are\nall binary all these inputs that you see here these inputs are fed to this mcculloch pitts\nneuron  which is an artificial neuron and it is divided into two parts  so  the first part\ncollects all the input so remember you had these dendrites which were taking all the\ninformation  from everywhere  so  this just collects all the information and then the\nsecond part is aggregation  i  have collected a lot of information from all the sources\nnow the second function will decide what this aggregation is and based on that it will\ntake a decision whether to fire or not \nso the output is again boolean if it\u2019s zero then neuron does not fire if it\u2019s one the neuron\nfires  so  let us take a concrete example so suppose  i  am trying to make a decision\nwhether i should watch a movie or not so xone could be is the genre of the movie thriller\nsimilarly there could be another variable say xn which says is the actor matt damon\nso these are all various such factors that i could take is the director christopher nolan\nthe music given by someone and so on so all these are probably factors which help me\ndecide whether i want to watch this movie or not and you want this neuron to help us\nmake that decision\nrefer slide time twotwentyone\n\nso now what is happening here is these all inputs they can be either excitatory or\ninhibitory so let me tell you what inhibitory is first so you are taking input from a lot\nof sources now see one of these sources or one of these inputs is am i ill today  am i\ndown  with  fever  so  if  that  input  is  on  irrespective  of  who  the  actor director  or\nwhatever is i am not going to watch the movie right because i just cannot leave from my\nbed so these are known as inhibitory inputs irrespective of what else is on in your input\nfeatures if this input is on your output is always going to be zero that means the neuron\nis never going to fire so you could think of it as suppose my mood is not good today i\ndo not feel like getting up or  if i injured my leg or anything  right if  any of these\nconditions is on irrespective of what the other factors are i am not going to watch the\nmovie \nso that is an inhibitory input and excitatory input are on the other hand is not something\nwhich will cause the neuron to fire on its own but it combine with all the other inputs\nthat you have seen could cause the neuron to fire and how so this is how so these are\nall the inputs that your neuron is taking all i am going to do is i am going to take a sum\nof these \ni am going to take aggregation of all of these so what does this count actually give me\nthe number of inputs which are on the number of inputs which are value one that is all\nthis aggregate this is a sum of all the ones in my input\nnow this is what g does this is a very simple function is taking a sum of my inputs\nnow the function y takes this as the input that means it takes this sum as the input and\nif the sum is greater than a certain threshold then it fires  if the sum is less than the\ncertain threshold then it does not fire so again see what is happening here is it is same\nas now if you depend on the actor director and genre and so on and you fine at least\ntwo of these three conditions are satisfied at least i am happy with the actor and the\ndirector even though the genre is not something that i care about \ni will watch the movie or you might be a very niche go movie watcher who only goes to\na movie if the actor matches your requirement the director matches your requirement\nand the genre and the music and everything matches your requirement  so  you are\nthreshold in that case it should be high so this is how it is going to help you make\ndecisions now again a very simplified model and this is theta is called the thresholding\nparameter that is the value which decides whether the neuron is going to fire or not and\nthis over all thing is known as the thresholding logic  so this is what a mcculloch pitts\nneuron looks like \nrefer slide time fourfiftytwo\nnow let us implement some boolean functions using this mp neuron so from now on i\nwill just called it mp neuron and we will try to implement some boolean functions using\nit so now  why are we interested in boolean functions it is because we have overly\nsimplified the way we take decisions we are saying that the way we take decisions is we\ntake a lot of boolean inputs is actor matt damon and genre thriller and so on and based\non that we produce a boolean output \nso an input is all booleans so we have xone to xn which are all booleans and your output\nis also boolean so that is a boolean function that you are trying to learn from x to y is\nthat clear you have x just happens to contain n different variables here ok and lot of\ndecision problems you could cast in this framework you can just imagine right whether\nto come for lecture  today or not  again is you could cast in it depending on various\nboolean inputs \nrefer slide time fivefortythree\nthis is a very concise representation of the mcculloch pitts neuron what it says is it\ntakes a few boolean inputs and it has certain threshold if the sum of these inputs crosses\nthis  threshold the n the neuron will fire otherwise it will not fire  t hat is the simple\nrepresentation of the m p neuron now suppose i am trying to learn the and function\nwhen would the and function fire\nall the inputs are on so what should be the value of the threshold in this case\nthree everyone agrees what about the or function\none let us see a few more this function so let me tell you what this function is so\nyou see this circle here so that means that this input is an inhibitory input if that is on\nthen the neuron is not going to fire that is how i am representing it so now tell me\nwhat should the threshold for this be it is not so hard \nsee if xtwo is on it is not going to fire so you have four rows zero zero zero one one zero one one so two of\nthose are ruled out and it is not going to fire now out of the remaining two when do\nyou wanted to fire\nso what should be the threshold\none  now what about this function  zero or three three  is not even a valid option  zero  everyone\nagrees to that and what about this zero so you get this  so now if you have a certain\nnumber of input variables and the function that you are trying to model the decision that\nyou are trying to make is a boolean function then you could represent using these mp\nneurons whether all boolean functions can be represented in this way or not that is still\nnot clear i am just showed you some good examples we will come to the bad examples\nlater on here is the question \nrefer slide time seventhirtyseven\nso can any boolean function be represented using a mcculloch pitts neuron so before\nanswering this question we will see a bit of a geometric interpretation of what mp\nneuron is actually trying to do\nrefer slide time sevenfortynine\nso  let us take or function where you have two inputs xone and xtwo and this neuron is\ngoing to fire\n if xone plus xtwo is greater than equal to one that is clear that is how the definition is  now\nif you look at this xone plus xtwo greater than equal to one  now let us ignore the greater than\npart first so we will just talk about xone plus xtwo equal to one what is this equation of a\nline everyone gets that ok n ow in this case since we are dealing with boolean inputs\nand we have two access xone and xtwo how many input points can we have  four right zero zero zero  one\none zero one one \nrefer slide time eightthirtysix\nso you could have these four points so just note that this is an xone and xtwo axis but only\nfour inputs are valid here this is not a real numbered access this is only boolean inputs\npossible here\nnow what is the line xone plus xtwo equal to one tell you which line is that \nso one which passes through one zero here and zero one here this is that line now what do we\nwant that for all those inputs for which the output is actually one they should lie on the line\nor on the positive side on the line and all those inputs for which the output is zero they\nshould lie on the other side of the line is that happening so what is actually mp in unit\nactually learning linear decision boundary it just what it is doing in effect is actually it is\ndividing the input points into two halves such that all the points lying on that line right\nare sorry all the points for which the input should be zero lie below this line and all the\npoints for which the output should be one sorry in both cases it should have been output \nso let me just repeat it all the points for which the output is zero lie below this line and all\nthe points for which the output is one either lie on this line or above the line is that fine\nand so let us convince  ourselves about  this  even it is not already clear from the\nequation for how many of you it is already cleared from the equation that this is exactly\nwhat it does for a large number of periods but still we will just do a few examples and\nmove ahead \nrefer slide time tenfifteen\n\nnow for the and function what is the decision boundary it is xone plus xtwo no that is\nthe decision boundary\nequal to two so again i have these four points only these four points are possible now\nwhere is my decision line\npassing through that one one and intercepting this somewhere around two zero and this around zero\ntwo so that is the line which i am interested in now again do you see that our condition\nis satisfied that all the inputs for which we want the output to be one are on or above the\nline and all the inputs for which we want the output to be zero or below the line now what\nabout this function what is the threshold\nzero so what would the line be xone plus xtwo equal to zero which passes through the origin\nright and again all the points are either on or above the line so this part we are going to\ncall as a positive half space and this we are going to call as the negative half space \nrefer slide time eleventwentyfour\n\nnow what if we have more than two inputs  in a two dimensional case when we just\nhad xone and xtwo we are trying to find a separating line in the three dimension case what\nwill we do\nplane in the higher dimensions\nhyper plane so this is now your three dimensional case again there are three axis here\nbut not all points are possible how many points are possible eight points and which is the\nfunction that we are trying to implement \nor  so for these eight out of these eight points for how many is the output one\nseven and for one it is zero so what is the kind of plane that we are looking at w e are looking\nfor a plane such that seven points lie on or above it and one point lies below it and which is that\npoint \nzero so now what is the equation of that hyper plane x one plus x two plus x three is equal to one\nyou see this so you see that all the seven points are visible but the points zero zero is not\nvisible because it is on the other side of the plane so this is doable in three dimensions\nalso and again in higher dimensions also right we could find in hyper plane \nrefer slide time twelvethirtyeight\nso the story so far is that single mcculloch pitts neuron can be used to represent\nboolean functions which are linearly separable so a linearly separable function is such\nthat there exists a line such that for that function whichever points produce an output of one\nlie on one side of the line and whichever points produce an output zero lie on the other side\nof the line"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.3 Perceptrons.wav", "duration": 619.12, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nmodule twothree\nlecture  two\nperceptron\nnow let us go to the next module which is perceptron \nrefer slide time zeroeighteen\n\nso far the story has been about boolean input but are all problems that we deal with we\nare only dealing with do we always only deal with boolean inputs so yeah so what\nwe spoke about is boolean functions now consider this example this worked fine for a\nmovie example where we had these as actor so much and his director and so on but now\nconsider the example where you are trying to decide you are in oil mining company and\nyou are trying to decide whether you should mine or drill at a particular station or not \nnow this could depend on various factors like what is the pressure on the surface on the\nocean surface at that point what is the salinity of the water at that point what is the\naquatic marina  aquatic  life at that point and so on so these are not really boolean\nfunction the salinity is a real number density would be a real number pressure would\nbe a real number and so on right and this is a very valid decision problem companies\nwould be interested in doing this so in such cases our inputs are going to be real but so\nfar mcculloch pitts neuron only deals with boolean inputs so we still need to take care\nof that limitation \nnow how did we decide the threshold in all these cases i just asked you you computed\nit and you told me right but that is not going to work out i mean it does not scale to\nlarger problems where you have many more dimensions and the inputs are not boolean\nand so on so we need a way of learning this threshold \nnow again returning to the movie example maybe for me the actor is the only thing that\nmatters and all the other inputs are not so important then what do i need actually i\nneed some way of weighing these inputs i should be able to say that this input is more\nimportant than the others now i am treating all of them equal i am just taking a simple\nsum \nif that sum causes a threshold i am fine otherwise i am not fine  but maybe i want to\nraise the weight for some of these inputs or lower the weight for some of these inputs\nso whether it is raining outside or not maybe does not matter i have a car i could go or\ni could wear a jacket or an umbrella or something so that input is probably not so\nimportant\nand what about functions which are not linearly separable  we have just been dealing\nwith the goody stuff which is all linearly separable  but  we will see that even in the\nrestricted boolean case there could be some functions which are not linearly separable\nand if that is the case how do we deal with it so these are some questions that we need\nto answer\nrefer slide time twothirtyfive\n\nso first we will start with perceptron which tries to fix some of these things and then we\nwill move forward from there so as we had discussed in the history lecture that this was\nproposed in one thousand nine hundred and fiftyeight by frank rosenblatt and this is what the perceptron looks like do you\nsee  any  difference  with  the  mcculloch  pitts  neuron  weights  you  have  a  weight\nassociated with each of the input otherwise everything seems \nso this is a more general computational model than the mcculloch pitts neuron  the\nother interesting thing is that of course we have introduced these weights and you also\nhave a mechanism for learning these weights so remember in the earlier case our only\nparameter  was  theta  which  we  are  kind  of  hand  setting  right  but  now  with  the\nperceptron we will have a learning algorithm which will not just help us learn theta but\nalso these weights for the inputs \nhow do i know that actor is what matters or director is what matters given a lot of past\nviewing experience past given a lot of data about the movies which i have watched in\nthe  past how  do i know  which  are the  weights to assign this  so we will  see an\nalgorithm which will help us do that and the inputs are no longer limited to be boolean\nvalues they can be real values also so that is the classical perceptron but what i am\ntalking about here and the rest of the lecture is the refined version which was proposed\nby  minsky  and  papert  which  is  known  as  the  perceptron  model  so  when  i  say\nperceptron i am referring to this model so this diagram also corresponds to that \nrefer slide time foursix\nso now let us see what the perceptron does  this is how it operates  it will give an\noutput of one if the weighted sum of the inputs is greater than a threshold so remember\nthat in the mp neuron we did not have these weights but now we have these weighted\nsum of the inputs and the output is going to be zero  if this weighted sum is less than\nthreshold not very different from the mp neuron \nnow i am just going to do some trickery and try to get it to a better notation or a better\nform so is this i have just taken the theta on this side now is this notice this here the\nindices were one to n now i have made it zero to n and the theta is suddenly disappeared so\nwhat has happened \nstudent w zero is \nminus theta right and xzero is one does anyone not get this right if i just start it from one to n\nthen it would be summation i equal to one to n wi xi plus wzero xzero but i am just saying wzero is\nequal to minus theta and xzero is equal to one which exactly gives me back this right so\nvery simple xzero equal to one and wzero is equal to minus theta  so  in effect what i am\nassuming is that instead of having this threshold as a separate quantity i just think that\nthat is one of my inputs which is always on and the weight of that input is minus theta\nso now the job of all these other inputs and their weights is to make sure that their sum\nis greater than this input which we have does not make sense so this is how this is the\nmore accepted convention for writing the perceptron equation so it fires when this\nsummation is greater than equal to zero otherwise it does not fire\nrefer slide time sixseven\n\nnow let me ask a few questions so why are we trying to implement boolean functions\ni have already answered this but i will keep repeating this question so that it really gets\ndrill in  why do we need weights  again we briefly touched upon that and why is w\nnaught which is negative of theta often called the bias\nrefer slide time sixtwentyfive\n\nso again let us return back to the task of predicting whether you would like to watch a\nmovie or not and suppose we base our decisions on three simple inputs  actor genre and\ndirector  now based on our past viewing experience we may give a high weight to\nnolan as compared to the other inputs so what does that mean it means that as long as\nthe director is christopher nolan i am going to watch this movie irrespective of who the\nactor is or what the genre of the movie so that is exactly what we want and that is the\nreason why we want these weights \nrefer slide time sixfiftyeight\nnow wzero is often called the bias as it represents the prior so now  let me ask a very\nsimple question suppose you are a movie buff what would theta be zero i mean you\nwill watch any movie irrespective of who the actor director and genre now suppose\nyou are a very niche movie watcher who only watches those movies which are which the\ngenre is thriller the director was christopher nolan and the actor was damon then what\nwould your threshold be three\nhigh in this case  i always ask this question do you know of any such movie always\ntakes a while interstellar so the weights and the bias will depend on the data which in\nthis case is the viewer history so that is the whole setup that is why you want these\nweights and that is why you want these biases and that is why we want to learn them\nrefer slide time sevenfortyeight\nnow before we see whether or how we can learn these weights and biases one question\nthat we need to ask is what kind of functions can be implemented using the perceptron\nand are these function any different from the mcculloch pitts neuron so before i go to\nthe next slide any guesses  i am hearing some interesting answers which are at least\npartly correct \nrefer slide time eighteleven\n\nso this is what a mcculloch pitts neuron looks like and this is what a perceptron looks\nlike the only difference is this red part which is weights which has added so it is again\nclear that what the perceptron also does  is it  divides the input space into two halves\nwhere all the points for which the output has to be one would lie on one side of this\nplane and all the points where which the output should be zero would lie on the other side of\nthis plane so it is not doing anything different from what the perceptron was doing so\nthen what is the difference\nyou have these weights and you have a mechanism for learning these weights as well as\na threshold we are not going to hand code them so we will first revisit some boolean\nfunctions and then see the perceptron learning algorithm\nrefer slide time eightfiftyfive\n\nso now let us see what does the first condition this condition if i actually expand it out\nthen this is what it turns out to be and what is that condition telling me actually w naught\nshould be less than zero clear so now based on these what do you have here actually\nwhat is this a system of linear inequalities right and you know you could solve this\nyou have algorithms for solving this not always but you could find some solution and\none possible solution which i have given you here is wzero is equal to minus one wone equal to\noneone and wtwo equal to oneone \nso just let us just draw that line so what is the line it is oneone xone plus oneone xtwo is equal to\none that is the line and this is the line and you see it satisfies the conditions that i have is\nthis the only solution possible no right i could have this also as a valid line if i could\ndraw properly right all of these are valid solutions so which result in different wone w\nnaught and w zeros so all of these are possible solutions \nin  fact  i  have  been  telling  you that  you had  to  set the  threshold  by hand  for the\nmcculloch pitts neuron  but  that is not true because you could have written similar\nequations there and then decided what the value of theta should be so you could try\nthis out for the mcculloch pitts neuron also you will get a similar set of conditions or i\nmean similar set of inequalities and you can just say what is the value of theta that you\ncould set to solve that"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.4 Error and Error Surfaces.wav", "duration": 220.6, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nmodule twofour\nlecture  two\nerrors and error surfaces\nbefore we go to the next section which is on learning  i just want to introduce the\nconcept  of errors and error surfaces  and tell  you what  it  relates  to these multiple\nsolutions that we were talking about \nrefer slide time zerotwentysix\nso for simplicity what we will do is we will just set the threshold to minus or minus w\nto one which is setting the threshold to minus one and now i will try different values of wone\nand wtwo ok so i was saying that there are multiple values of wone and wtwo possible and\nthese are all real numbers we are not constrained by having them as boolean values so\nnow this is one solution which i tried i tried setting wone to minus one and wtwo to minus one\nwhat is wrong with this line does it lead to any errors how many\njust one error so this makes an error of one out of the four inputs now let me just try some\nother values of wone and wtwo this line again one error what about this line  n ot four three\nbecause zero zero is anyways on this side of a line\nso now given this now tell me that i my quest is to find these w so i would want to\nfind wone wtwo and so on given this discussion on errors can you tell me a condition that i\nam looking for i want to find w one wtwo or up to wn such that errors are minimized and in\nthe best case errors are zero so that is what i want so this just i want to make a case that\nthese search for w\u2019s is driven by certain objective and this objective is to minimize the\nerror\nso now since we are doing this let us plot the error surface corresponding to different\nvalues of w naught wone and wtwo \nrefer slide time twothree\nonce again for simpler analysis we will just keep w naught to be fixed at minus one and\nnow what i have so just do not read this bullet as of now even this one so i have this\nwtwo here so that is my one axis and i have wone here which is my another axis now what\ni am going to do is i am going to try different values of wone and wtwo so this axis can go\nfrom minus infinity to plus infinity of course for showing the sake of showing here i\nhave just had it from minus four to four \nso now what i am going to do is i am searching for some values of w\u2019s wone and wtwo so\nthat my errors is zero and let us do a brute force and i will just try every value between\nminus four to four ok in fact one of the solutions which i proposed actually was this oneone oneone\nright that is the line which we saw on the previous slide and which led to zero errors and\nthat is the dark blue surface here  so  how did i compute this error actually i just\nsubstituted minus sorry oneone oneone here and then i put in all the four values combinations\nfor xone x two and i realized that i am able to satisfy all of them so i do not get any error\nnow instead of that if i had put something different\nso let me just go back to the previous slide which was see minus one minus one  which is i\nthink yeah somewhere around here right minus one minus one i guess so for that i am in\nthis light blue region where the error was one i make errors for one of the inputs so it\nis a very brute force way of finding this and this is not going to work because we have\nlots of inputs to check  but  this is just to give you an intuition that we are looking at\nerrors and we are trying to find a value of wone wtwo which minimize this error so that is\nthe idea behind errors and error surfaces"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.5 Perceptron Learning Algorithm.wav", "duration": 774.62, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras\nmodule  two five\nlecture  two\nperceptron learning algorithm\nwe will now go to the next module which is the perceptron learning algorithm \nrefer slide time zeroseventeen\nwe now see a more principled approach of learning these weights and threshold  but\nbefore that we will just again revisit our movie example and make it slightly more\ncomplicated \nrefer slide time zerotwentyfive\nnow here  what  the  situation  is  that  we  are  given  a  list  of  m  movies  and  a  class\nassociated with each movie indicating whether we like the movie or not so now we\nhave given some data of the past m movies that we have seen and whether we like this\nmovie or not and now instead of these three variables we have these n different variables\nbased on which we are making decisions and notice that some of these variables are\nreal they are not boolean anymore the rating could be any real number between zero to one\nok and now based on this data what do we want is the perceptron to do actually \nso i have given you some data these factors i have also given you the label one and zero\nso if the perceptron if i tell you my perceptron has now learnt properly  what would you\nexpected it to do perfect match so whenever i feed it one of these movies it should\ngive me the same label as was there in my data and again there are some movies for\nwhich i have a label one which are positive and some movies which i have a label zero \nso i am once again looking to separate the positives from the negatives so it should\nadjust the weights in such a way that i should be able to separate so that is the learning\nproblem that we are interested in \nrefer slide time onethirtyone\nso  now  with  that  i  will  give  you  the  algorithm  this  is  the  perceptron  learning\nalgorithm  we have certain positive inputs which had the label  one we have certain\nnegative inputs which had the label zero and now i don\u2019t know what the weights are and i\nhave no prior knowledge of what the weights are going to be i need to learn them from\nthe data  so  what i am going to do is i am just going to initialize  these weights\nrandomly as i am also going to pick up some random values for this so this should be\nsmall n so this should be small n and now here is the algorithm while not convergence\ndo something \nso before i tell you what to do  can you tell me what is meant by convergence when\nwill you say that it has converged when it is not making any more errors on the training\ndata right  or its  predictions  are  not  changing  on  the  training  data  so  that  is  the\ndefinition of convergence now here is the algorithm  i pick up a random from point\nfrom my data which could either be positive or negative so it comes from the union of\npositive negative basically all the data that i have i pick up a random point from there \nif the point is positive right and this is the condition which happens what does this tell\nme if the point was positive what did i actually want greater than zero but the condition is\nless than zero that means i have made an error so i have made an error  then i will just\nadd x to w i see a lot of thoughtful nodding and i hope you are understanding what is\nhappening let us see so what is w actually a dimensional \nn dimension n plus one right because w naught is also inside there  so  actually there\nshould be w naught also here  right and what is x again n dimensional right and that is\nwhy this addition is valid  so  let us understand that w and x both are n dimensional\nnow let us look at the other if condition can you guess what the other if condition is if\nx belongs to n and \nsummation  is  greater  than  equal  to  zero  then so  that  means you  have  completely\nunderstood how this algorithm works well that is so now consider two vectors w and\nx so remember what we are trying to prove is or get an intuition not prove actually\nget an intuition for why this works ok\nrefer slide time threefiftyfive\nso we will consider two vectors w and x and this is what my vectors look like very\nsimilar to the case that we are considering wzero to wn and one to n so this again x naught is\njust one\nnow this condition that i have been talking about is nothing but the dot product how\nmany of you have gone through the prerequisites for todays lecture  ok good so it is\njust a dot product now we can just read write the perceptron rule as this instead of the\ndot product i mean instead of using that summation thing we can just say that it is a dot\nproduct \nnow we are interested in finding the line w transpose x equal to zero so that is our\ndecision boundary which divides the input into two halves now every point on this line\nsatisfies the equation w transpose x equal to zero what does that mean actually\nso just a simple example is that if i have the line xone plus xtwo equal to zero  then all the\npoints which lie on the line satisfy this equation so you could have one minus one two minus\ntwo and so on  but two two  is cannot be a point on this line at every point lying on this line\nsatisfies this equation so every point lying on this line actually satisfies the equation w\ntranspose x equal to zero \nrefer slide time fivefourteen\nso can you tell me what is the angle between w and any point on this line  how many\nsay how many of you say perpendicular why\ndot product is zero so if the dot product is zero they are orthogonal so that means if i take\nthis line then my vector w is orthogonal to this it is orthogonal to this point or this point\nto this point to every point on the line which is just the same as saying that the vector is\nperpendicular to the line itself right as simple as that so the angle is ninety degrees because\nthe dot product gives you the cos alpha and that is zero right and since it is perpendicular as\ni said to every point of the line it is just perpendicular to the line itself \nrefer slide time fivefiftyeight\nso this is what the geometric interpretation looks like this is our decision boundary w\ntranspose x and the vector w is actually orthogonal to this line and that is exactly the\nintuition that we have built so far\nnow let us consider some points which are supposed to lie in the positive half space of\nthis line that means these are the points for which the output is actually one now can you\ntell me what is the angle between any of these points and w or you guys are actually\ntrying to tell me the angle we have got some measuring stuff  no so i will give you\nthree options i e equal to ninety greater than ninety and less than ninety \nless than ninety it is obvious from the figure now if i take any point which lies in the\nnegative half space  what is the angle going to be between them it is greater than ninety\nagain obvious and it also follows from the fact that cos alpha is w transpose x by\nsomething and we know that for the positive points w transpose x is greater than equal to\nzero that means cos alpha would be greater than equal to zero  that means the angle alpha\nwould be less than ninety degrees and for the negative points w transpose x is actually less\nthan zero that means cos alpha would be less than zero that  means alpha would be greater\nthan ninety degrees \nso it actually follows from the formula itself  but  it is also clear from the figure  so\nkeeping this picture in mind let us revisit the algorithm so this is the algorithm \nrefer slide time seventhirtytwo\nnow let us look at the first condition which was this now if x belongs to p and w\ntranspose x is less than zero  then means that the angle between x and the current w is\nactually greater than ninety degrees  but what do we want it to be less than ninety degrees and\nour solution to do this is  but we still do not know why this works now anyone knows\nwhy this works so let us see why this works \nso  what is the new cos alpha going to be it is going to be proportional to this it is\ngoing to be proportional to this i will just substitute what w new is fine t hat means if\ncos alpha new is going to be greater than cos alpha  what is alpha new going to be it\nwill be less than and that is exactly what we wanted this angle was actually greater than\nninety degrees so you want to slowly move it such that it becomes less than ninety degrees it\nis not going to get solved in one iteration and that is why till convergence \nso  we will  keep  doing  this  i will  keep  picking  xs again  and  again  till  it  reaches\nconvergence that means till we are satisfied with that condition \nrefer slide time eightfortyseven\nlet us look at the other condition x belongs to n and w transpose x was greater than\nequal to zero then it means that the angle alpha is actually less than ninety degrees and we want\nit to be the opposite i  will  just quickly skim over this w minus this  x ok i  forgot to\nmention that this is actually a positive quantity i mean that is why that result holds that\nmeans  cos  alpha  new  is  going  to  be  less  than  cos  alpha  and  this  slight  bit  of\nmathematical in correctness i am doing here but that does not affect the final result \nso i will just gloss over that and you can go home and figure it out  but still it does not\ntake away from the final intuition and interpretation so now the new cos alpha is going\nto be less than the original cos alpha that means the angle is going to be greater and that\nexactly what we wanted \nrefer slide time nineforty\nso we will now see this algorithm in action for a toy data set \nrefer slide time ninefortyfour\nso this is the toy data set we have and we have initialized w to a random value and that\nturns out to be this i just picked up some random value for w and ended up with this\nparticular configuration for w \nnow we observe that currently w transpose x is less than zero for all the positive points and\nit is actually greater than equal to zero for all the negative points if you do not understand\nw transpose x it is just that the all the positive angle points actually have a greater than\nninety degree angle and all the negative points actually have a less than ninety degree angle  so\nthis is exactly opposite of the situation that we want and now from here on  we want to\nactually run the perceptron algorithm right and try to fix this w how does it work\nremember we randomly pick a point so say we pick the point pone  do we need to apply\na correction\nyes why because it is a positive point and the condition is violated so now we add w\nequal to w plus x and we get this new w  so notice that we have a new w we again\nrepeat this we again pick a new point and this time we have picked ptwo do we need a\ncorrection\nyes at least from the figure it looks like the angle is greater than ninety so we will again\ndo a correction we will add w is equal to w plus p this x is actually sorry ptwo and this is\nwhere we end up \nrefer slide time eleventwelve\nnow again we pick a point randomly none do we need a correction so this is what our\nw is this line here and none so we need a correction now what is the correction going\nto be it will be minus and then the w changes \nrefer slide time eleventhirtytwo\nnow we pick another point nthree  do we need a correction no at least on the figure it\nseems like the angle is greater than ninety and we continue this \nrefer slide time eleventhirtynine\nfor ntwo we do not need a correction now for pthree again we do not need a correction \nrefer slide time elevenfortythree\nthe angle looks less than ninety sorry actually it is we need a correction the angle is\nslightly greater than ninety and this is our correction and now we keep cycling \nrefer slide time elevenfiftytwo\nnow as i keep cycling over the points i realize that i no longer need any correction \nrefer slide time elevenfiftyfour\nit should be obvious from the figure that for this particular value of w now all my\npositive points are making an angle less than ninety  and all my negative points are actually\nmaking  an angle  greater than ninety t hat means  by definition  now my algorithm  has\nconverged so i can just stop it  so i  can just make one pass over the data if nothing\nchanges i will just say it has converged now does anyone see a problem with this\nit will never converge in some cases  so can someone tell me why  we are considering\nonly cases where the data is linearly separable that we already assumed so what you\nare trying to tell me is that you are going over these points cyclically  so  let me just\nrephrase and put words in your mouth that what you are trying to tell me actually is that i\ntake a point i adjust w  but  now for the next point i maybe go back to the same w\nbecause that point asked me to move it again and i keep doing this again and again and\nbasically end up nowhere that is why this will never converge that is exactly what you\nare trying to tell me \nnow  that is exactly what i am  forcing you to tell me  so  that is not the case this\nalgorithm will converge"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.6 Proof of Convergence of Perceptron Learning Algorithm.wav", "duration": 897.16, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nmodule \u2013 twosix\nlecture \u2013 two\nproof of convergence\nin this module we will talk about the proof of convergence for the perceptron or the\nlearning algorithm that we saw in the previous module\nrefer slide time zerotwentyone\nso we have some faith and intuition that it actually works  we just need to formally\nprove it that it actually converges so that is what we are going to do in this module\nrefer slide time zerothirtyone\nso before that a very few very simple definitions  so if you have two sets of points p\nand n in an n dimensional space and we call say that these points are absolutely linearly\nseparable if there exists some n plus one real numbers which has wzero to wn such that every\npoint which belongs to p right p is the case where the output is one\nthen these set of weights satisfy this condition and every point which lies in the negative\nset the set of weights satisfy this condition  so nothing very different from what has we\nhave been saying so far it is just formally defining it\nnow our proposition is that if the set p and n are finite and there is a fixed number of\npoints in that which was the case in the toy example that we were doing and which will\nbe the case in most examples that we do and linearly separable  the perceptron learning\nalgorithm updates the weight vector ok before i go there ok let me not give you the\ndefinition and let me ask you the definition\nso now i have given this definition  the first definition  and given this part of the\nproposition can you tell me what do i need to prove if i need to prove that the algorithm\nconverges that is one way of looking at it  but  what was happening in that wrong\nargument which was i was making that it continuously kept toggling  that means i am\nnot making a finite number of updates right i have to keep changing again and again and\nthis process continues in a loop\nso that is how i am going to define convergence that the perceptron learning algorithm\nupdates a weight vector of finite number of times it only needs to update it finite number\nof times and it will reach a configuration such that now it is able to separate the p from\nthe n ok that is what the proof of convergence means\nso in other words if you are going to pick up these vectors randomly from the set p and\nn cyclically as we were doing in the toy example  then a weight vector wt is found after\na finite number of steps which will separate these two steps  these two sets  so that is\nwhat we are trying to prove so that is the definition of converge does it make sense \nrefer slide time threetwo\nso proof is on the next slide and it is going to take me around five to ten minutes to prove\nit so just stay focused all right so here is a few set up right  so i am going to before i\ngo to the actual proof i am going to make a set up so that it becomes easier for us to\nprove it so the first thing that i am going to say is that if there is a point which belongs\nin negative set then the negative of that point belongs in the positive set and that is very\nclear because if the point belongs in the negative set then w transpose x is less than zero\nbut then w transpose minus x would be greater than equal to zero right  so i  take the\nnegative of the point i can just put it in the positive set  so instead of considering these\ntwo different things p and n i am just going to consider one p prime  which is an union\nof p and all the n points negative ok will the set up clear  if this is a setup then what is\nthe condition that i need to ensure for every point in p dash\nstudent refer time threefiftyseven\nw transpose p should be greater than equal to zero right so i do not care about the\nnegative case i have just made everything positive now and it is i am not done anything\nwrong here it is just a simple trick ok  and now this is how the algorithm will look in\nthis setup  these are the inputs with label one inputs with label zero n  minus  contains a\nnegation of all the points in n and p prime is a union of these  now again i start by\ninitializing w randomly while convergence i will do something  i will pick a random p\nfrom p prime now what is the if condition\nless than zero\ndo i need the other if condition\nno right because everything is now positive ok and the other small thing that i am\ngoing to do is i am going to normalize p ok  so that again does not mean because we\nare talking in terms of angles and i am not changing the direction of the vector  i am just\nshrinking it right so i am just or maybe scaling it also i am just making it unit norm\nso that does not change anything so it is still everything still holds\nand in particular you can see here so if this condition was true this condition will also\nbe true ok so so far just i am done some simple tricks to make things easier for me\nlater  on so  now p has been  normalized now  remember  that  this  data  is  linearly\nseparable that is what we started the proposition  if p and n are linearly separable then\nthe  perceptron  learning  algorithm  will  converge  so  now  if  p and  n  are  linearly\nseparable irrespective of whether we have the perceptron learning algorithm or not what\ndo we  know\nthere exists a w star which is the solution vector right there exists at least one w star\nwhich is the solution vector right such that it will separate the p points from the n\npoints so this vector which we do not  know but we just  know that it exists  so you\ncan refer to it so we will call this w star fine now we start the proof\nrefer slide time fivefiftysix\n \nso w star is some optimal solution which we  know exists\nbut we do not  know what it is right  now suppose you had a time step t  so remember\nthat this algorithm is going on while convergence  so you have time step one two three you are\npicking up points so we are at a time step t at which you pick up a random point pi and\nyou find that the condition is actually violated so this should actually be less than zero if i\nknow the condition is violated so now what will you have to do\nw is equal to\nwone so i will just call it the new w wt plus one is equal to the old w plus pi ok  now what\ni am going to do is i am going to consider the angle beta between w star and wt plus one  i\ndo not know what w star is but we can still assume it exists and make some calculations\nbased on that so what is the angle between w star and wt plus one  its beta and what is the\ncost of that angle this \nand remember that we do not have w star here because we had assumed that it is the\nnormalized vector so we do not need that but this is actually equal to one ok so now if i\njust take the numerator w star in dot product wt plus one  now i am going to expand wt as\nwt plus pi fair that is exactly what i did on the previous step\nnow now what is pi actually it is so what you had is you had these pone ptwo pthree  my\nhand writing is really horrible and up to pn right so i have just picked one of these pi\u2019s\nok now what i am going to define is now suppose this is my these are my pi\u2019s so\nthese are all the vectors that i have now suppose i have this w star suppose this was the\nw star that i am interested\nnow for each of these i could compute w star pone w star ptwo and so on up to w star pn\nand i could sort them  now what i am doing is that for whichever of these points w star\npi is the minimum  i  am going to call that value as delta suppose w star p one is the\nsmallest quantity out of w star pone w star ptwo w star pn and i am calling that quantity\ndelta\nso i have this quantity here and my delta is the minimum of all the possible values that\nit can take it can make w star pone ptwo up to pn  so delta is the minimum quantity so here\ni have an equality \nrefer slide time eightfortyfive\nnow are you ok with this  this is the minimum quantity right so any pi that i put in\nhere it is always going to be greater than or worst case equal to delta\nnow again this wtwo itself i could write it as wt minus one plus pj  because that also would\nhave come up from some update in the previous step ok  again this is there which i\ncould call it as delta and still retain the greater than equal to here ok fine so let us see\nwhere are we heading with this\nnow notice that we do not make a correction at every time step when i was running that\ntoy algorithm i was not making a correction at every time step  we were only making a\ncorrection at those time steps for which the condition was violated  so now if i am at\nt\u2019th time step maybe i have made only k which is less than or equal to t corrections at\nmax i would have made t corrections but it could have been less than that also\nso now every time we make a correction we are adding a value delta to this so at the\ntime step t what would happen  i  had started off from w naught i have reached time\nsafety and i have made a case that  i have not made t updates i have made k less than\nequal to t updates so how many deltas would get added\nk delta so i could say that with respect to w naught where i had started from this is\nwhat this quantity is ok is that fine anyone has a problem with this\nrefer slide time tennineteen\nso far what are we shown we started with this  this condition was true again not less\nthan equal to and hence we made the correction and this was the point that we picked up\nat the t th step and thence we made that correction\nand we also showed that the numerator is actually greater than equal to this quantity we\nshowed it by induction fine  now let us look at the denominator and particularly let us\nlook at the denominator squared ok is a step right\nthis is actually wt plus one dot product wt plus one but  wt plus one can be written as wt plus p\ni this bracket needs to disappear right is that ok fine now what is  what is this\nquantity\nthat is less than equal to zero so now can you guess what is the next thing that i am going\nto write \nthat is correct  yeah it is a negative quantity  so that is going to be less than equal to\nthis so that is fine and what about pi square or this term \nbecause this is less than right that is why\ncorrect is this fine ok now what is pi square\none now can you guess what i am going to do by induction\nso what is wt square again just this wt plus one square was wt square plus one  wt square is\ngoing to be wt minus one square plus one right and how many such ones will get added k of\nthose right starting from w naught ok\nrefer slide time twelvethirtysix\nso what have we shown the numerator is greater than equal to this  the denominator is\nless than this ok  now if i put them together i actually get that cos beta is going to be\ngreater than equal to the numerator over the denominator ok  now what is this quantity\nproportional to k k square k cube square root of k k by two\nstudent square root of k\nsquare root of k right you have i mean roughly speaking you have a k here  you have a\nsquare root of k here  so i could roughly speaking say that it is proportional to square\nroot of k so as k grows what will happen to cos beta  it will grow and that is fine right\nit can keep growing\nstudent refer time thirteenthirtyone\nonly until one right so cos beta is going to be proportional to k  what is k the number\nof updates that you make  now if i were to take that degenerate case which you guys\nwere hinting at where that it will keep changing again and again  what will happen to k\nit will keep going to infinity can that happen\nno because cos beta will blow up right and that is not allowed  so k has to be finite so\nthat cos beta stays within its limits right hence are we done  how many if you think we\nare done how many if you are satisfy that we are done\nrefer slide time fourteentwentynine\nso yeah so this says that we can only have a finite number of such k updates that we\nmake and after that the algorithm will converge so we have a proof of convergence\nnow coming back to our questions this is where we had started at one point  what about\nnon boolean inputs so perceptron allows that we took imdb rating and critics rating as\nan input do we always need to hand code the threshold\nno in our perceptron learning algorithm are all inputs equal no we now assign weights\nto input what about functions which are not linearly separable we still do not  know\nso that is where we are headed now  not possible with a single perceptron  but we will\nsee how to handle this"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.7 Linearly Separable Boolean Functions.wav", "duration": 337.74, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nmodule  twoseven\nlecture  two\nlinearly separable boolean functions\nso in this module we look at linearly separable boolean functions again and we will\ntry to make some more statements about them\nrefer slide time zerotwenty\nso what do we have do so the guiding question that we have is what do we do about\nfunctions which are not linearly separable and let us see one such very simple function\ncan you guess what function i am going to talk about all of you are paying attention in\nthe first lecture\nrefer slide time zerothirtysix\nso here is the xor function now these are the set of inequalities that result from xor\nfunction i hope right now let us see the first condition implies that w naught should be\nless than zero second condition implies this third condition implies this fourth condition\nimplies this just looking at this can you tell me can you find a configuration for w\nnaught wone wtwo such that these inequalities can be satisfied together no right because two\nand three want it to be greater than minus one minus w naught and when you take an addition\nof that it has to be less than minus one\nso that is not going to happen so you see a contradiction so this is a simple boolean\nfunction which the perceptron cannot handle because it is not linearly separable it is not\nlinearly separable there does not exist a line if there does not exist a line you cannot\nfind the line in fact you can look at it visually\nso these are the red points for which the output should be zero or one and the blue points are\nthe points for which the output should be zero if we need to change this i think we were\nusing blue as positive and red as negative and you cannot just draw a line there is no\nway you can draw a line such that the blue points lie on one side and the red points lie on\nthe other side so it is a simple two input function so it is not that i have taken a very\ncontrived example\nrefer slide time onefiftyseven\nmost real world data is not linearly separable and it always contains some outliers right\nso here maybe you have some data where you are trying to say that people which live in\nthis part of the world belong to a certain or maybe people who live or work here have a\ncertain qualification people who work in this company may have a certain different\nqualification and there might be some outliers right it is not that is always going be\nvery clean so now what do i mean and it is not necessary that the points will only be\noutliers\nin fact there could be a clear case where there are no outliers but still you cannot find a\nline such that you separate the positive from the negative can you think of such an\nexample good right this is clear data there is no outliers here as well i mean it is just\nsaying  that  everyone  who lies within  this boundary has a certain  characteristic  and\noutside that boundary people have a different characteristic right and there is no outlier\nhere but you cannot separate this data with a line so all functions that you deal with\nwill not go or are not going to be linearly separable\nso we have to work around those right and while a single perceptron cannot deal with\nthis we will show that a network of perceptron\u2019s can indeed deal with such data so that\nis where we are headed\nrefer slide time threesixteen\nso before going there we will discuss some more boolean functions in more detail and\ni will try to see what kind of nonlinearly separable boolean functions are there\nrefer slide time threetwentysix\nso first of all how many boolean functions can you design from two inputs how many\ncan you design sixteen looks like a good number from three inputs two hundred and fiftysix how many if you\nunderstand this let us see so let us begin with some easy ones that you already know\nright so these are two inputs xone xtwo what is this function always off the other\nextreme is always on and i have already given you the answer f sixteen\nso then you have the and function and or function then some other functions right\nso why did you reach sixteen actually because with two inputs we will have these four\nvalues to take care of and each of these are again binary so you actually have two raise to\ntwo raise to n right so for three inputs two raise to two raise to three would be two hundred and fiftysix now that is\nthe easy part of these how many are linearly separable i will have to do any actually\nstare it in and seriously try to find the answer when you cannot really do that\nso turns out all of them except xor and in not of xor ok so for the two input cases\nthere  are  two  functions  which  are  not  linearly  separable  for  n  inputs  how  many\nfunctions would be not linearly separable it is an arbitrary n is not the answer you are\nnot going to disappoint me not n ok but what is the answer so for n inputs we will\nhave two raise two n functions of these we do not know how many are going to be not\nlinearly separable that is not a solved problem although i encourage you to go and find\nthe answer\ni am looking for a good will hunting kind of a moment but all it suffices to know is that\nthere exists some which are not linearly separable and that everyone agrees that there\nexists some right and as n grows probably that number will increase and so on but it is\nnot known exactly you cannot write it as a function so what we have done so far is\nlooked at boolean functions how many boolean functions can exist and of that we just\nhave concluded that there would be some which are not linearly separable"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 2.8 Representation Power of a Network of Perceptrons.wav", "duration": 769.56, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering indian institute of technology \nmadras\nmodule \u2013 twoeight\nlecture \u2013 two\nrepresentation power of a network of perceptrons\nwe will go to the next module where we talk about a network of perceptrons and then \nwe talk about the representation power of a network of perceptrons so this module \nshould have been titled as network of perceptrons so  now in particular what we are \ngoing to see is how any boolean function whether linearly separable or not can be \nrepresented using a network of perceptrons\nrefer slide time zerotwentyfive\nnow what do i mean by represented during a network of perceptrons what it means is\nthat i will give you a network of perceptrons you take any boolean function feed any\nvalue of xone to xn and the network will give you the same y as it is expected from the\ntruth table ok that is what representation means just to put it out clearly\nrefer slide time zerofiftyseven\nand now i am going to again do a setup i am not giving you the solution i am just\nmaking some set up and then we will discuss the solution\nso for this discussion we will assume that true equal to plus one and false equal to minus\none so instead of zero and one we will assume minus one and plus one and these are your inputs\nxone and xtwo we are taking the two input case and i will have four perceptrons first i will have\nfour perceptrons and i will also have very specific weights connected to form the inputs to\nthese perceptrons so red means minus one and blue means plus one right so the first two\ninputs are connected with a weight of minus one the next two inputs with minus one plus one\nplus one minus one and the last would be\nplus one now once i have this i will set the bias of all these perceptrons to minus two so\nthat will that means they will fire only if they are weighted sum of the inputs is greater\nthan two now after this i will have one more perceptron so i had two inputs i converted\nthat to four values these four values are now going to feed into one more perceptron\nand these weights i will not fix them these are the weights that i am going to learn ok\nthese and the final output of this perceptron which is the green perceptron is the output of\nthe network right so now coming back to what i said that it can represent any function\nwhat i mean is that you take any function feed in any combination of xone xtwo this\nnetwork will give you y and i am telling you that it will match the truth table of that\nfunction\nrefer slide time twofortythree\nnow let us define some terminology this will also stay with us for the rest of the course\nso this network contains three layers  the layer containing the inputs it is called the input\nlayer very creative the middle layer containing the four perceptrons is called the hidden\nlayer and the output layer which gives you the output is called the output layer output\nperceptron which gives you the output is called the output layer right so you have a\ninput layer a hidden layer and an output layer \nand the outputs of the four perceptrons i am going to call them as hone htwo hthree and hfour and\nthe red and blue edges are called the weights for the layer one which we have not learned\nwe have actually set them by hand and the weights for wone wtwo wthree wfour are called the\nweights for the second layer other the layer two weights and these are the weights that we\nwant to learn\nrefer slide time threethirtyfour\nnow  i make this claim  that  this  network it can take  any  boolean  function  it can\nimplement any boolean function so this same network can implement any boolean\nfunction that means if i take this network and if i try to learn the values of wone wtwo wthree\nwfour for any boolean function whether it was originally linearly separable or not i will be\nable to implement it\nso  isn\u2019t this  an  astonishing  claim  any  boolean  function  do  you think  this is  an\nastonishing claim well not really if you really understand what is happening here right\nso let us see what exactly is happening here so when will perceptron one fire when the\ninput is false false zero zero will it fire for any other input when will perceptron two fire any\nother input same for the next perceptron same for the next perceptron\nso you start getting an intuition of what is happening here you do  ok let us see so\nnow for this particular network now that i have given you some intuition of what is\nhappening basically every node or every neuron in the hidden layer is catering to one of\nthe inputs and it will fire only for that input it will not fire for anyone else\nrefer slide time fourfiftynine\nso now let us try to implement the xor function and see what are the so now let us\ntry to implement the xor function and see what are the set of inequalities that result\nfrom this earlier when we try to look at the set of inequalities we ended up with a\ncontradiction let us see if that happens now so this is xone xtwo this is your xor function\nso that is just like any truth table then i am noting down the intermediate values and\nthen my final input to the green perceptron is going to be summation of these and it will\nfire if this summation is greater than equal to zero or else it will not fire\nnow for the first case when the input is zero comma zero what is hone going to be one and\neverything else is going to be zero that is exactly what we saw in the previous slide so\nwhat is the summation going to be just wone right so it is wone hone plus wtwo htwo so on but htwo\nto hfour are zero so only thing that remains is wone for the second case wtwo for the third case\nwthree for the fourth case wfour\nso is it clear now what is happening let us go a bit more into detail right so now for\nthe xor condition what are the conditions that we need wone should be less than wzero\nbecause this should not fire wtwo should be greater than equal to zero wthree should be greater\nthan equal to sorry w naught not w is not zero and wfour should not fire so wfour should be less\nthan wzero are there any contradictions here by design no right so we have made sure\nthat for the final layer only one of these guys feeds to it\nso it does not matter what the remaining outputs are they do not interfere with each\nother unlike earlier where we had conditions like wone should be something wtwo should be\nsomething and then wone plus wtwo should be something there are no such contradictions\nhere because we have made sure that every neuron in the middle layer actually caters to\none specific input and now the weights in the final layer can be adjusted so that we get\nthe desired output for that input\nso i can set whatever value of wone i need to set so that i can fire the neuron in fact i\ncould just fix wzero as zero and then i can adjust the weights of wone wtwo wthree wfour and i can\nimplement the xor function so are you convinced that this can be used to implement\nany boolean function how many if you are not convinced so the negative question\nnever works how many if you are convinced sure\nnow what if we had three inputs\nrefer slide time sevenforty\nbefore that it should be clear that the same network can be used for any of the remaining\nfifteen functions and for each of these functions we will end up with a different value of wone\nwtwo wthree wfour but you will be able to satisfy the truth table right and you can go home and\ntry it which i am sure you will do\nrefer slide time sevenfiftysix\nah so what if we have a function of three inputs two hundred and fiftysix what is two hundred and fiftysix no\neight fine sure\nrefer slide time eighteleven\nso this is what it will look like and anything specific about the weights of the initial\nlayer can you tell me what the weights would be just tell me red red red red blue blue\nwhatever colours you like this thing first perceptron what would the weight colours be\nred red red then\nenough so this is how it will look right right and now this same thing will work with\nthe same logic for any boolean function of three inputs you will get these eight inequalities\nand they will not interfere with each other and you can set the values of wone to weight so that\nyou can satisfy it ok fine\nrefer slide time eightfortyseven\nso what if we have n inputs\ntwo power n perceptrons in the middle layer right ok\nrefer slide time eightfiftyfive\n\nso now here is the theorem any boolean function of n inputs can be represented exactly\nby a network of perceptrons containing one hidden layer with two raised to n perceptrons\nand one output layer containing one perceptron we just saw an informal proof of this we\njust constructed i just gave you the answer it this is how you will get it now note that a\nnetwork  of two raised  to  n  plus  one  perceptron  is  it  sufficient  or  necessary  or  both\nsufficient yes that is what it says is it necessary \nno we already  saw  the  and  function  which  we can  just  represent  using  a  single\nperceptron right so it is not necessary but it is sufficient so this is a very powerful\ntheorem if you think of it right so now this whole objection right remember this history\nand when we have the c i winter when people showed that perceptron cannot handle the\nxor function that is for a single perceptron\nif you have a network of perceptrons you can actually have any boolean function but\nwhat  is  the  catch  as  the  value  of  n  increases  the  number  of  neurons  increases\nexponentially right but still in theory you could have a solution\nrefer slide time tenone\nnow again why do we care about boolean functions i keep coming back to this why do\nwe care about boolean functions because you took this and so the question that i the\nquestion that i want to answer is how does this relate back to our original problem\nright we know any boolean function can be implemented how do we go back to our\noriginal problem which is whether we like a movie or not right\nand you could see that there is a whole family of problems there right whether we like\na movie or not whether we like a product or not whether i want to go home today or\nnot yes no any kind of a yes no problem it is a whole family of problems there\nrefer slide time tenthirtyfour\nso let us see so we are given this data from our past experience right so we are told\nthat this is what the movie looks like these are the actor\u2019s director\u2019s joiners everything\nwe also know whether we like these or not\nso we have a set of positive points and we have a set of negative points right and now\nwe want to have a computational model which can satisfy this data that means once the\nmodel is trained once whatever algorithm i algorithm i use has converged it should be\nable to give me the correct output for a given input that is what we are interested in and\nthat is a real classification problem that we are interested in now for each movie we are\ngiven these factors as well as the decision\nand i said pi\u2019s and ni\u2019s are positive and negative points the data may or may not be\nlinearly separable it is not necessary that the data is linearly separable those were the\ngoody cases it but in general that may not happen but do we worry about it now no\nwhat the previous theorem told us is that irrespective of whether your data is linearly\nseparable or not i can give you a network which will be able to solve this problem\nmodulo that it might be very expensive in the number of neurons in the middle layer\nbut if you keep that aside i have a solution for this\nand that is why we care about boolean functions because many problems we could\nactually cast to it in a simplistic way if we ignore the real inputs and if you even think\nof the real inputs suppose it could take all values between zero to one you can always make\nit binary you could say that is the value between zero and zeroone is the value between zeroone and\nzerotwo and you could make it as small the scale as small as possible right so that is why\nwe care about this\nrefer slide time twelvenine\nso the story so far has been that the network of networks of the form that we just saw it\nwhich contain one input layer output layer and one or more hidden layers these are\nknown as multilayer perceptrons but a more appropriate terminology would be multi\nlayered network of perceptrons \nbecause the perceptron is not multilayered you have a network of perceptrons and that\nnetwork has many layers right but generally there is abuse of notation we always call it\nmlp which is multilayered perceptrons and the theorem that we just saw gives us the\nrepresentation power of an mlp and basically tells us that it can represent any boolean\nfunction that we want to deal with so that is where we will end this class and in the\nnext class we will talk about sigmoid neurons"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 3.1 Sigmoid Neuron.wav", "duration": 709.12, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nlecture \u2013 three\nsigmoid neurons gradient descent feedforward neural networks \nrepresentation power of feedforward neural networks\nwe are in lecture three of csseven thousand and fifteen and today we are going to cover the following modules\nwe are going to talk about sigmoid neurons gradient descent feedforward neural\nnetworks representation power of feedforward neural networks\nrefer slide time zerothirtyone\nso let us start so here are some acknowledgments so for one of the modules i have\nborrowed ideas from the videos of ryan harris on \u201cvisualize back propagation\u201c they are\navailable on youtube you can have a look if you want for module threefive i have borrowed\nideas from this excellent book which is available online it is the url as mentioned in\nthe footnote \nand i am sure i would have been influenced in borrowed ideas from other places and i\napologize if i am not acknowledge them probably properly if you think there are some\nother sources from which i have taken ideas and let me know i will put them in the\nacknowledgments\nrefer slide time onetwo\nso with that we will start with module threeone which is on sigmoid neurons so the story i\nhad is that it is enough about boolean functions\nrefer slide time oneten\nnow we have done a lot of boolean functions but now we want to move on to arbitrary\nfunctions of the form y is equal to f of x where x could belong to rn and y could belong\nto r so what do i mean by this so let me just explain this with the help of an\nexample so i will again go back to our oil mining example oil drilling example where\nwe are given a particular location say in the ocean and we are interested in finding how\nmuch oil could i drill from this place and that is what i would base my decision alright\nwhether i want to actually invest in this location or not\nand then what we are saying is that this could depend on several factors so we could\nhave xone xtwo xthree up to xn right where this could be the salinity of the water at that\nlocation so this could be a real number this could be the density of the water it is\naverage density this could be the pressure on the surface of the ocean bed and so on and\nso forth\nso each of these values independently belongs to the set of real numbers so each of\nthis is a real number and we have n of these so together they belong to rn so i can\nread that i have n such real numbers and i could just put them in a vector and say that i\nhave a input x which belongs to r raised to n\nso we have this x which we can say belongs to rn and in this particular case we want\nto predict y we want to take this as an input and predictor y and what is y in this case\nyou want to predict the quantity of oil that we could mine so what does ry belong to\nagain a set of real numbers and it could be some gallons or litres or kill of water so this\nagain belongs to r so these are the kind of functions that we are interested in now\nwe want a function which takes us from i am having this x which belongs to rn right it\nis a vector of dimension n and takes us to a value belonging to r so you clearly see\nthat this is different from the case when we had n variables each of this was just boolean\nso these were only zero one inputs now we have real inputs and these are the kind of\nfunctions that we are interested in\nrefer slide time threethirtyfive\nnow can we have a network which can represent such functions  now what do i mean\nby represent such functions we already spoke about this when we were doing boolean\nfunctions so what do we mean by representing the function we mean that if i am\ngiven a lot of training data right so i am given these xone xtwo each of these belongs to rn\nand i am also given the corresponding labels now i want a network which should be\nable to give me the same predictions as is are there in my training data\nso it should be able to take any of these x\u2019s as input and it should give me the same y i\ncorresponding to it and i am saying approximately which means i am with some error\nrate whether if it is within some to with as long as it is close to the actual value i am fine\nwith it so that is what i mean by a network which can represent such functions is that\nworking definition of representing clear right so that is a very similar to the definition\nthat we were used for boolean functions we had said that we should be exactly be able\nto get the truth table the network should be able to represent the truth table exactly\nso that is very similar to the definition that i am using here\nrefer slide time ninefortyfive\nand then before we do this right before we come up with a network which can do this\nfor arbitrary functions we have to graduate from perceptron\u2019s to something known as\nsigmoid neurons so please remember this overall context that we dealt with a lot of\nboolean functions we analyze them carefully and we saw that we could come up with\nthese networks which could represent arbitrary boolean functions\nand they could represent them exactly as long as we have one hidden layer of course\nthe catch was that that hidden layer could grow exponentially now we want to graduate\nfrom boolean to real functions that means you have a real input of n variables and one\nor more outputs and you should be able to represent this exactly so that is where the\ntransition is where so that is the story that we are looking for\nrefer slide time fivethirty\nso let us start so recall that a perceptron will fire if the weighted sum of it is inputs is\ngreater than the threshold just recall that fine\nrefer slide time fivethirtyeight\nso now i claim that the thresholding logic which is used by a perceptron is actually very\nharsh now what do i mean by that let us see so let us return to a problem of deciding\nwhether we like or dislike a movie that is the same problem that we have been dealing\nwith and now consider that we base our decisions only on one input which is the critics\nrating which lies between zero to one and this is what my model looks like it takes the input\nas the critics rating i have learned some weight for it and my threshold is zerofive what\ndoes this mean it means that if for a given movie the rating is zerofiftyone will it predict like or\ndislike like so then i should go and watch the movie what about a movie for which the\ncritics rating is zerofortynine dislike so now you see what i mean by harsh\nso both these values are very close to each other but for one i say i like it for the other\ni say that i would not like it so it is not how we make decisions right you would have\nprobably said something equal for both the movies you would have not given such a\ndrastic decision\nrefer slide time sixfiftytwo\nso why is this happening so you might say oh this is a characteristic of a problem that\nyou  have  picked  up  maybe  that  is  the  critics  rating  which  is  between  zero  to  one  or\nsomething but i want to convince you that this is not a characteristic of the problem that\ni have picked up but this is something to do with the perceptron function itself so this\nis what the perceptron function looks like so this sum of all the inputs the weighted\nsum of all the inputs i am calling it by a quantity z and this is what i am going to plot on\nthe this axis so this is my z axis\nnow what does the perceptron say that when this value of z becomes greater than w\nnaught or minus of w naught it will fire and when it is less than minus of w naught it\nwill not fire that is what it says so this is a characteristic of the perceptron function\nitself it is going to have this\nrefer slide time sevenfortythree\nsharp decision boundary that whenever your sum crosses this threshold you will say one\nand whenever your sum does not cross this threshold you will say zero so in this toy\nexample over the movie critics it just happened that this was zerofive and so it was saying\nyes for zerofiftyone and it was saying no for zerofortynine so this will happen for any problem that you\npick up\nrefer slide time eightsix\nso to counter this we introduce something known as sigmoid neurons and this is just a\nsmoother function or a smoother version of the step function you see that\nhow many if you know what a sigmoid function what is the formula for a sigmoid\nfunction quite a few good and here is one such sigmoid function which is called the\nlogistic function so remember that sigmoid is a family of functions these are functions\nwhich have this s shaped logistic function which i have shown here is one such function\nand the other function that we will see in this course is something known as the tanh\nfunction so let me just get into a bit more detail with this logistic function\ni just want you to understand it properly so this quantity here remember we were\nwriting it as w transpose x which was summation i equal to zero to n wi xi remember this\nso now i am just going to consider this to be one over one plus e raise to minus w transpose\nx now i am going to ask you some questions and try answering those\nwhat  happens  when  w  transpose  x  tends  to  infinity what  happens  to  the  sigmoid\nfunction\none and that is exactly what is happening here as this tends to infinity as this keeps\ngrowing so remember this axis is z which is the same as w transpose x right this is w\ntranspose x so as it tends to infinity your sigmoid goes to one what  happens if w\ntranspose x is minus infinity\nzero and that is exactly what is happening here and what happens when w transpose x is\nequal to zero half so this is that value corresponding to half is that clear so that is how\na sigmoid function behaves fine\nrefer slide time tennine\nnow we no longer see a sharp transition it is a very smooth function and the sigmoid\nfunction lies between the values produced by the sigmoid function rate what is the range\nthat they lie between\nzero to one what is another quantity of interest that you know which lies between zero to one\nprobability so that is one advantage of sigmoid functions so now you can interpret the\nvalue given by a sigmoid function as a probability so what does it mean in our movie\nexample again so it just tells me in those two cases that with fifty one percent probability i\nlike the movie or with fortynine percent probability i like the movie so now this is not very\ndrastic or very harsh right i am not saying yes or no i am not committing myself i am\njust giving you a number which is proportional to how much i like the movie so it can\nbe interpreted as a probability\nrefer slide time tenfiftynine\nnow  here\u2019s the  overall  picture  it  so  this  is the  difference  between  the  perceptron\nfunction and the sigmoid function so notice that here we had this if else condition right\nwhich was leading to that sharp boundary\nnow here we do not have that defence condition we just have a function which is a\nsmooth function\nrefer slide time eleveneighteen\nand here is another picture so this is not smooth not continuous and not differentiable\neveryone  agrees  with  that  it  is  not  smooth here  right  it  is not  differentiable  here\nwhereas this is smooth continuous and differentiable and the contents that we covered\ntoday it will be very important to deal with functions which are smooth continuous and\ndifferentiable\nso for lot of this course calculus is going to be the hero of the course lot of the things\nthat we do will be based on calculus and in calculus always if you have smooth and\ncontinuous and differentiable functions they are always good so that is why we want to\ndeal with such functions"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 3.2 A typical Supervised Machine Learning Setup.wav", "duration": 921.5, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nmodule \u2013 threetwo\nlecture \u2013 three\na typical supervised machine learning setup\nwe will start module two which brings us to a typical supervised machine learning setup\nthis is a very important module please pay attention\nrefer slide time zerotwentyone\nso now we have a sigmoid neuron we have taken care of the fact that the perceptron was\na very harsh function so we have a smooth function so things are fine now what next\nwhere do we go from here what is my next topic going to be yes a lot of you are\ngiving the right answers we need to learn these weights it does not help just to define\nthe function this function depends on certain weights and now i need to give you an\nalgorithm which will help you to learn these weights\nnow remember when i talked about perceptrons before giving you an algorithm what did\ni revisit what did i talk about the error surfaces and then i had motivated from there\nthat our goal is to find a set of weights which give us close to give us zero error in that case\nor in general\u2019s speaking generally they should give us a minimum error they should\nhelp us to minimize the error rate so i need to set up that similar story here so we will\nagain revisit the concept of error\nrefer slide time onethirteen\n \nso now in the case of perceptron i had shown you this figure which they were this data is\nnot linearly separable which is obvious and i told you that perceptron cannot handle this\ndata but what do i mean by it cannot handle this data it cannot give zero error but what\nwould happen if i run the perceptron algorithm on this take a guess what does the\nperceptron algorithm do \nfine and i could convergences my condition i could make that condition a bit loose\nwhat is a valid convergence condition that you would lose here use here till almost all\nmy points are separated properly so instead of aiming for one hundred percent separation i\ncould have a threshold which says as long as ninety percent of the points are separated i am\nfine with it that looks like a reasonable thing to do\nso now if i run the perceptron algorithm with that condition what do you think will i get\nas a decision boundary everyone has a picture in mind ok let us see does this match\nwhat  you  had  in  mind  roughly  of  course  there  many  things  possible  but  it  will\nbasically pass through this now what is happening here what is the problem there\nare three blue points which are wrongly classified and three red points which are wrongly\nclassified\nbut in most real world applications we will find that this line is not too bad you could\nlive with this error this is probably three out of thirty on both sides which is roughly ten\npercent error unless you are using it when some mission critical applications  or in\nhealth care where it is a life and death situation or something in most cases you could\nlive with this right so if you are trying to predict whether people will vote for a\nparticular party if you make this kind of error it would be largely ok unless it is a very\nclose election but largely it would be ok\nso we could live with this kind of errors in most cases so from now on we are not\ngoing to be too optimistic and if you are going to say that there would be some error but\nmy job is to find the weights such that my error is minimizedi want the minimum\npossible error that i could get is that fine so again whatever weights we want to learn\nwe are going to be driven by some error function and we would want to minimize that\nerror function\nrefer slide time threefourteen\nso  this  brings  us  to  a  typical  machine  learning  setup  which  has  the  following\ncomponents so this perhaps is the most important slide in the course and i will say this\nat least for one hundred other slides in the course but at least for now this is the most important\nso you are given some data xi yi and you are given n such elements right so let me\njust elaborate on this and give me i will give you some instances of this let me give\nyou some instances of this right so one thing we say i already told you was this so this\nis my x and this is my y so one example which i gave was about movies so this was\ngenre actor and critics rating and so on this is one instantiation of this problem i could\nalso give you another instantiation which was i just told you oil right so this is how\nmuch oil can i get and here my factors were salinity density and so on there were\nmany other factors\nso this was my x again x belongs to rn where n is some number integer and another\nexample could be say fraud detection so i have a customer i am a bank i have a\ncustomer who has bought some credit card and i want to predict whether he or she\nwould commit a fraud and i would look at factors like what is his occupation maybe\nsalary maybe family size and so on there could be various factors which i could look at\nso here again this becomes an x ok and you could think of various such examples\nright where you are given an x and you are given a y ok so this is the data that you\nhave\nnow what is machine learning where does machine learning fit into this so we know\nthat there is some relation which exists between y and x in each of these cases all of us\nare convinced that there is some relation so whether a person would commit a fraud\nwould depend on these factors it is reasonable to assume that it is not a very wild\nassumption whether you would find oil at a location would depend on some of these\nfactors and it is related and similarly for the movie case\nso there exists some true relation between x and y such that if i plug this value of x into\nthe relation it would give me the value of y there exist a true relation this true relation\ncould  be  governed  by  various  things  right  it  could  be  governed  by  physical  laws\nexample in the oil mining case it could be even governed by biological laws again the\nmarine life in that location and so on it could be governed by economic law\u2019s it could\nbe covered by psychology right we do not know why a person cheats what is his\nfunction that he is using when he cheats and so on right so these could depend on\nvarious factors\nbut we all agree that some function exists hence we get these values for this particular\ninput for every input we get a certain value so there is some function which takes us\nfrom the input to the output we do not know what this function is we never know in\npractice it is a very very complex function is all that we know we do not know this exact\nfunction if you knew this exact function then there is no problem to solve we just use\nthat function and you can predict how much oil and all of us can become billionaires\nso that is not the case we do not know what this function is so then what do we do in\nmachine learn we make an assumption ok we make an assumption that there is some\nfunction which takes x to y and this function is governed by some parameters and this\nis our approximation of how the real world works and now under this assumption we\nwant to predict the parameters of this model given the data\nnow let us take a very simple case where we could assume that y is equal to wx plus b i\nam taking this in the scalar single dimensional case now how would you estimate the\nvalues of w and b oh come on if i give you two data points you can estimate the value or\nshould i write it that would jog your memory right this is how we all learn right so m\nand c you can estimate if i give you two data points so that is the simplest case now we\nwill  make  similar  assumptions  but  more  complex  functions  and  just  as  we  could\nestimate m and c from the data we would expect to estimate ws also from the data so\nthat is what the machine learning setup is so let us see\nrefer slide time sevenfortyeight\nso the model when we talk about a machine learning model it is our approximation of\nthe relation between x and y and we are free to make any such approximation so i\ncould say that this is what i think is the relation between y and x and which is governed\nby some parameters w do you know what is this function have you seen this before\nno not sigmoid\nwhich model is this logistic regression ok but i could also have made a different\nassumption i could have made this assumption what do i get linear regression ok\nplease note that this error on the slide ok and i could make some other assumption i\ncould  assume  that  y  is  actually  a  quadratic  function  of  x  i  am  free  to  make  any\nassumptions the only thing i need to ensure is there is some parameter involved what is\nwrong with making this assumption if this is valid is this also valid if not why there\nare no parameters\nso no not for any x we will get the we will it will depend still depend on the value of x\nif i plug in different values of x i will still get a different output there is nothing to\nlearn what do i do with all the data that i have there is absolutely nothing i can use it\nto learn i have just said that y is equal to one over one plus e raised to minus x i can ignore\nall the data that you had given me whenever you give me a new x i will just plug it into\nthis formula and tell you the answer and that is bound to be wrong because i have not\nadjusted this formula\nnow once i put in the ws it gives me this degree of freedom where i can now adjust the\nformula i can learn the ws in such a way that my predicted y\u2019s are very close to the\nactual y\u2019s so that is why we need always need a parametric form of course there is\nnonparametric  learning  also  but  i  am  just  saying  in  this  supervised  setup  we  are\nthinking of models whether you have parameters so you have the data you have the\nmodel the model always has some parameters\nrefer slide time ninefifty\nin all of the above cases w is a parameter right either the small w which is a vector or\nthe capital w which is a matrix right so notice that this is a matrix this is one cross n n\ncross n and n cross one\nnow how do we learn these parameters that is the question that we need to answer\nhow do we learn these parameters we are convinced about two things that we never know\nthe true function so we come up with an approximate function and we have to insert\nsome parameters in that function so far good now i have to be able to learn these\nparameters \nnow for learning these parameters we have something known as an learning algorithm\nso did you see any learning algorithm so far perceptron learning algorithm right so\nyou already saw the perceptron learning algorithm and it was able to learn the weights\nfor a perceptron\nrefer slide time tenfortythree\nthere are various such algorithms today we are going to learn one such algorithm which\nis gradient descent\nnow any kind of learning what is it driven by learning is driven by errors objective\nfunction and so the analogy which i like to give is suppose you are trying to learn\ntrigonometry you have a chapter that is your training data the chapter has a lot of\nformulae that is your training data now what is your objective there are two objectives\nactually i will tell you the easy objective the training time objective is that once you read\nto the chapter a few times at least whatever formulae are given in the chapter you\nshould be able to produce the correct output for that so if i ask you what is sine square\ntheta plus cos square theta you should be able to answers them and you should be able\nto give me the correct answer\nso in other words you are trying to minimize the error on the training data whatever\ntraining  data you have which is the chapter content you want to make zero errors in\nanything which is given in the chapter that is your training error of course there is also\nsomething as known as a test error because after you have learned the chapter i will\ngive you an independent set of exercises which might contain questions which are not\nseen in the textbook earlier so you would have seen sin square theta plus cos square\ntheta\nbut now i could ask you some other formula which you should be able to give me\nanswers if you have learned properly right so now right now we are just talking about\nthe training error that means getting all the formula in the chapter correctly and our\nchapter is actually the training data which is given to you this is what we are reading\nso this always going to be driven by an objective function and our goal is just as we\nwanted to minimize the errors that we make on the formula given in the chapter\nwe want to minimize the errors on the training data is this set up absolutely clear to\neveryone anyone who does not understand this has any doubts so this is something\nthis is the same framework that we will use again in lecture eighteen nineteen twenty and so on to\nexplain some more complex models so you have to absolutely make sure that you\nunderstand this it is not very difficult but just make sure you understand this fine so\nlet us concrete at this a bit more\nrefer slide time twelvefortyfour\n\nand we will consider our movie example and try to fit that into this framework so what\nis our training data there they are given movie comma like dislike and when i say\nmovie i am just using a shortcut it is actually all the details of the movie genre actor\ndirector critics rating and so on that is our input and our output is the like dislike value\nwhat is a model that i chose what is a model that i chose i do not know what is my\ntrue relation between when i like a movie or not but i made this approximation that this\nis how y depends on x and i made sure i introduced some parameters there i could have\nchosen some other functions also but i chose this now the parameter is w this should be\nbold w the learning algorithm that we are going to use is gradient descent which we will\nbe seeing soon\nand what is an objective function here can you tell me a formula so we have been\ntalking about it in terms of english that i should be able to get predictions which are as\nclose to the true prediction can you put it into a formula y i minus \ny i hat where hat is the prediction this is the prediction so that is y i minus y i hat that\nshould be minimized is that fine whole square of that so why do you square it so\nthat is correct\nso for all the training points n training points i want to minimize the square difference\nbetween y i the true prediction and the prediction sorry the true value and the predicted\nvalue is that fine and why do i use squares differentiable is one the other thing\nthe positive errors and the negative errors should not cancel so it would be happen that\non some movies i make a positive error of zerofive right that means the actual label should\nhave been zerofive and i gave one on some movies i make a negative error of zerofive and these\nwould cancel each other and i will get the false impression that i am making zero errors\nbut once i square the values the negative values also become positive  so this cannot\nhappen right so that is why we always use the squared error function and also this is\ndifferentiable which is more important\nnow the learning algorithm should try to minimize this particular quantity ok so this\nis a typical machine learning setup almost any supervised learning problem that you see\nyou could cast it in this framework change the y hat function appropriately change the\nparameters appropriately maybe use a different learning algorithm depending on the\nproblem that you are trying to tackle and you should be able to fit it into the same thing\nis that fine ok at least for this course everything that we do we will largely be able to\nfit it into this framework\nrefer slide time fifteentwentyeight"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 3.3 Learning Parameters\uff1a (Infeasible) guess work.wav", "duration": 673.02, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nmodule \u2013 threethree\nlearning parameters infeasible guess work\nlecture \u2013 three\nin this module we will try to learn these parameters and initially we will try to learn\nthem by guesswork and i will show that that is actually infeasible that is w hy we need a\nmore principled approach\nrefer slide time zerotwentysix\nso we will keep the supervised machine learning setup in mind and now we will focus\non this model and discuss an algorithm for learning the parameters which are w and b\ngiven some data using a giving appropriate function objective function so that is what\nwe are going to focus on\nrefer slide time zerofortyseven\nnow sigma here stands for the sigmoid function the logistic function in this case when\nthis sigma is actually the logistic function and now i am going to simplify this further\nso that it helps us to do a better analysis i am just going to consider the case where i\nam just in one input and the bias ok and also following the normal terminology in the\nliterature this w naught from now on i am going to call it b because that is the normal\nconvention b stands for bias\nso i have two parameters w and b which i need to estimate ok and this is my model for\nthe movie example  and the other change  which i am going to make is instead  of\ndeciding whether i like or dislike which is one zero the setup that i am going to work with is\nthat i am giving the critics rating and i want to predict the imdb rating so i am given\na real value and i also want to predict a real value\nfor no particular reason this just makes life easier for me for explaining a few things but\nthe same thing or the same algorithm would also hold if you add a binary output right\nand you will see that later on in the course so here is a setup clear we just have two\nparameters w and b and we are going to assume that y belongs to real numbers it is a\nimdb rating and x also belongs to real number it is a critics rating\nrefer slide time twofive\n \nnow let us see what we are given as training is a set of points we are given some n\ntraining pairs and now we understand what this means that means for a lot of movie i\nam giving the critics rating and i am also given the true imdb rating for them of\ncourse in the two variable case this does not make much sense but just bear with me\nand now the training objective is such that whatever my function predicts which is a\nfunction of w x and b that should be very close to the true output that i know this is the\nfunction that i want to optimize now let me ask you this\nrefer slide time twothirtyseven\n\ni am trying to tell you that i am going to give you an algorithm for training this network\nsuppose i have trained this with two data points zerofive comma zerotwo and twofive comma zeronine right\nat the end of training i will give you some values of w and b let us call them w star\nand b star these are the final values of w which i have given w and b what do you\nexpect from these values \nwhat do you expect at the end of training if i say now the network has learned what do\nyou expect you are still going to the test case i am just talking about the training still\nwe expect such that what happens if i plug in at the end of training if i plug in the value\nzerofive here what should happen \nzeronine so this is what you expect at the end of training if you plug in the value zerofive it\nshould be very close to zerotwo the output and if you plug in the value twofive it should be very\nclose to zeronine\nthis is exactly what you expect and this is what training means ok fine in other words\nwe hope to find a sigmoid function such that these two points lie on that function can\nyou imagine a geometric picture for this what would happen actually how many if we\ncan imagine it ok how many of you get it now this is what will happen right so you\nwill get a sigmoid function such that these two points lie on that fair ok and that exactly\nmeans that when i plug in this value i will get this value and when i plug in this value i\nwill get this value right so that is what it means\nrefer slide time fournine\n\nso let us see this in more detail\nrefer slide time fourten\nand now what we will do is our quest is for this w star and b star i will try to find this\nmanually i will do some random guesswork and try to find this because i do not have\nany clear principle algorithm for finding it as of now so i will just use some guesswork\nso i will give my initial guesswork as w equal to zerofive b equal to zero for no reason i just\npicked up some values right and this is what the function that i got what does this\nmean this function an error so the sigmoid formula should be here we should have this\nsigmoid formula here\nso is this a good are you happy with the solution if i give you are you happy with this\nsolution is this good bad ugly has to be something bad we will not call it ugly ok\nso why is it bad it is not passing through those points i will ask you a question how\nbad is it can you assign a number to it we are always good at qualitative stuff but\nquantitatively can you tell me a number how bad is this can you tell me a way of\nfinding how bad this is i already told you in detail how to find that how bad it is\nthe loss function right\nrefer slide time fivetwentyseven\nwe have the loss function let us see that again and see if we can find out how bad this is\nrefer slide time fivethirtytwo\nso this is what my loss function is ok and i have two data points i will just expand it out\nfine now i will plug in the values i know this is zeronine and i will compute the value of f\ntwofive i will plug in this and i will plug in this ok and this is what i get so this is how bad\nit is what did we actually expect it to be in the good case zero so this is not zero this is\nzeroseventythree so now we have a quantitative handle on how bad this is ok so let us keep this\nin mind and let us try to continue guessing so we want the loss function to be asked\nclose to zero as possible we are not there yet\nrefer slide time sixfourteen\nso then i make a different guess i say let me try minus zeroten zerozero what happened now\nis it now good bad ugly \nnow let us call it ugly right so it is worse and how do i know it is worse because i\nplugged it in to the loss function and i got a value which is greater than the value at\nwhich i was so i clearly know this is bad\nso now this is how my mind is working right oh i as far as w was positive things looked\nat least i was close to zero in the first decimal now when i made it negative that does not\nlook good so let me just keep it positive and keep increasing it right so i saw zeroninetyfour and\ni also tweaked the b of it i have done complete random guesswork right now what\nhappened good bad ugly \nbetter ok now what will you do what would your next case would be make w even\nmore positive perhaps that would help and be even more negative and so on\ni can continue in this manner and actually get close very close to the solution so i can\ndo this guesswork and find these values but it is still an educated guess right i am not\nguessing in the dark this is what is helping me drive towards those guesses and i am just\nlooking at these values and making an educated guess right and that is the educated\nguess which i took that probably making w even more positive would help but this is\nstill brute force in a sense right this is not something that you would want to do when\nyou have one hundred one thousand parameters and so on right and one million data points and so on\nrefer slide time sevenfortytwo\nso let us look at something which is better than our guesswork algorithm ah so we are\nnot there yet actually on the next slide i am still going to talk about the guesswork\nalgorithm\nrefer slide time sevenfiftythree\nand eventually we will get to something which is better than the guesswork that ok\nso since we have only two points and two parameters what i can do is i can take all\npossible values of w and b right that is what i was trying i was picking up some values\nof w and b why just pick some values of w and b i will pick all possible values of w\ncomma b right and i will fix the range i cannot fix pick it from minus infinity to\ninfinity but i will pick a range i will say from minus six to six let me try all values of w\ncomma b compute the loss and plot it right let me tell something about this error\nfunction because this is going to stay with us for quite some time\nso what you see here is something like a flying carpet this is colour coded red is bad\nred are the places where the error is high blue is good blue are the places where the\nerror is low darker the shade of blue lower the error darker the shade of red higher the\nerror so in particular if i look at this point what has happened is i have taken the\ncorresponding value of w comma b right which is say minus four comma minus one right\nsomething like that\ni have plugged that value into my loss function and i got this as the loss function this\nhas the loss value and that is what i have plotted for all values between minus six to plus six\nand minus six to plus six for w and b so everyone understands how i have constructed this\nerror surface\nnow this of course becomes and now what i can do is once i see this error surface i\nknow how good this is the point where i need to be this is the darkest ah shade and this\nis where the error is the lowest so i can just pick a w comma b value which lies there\nthis is fine for this toy example where you just have two parameters but this becomes\nuntractable once you have more data points and many more parameters\nand that is what happens in most real world applications right so this is not a feasible\nway of going about things right and here again note that i have only taken the range\nfrom minus six to six i do not even know what will happen if i have to look at all values of\nw comma b right maybe there was something outside here right which was even more\nlower error or something right so i do not really know that\nso i cannot really use this so i need something better than this plotting the error\neverywhere and finding it order that is pure brute force or surrogate to this was the\nguesswork algorithm but which is again something we cannot do for if you have large\nnumber of parameters\nso everyone gets this that this is a way of finding the solution but this is not feasible\nright that is the only point i am trying to make\nrefer slide time tentwentyfour\nand we look at the geometric interpretation of what was actually happening in the case\nof the guesswork algorithm with respect to the error surface\nrefer slide time tenthirtythree\nso i had chosen some values of w comma b the first value that i chose actually gave me\nan error of if you remember it was some zeroseventythree or something like that right so that is the\npoint then i decided to take a very random guess and my error actually increased so\nyou see that i am actually climbing up on this error surface i have gone from a slightly\ndarker shade of blue to a lighter shade of blue right and then i corrected myself and then\nkept moving in a direction where i was going towards the darker and darker shades of\nblue\nso what i was actually doing is i was trying to traverse the error surface and land up in\nthe good regions which were the dark blue regions now what i want to do is i want an\nalgorithm which will allow me to do this in a principled manner which is neither brute\nforce nor guesswork so that is what we learn in that module"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 3.4 Learning Parameters\uff1a Gradient Descent.wav", "duration": 1840.42, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  threefour \nlecture  three \nlearning parameter gradient descent \nin this module we will talk about gradient descent \nrefer slide time zerotwentytwo \nso what we want to do is find a more efficient and principled way of navigating the \nerror surface  \nrefer slide time zerothirty \n \nand the goal is to find a better way of doing this \nrefer slide time zerothirtyfive \n \nso let us start by setting up t hings we will define some notations and some parameters \nand so on and from there on we will try to come to the algorithm  ok so my parameters \nin this case were w co mma b what i am going to do is i am going to put them into an \narray or a vector  right and call that vector as theta  so theta is the vector of parameters \nand theta belongs to r r what rtwo right there are two parameters here so it is a two \ndimensional vector \nrefer slide time oneeight \n \nnow what i want is again what i will do is i do not know what the value of w comma b \nis so i started with a random guess  so that is  always going to be my starting point  i \nwill always start with a random guess and from there on move on to good values  now \nonce i have started with a random guess  i want you to tell me some changes that  i can \nmake to w and b  so that i land up in better situations  right that means  i land up in \nsituations where the error is less is that fine \nso that change in w and b i am going to call it as delta w and delta b and that again is a \nvector which is storing these two values  so this is the picture right  i want to take theta \nand i want to add a small change to it  so this is my theta this vector is actually theta \nright this is the theta vector  i want to add a small c hange to i t which is again a vector \nthis is delta w comma delta b  such that i will get a new value for theta new so theta \nnew would be what actually theta new is equal to w new comma b new is that fine that \nis what theta new means \nrefer slide time twoseventeen \n \nnow what has happened is actually when i have added delta theta to theta i have moved \nin the direction of delta theta i have come from here to here now i am going to be a bit \nconservative and i am going to say that while i am ok in moving in the direction of delta \ntheta i do not want to make a giant stride what  i will do is  i will just move by a small \nquantity in that direction \nso this delta theta is this large magnitude  so all i am saying is that i will not i move in \nthat direction i am fine with that but i do not want to make a giant stride  i will just take \na small stride in that direction  so eta is a scalar which actually scales down delta theta  \nso now if i am going to take only a  small step in that direction instead of this large \nchange i will just get a smaller change theta new so red the red vector is actually going \nto be the movement which  i make that is the new value of theta  so theta new is equal \nto the original theta plus a small step in the direction of delta theta \nso everything is clear  you are done we are  done with gradient descent  what is \nmissing what is delta theta right i am telling you i want to move in a certain direction \nbut what is the right delta theta to use how many of you know the answer to this what \nis the answer move in the direction \nopposite to the gradient  why where does that answer come from  not the ml class \nfolks how many of you know why we need to move in the direction opposite to the \ngradient why ok we will see ok so that is  the question that we need to answer  if i \ngive you an answer to this question then what is it  i am doing  i am giving you a \nprincipled way such that you start from a random value of theta move in certain direction \nand you will ensure that your loss has decreased  and then you have to keep doing this \nright so that is the set up and the answer to this comes from taylor series \nrefer slide time foureleven \n \nso now what i am going to do is  i am going to give you the right direction delta theta \nfine and for ease of notation i am going to call it as u so remember what this delta theta \nis what is it change in w comma change in b  so it is a vector in rtwo remember that \nok i am just going to call it as u now this is what taylor series tells me what it tells me \nis that if i am at a certain value of theta and if i want to change that value a bit then what \nis going to be the new value of the loss function or any function for that point and this is \nthe formula for that  ok now what is let  us see what are some quanti ties here what is \nthis quantity scalar vector matrix \nscalar this \nvector we just did that right it is a vector what about this \nwhat is this quantity actually \ngradient what is the gradient  what is the gradient  no you are telling me how to use \nthe gradient i am asking you what is the gradient you are giving me absolutely correct \nand absolutely useless definitions \nthat is a very good answer ok so now what i am going to do is i am going to digress a \nbit and i am going to tell you something ab out derivatives partial  derivatives and \ngradients and then we will come back to this  ok so now suppose you have a function l \nthis is l in my handwriting this function of w  and say this function is w  square ok \nnow what is what is this called  a derivative of the function with respect to w  this is \nthe derivative and you know this is twow ok now suppose i have a function b square \nnow what is this quantity \nis a partial derivative of the function with respect to w why partial \nbecause it is considering b as a constant and taking the derivative with respect to only \none of the variables right this happens to be and what is this quantity  oh sorry so is w \ncomma b right this is the partial derivative with respect to b ok \nnow can you tell me what  is a gradient the gradient is nothing  but it is just these two \npartial derivatives taken together and put into a vector  right now suppose i had a \nfunction which depended on hundred variables  what would the gradient be size of the \ngradient \nrone hundred it would lie it would be a hundred dimensional case ok so now can you tell me \nwith this evidence in knowledge but this primer can you tell me  what this is this is a \ngradient vector \nwhich is right there in front of you in a red ink \nthis is what it is  right fine everyone ok with that so actually the right way to write \nthis and probably we need to correct in the slides would be theta so remember that theta \nis equal to w comma b  so this is the derivative of l theta with respect to theta which is \nnothing but the collection of the partial derivatives with respect to the components of \ntheta is it fine  so everybody understands what is  a derivative partial derivative and \ngradient ok fine so now the gradient is a vector in this case fine ok \nrefer slide time eighttwentyfive \n \nso now what is this quantity it is a \nno it is what is this \nthe dot product between these two vectors ok fine now one last thing and many more \nthings actually so what is this square of the gradient \nthis is not the square  of the gradient what is this  hessian fine everyone knows the \ntextbook what said can you tell me what does it is a scalar vector matrix \nmatrix what is the size of this matrix \ntwo by two what are the elements of this matrix \nsecond order partial deriv atives right so it is the gradient of the gradient  right is that \nfine so what does that mean you had thi s gradient this is the gradient  now you want \nto take the gradient of this again with respect to w comma b  right that is  what this \nmeans it is a gradient of the gradient right  so what that means is we will take the \ngradient of the first quantity again with respect to w  so that would be dou square by \ndou w what would this quantity be  \nwhat would this be \nis that fine and you can fill in th is quantity right so now it is clear what the hessian \nis it i s the derivative of the derivative and it would be a matrix  ok is that clear to \neveryone so i have a habit of doing a lot of these basic stuff  i know that the top twenty \npercent of the clas s gets really pissed off when  i do this but as a philosophy i teach for \nthe bottom thirty percent of the class \nso i do not mind that and the other thing is  i use slides so i do not write a lot of math  \nso i can cover a lot of material despite doing all th is basic stuff right so i am going to \nstick to that what i am trying to say is that write this in the feedback that you do not like \nthis basic stuff  but it is just that  i am going to ignore that feedback  i mean just being \nhonest right so i like doing this because it just takes me ten minutes to do this and for \nthe rest of the class  i do not have to look at blank faces  afterwards right so it really \nhelps me a lot fine so is that all clear all the quantities here are clear \nso now so this is th e gradient this is the hessian and now eta remember what did we \nsay about eta \nit is a small quantity and what do we do with small quantities always in maths \nwe ignore them so once we take their powers you are always ignore them  whether it \nis correct or not who cares  i mean someone has told it  it is good to ignore so we will \nignore it right so now all these higher order terms we can ignore right  that means  i \nwill only consider this fine \nso let us again look at what the setup is  the setup is that i have some value of theta  i \nwant to move away from that value such that  what do you say about this loss compared  \nto this loss  i will call this the new loss and  i will call this the old loss  what is the \nrelation between them \nthe new loss should be \nless than so if i or someone gives you a u i am not getting ok someone gives you this u \nthen what does what when would you say it is a good u \nrefer slide time twelvetwentyeight \n \nif this condition holds everyone agrees with that  right so i have found a good direction \nto move in if this condition holds now this condition actually implies that this condition \nshould hold right this is l theta plus eta u  right so if i just do minus  here i get this \nright so this quantity which should be less than equal t o zero implies that this quantity \nshould be less than equal to zero and remember eta is a positive constant ok why cannot it \nbe negative \nwhy because you wanted to take a small step in that direction  if we make it negative \nwe will do what \nwe will reverse the direction we do not want that as of now  right so eta is that for a \npositive quantity  so that means this quantity should be less than zero is it fine with \neveryone \nrefer slide time thirteentwentynine \n \nso so far after all this  story what we are left with is  this condition should hold for the u \nthat i am trying to choose so that i can be sure that i have chosen the correct u right and \nthe definition of correct u is that the loss at the previous step  the loss of the new step \nshould be less than the loss at t he previous step  is that fine  so that is  what we have \narrived at \nnow what is the range of this quantity that is why i asked you what is this this is a \ndot product i will leave it at that  so now you tell me what is the range of this people \nfrom the ml class cannot answer did i cover this in the ml class no ok fine what \nis the range of this not a very hard question \nplus or minus \nstudent refer time fourteentwentyfive mod of u refer time fourteentwentysix \nvery good how many of you understood that answer  he said plus or minus mod of u \ninto mod of gradient the gradient vector right why is it so easy \nlet beta be the angle between u t and this between sorry it should not be u transpose \nbetween u and the gradient then we know that this condition holds  cos beta is given by \nthis quantity and we know that cos beta lies between minus one and one ok now if i just \nsay that this quantity is equal to k then i can just get this condition \nnow let us see what are we trying to do we are trying to find a u such that this quantity \nis negative we are trying to find the use such that this quantity is negative  now i just \nstop at negative we would like to make it as negative as possible  right because the more \nthan negative it is the more will be the decrease in my loss fu nction right because this \nquantity tell me tells me how much my loss decreases  so the more the negative it is the \nmore the loss will decrease so let me make it as negative as possible \nnow what is that value  when will that happen  when alpha is you k now the answer \nyou started with the answer \nstudent refer time fifteenfiftyfour \nno what is that one phrase which you have marked up move in the direction  \nstudent refer time sixteenseven \nok now think of that \nstudent refer time sixteennine \nwhat would happen when this is the most negative it can be what would the angle be \nstudent refer time sixteennineteen \none hundred and eighty degrees h ow many of you get that because when this is the most negative  that \nmeans the cos beta is actually minus one and when is cos beta minus one when the angle is \none hundred and eighty degrees that means u should be such that it is at one hundred and eighty degrees to the gradient hence \nrepeat the phrase \nstudent refer time sixteenfortytwo \nmove in a direction opposite to the gradient  is that fine  everyone gets it now  why \nyou need to move in the direction opposite to the gradient \nrefer slide time sixteenfiftythree \n \nso this is what the gradient descent rule is  you are at a particular value of theta you \nwant to move to a new value of theta such that your new loss is less than the current loss  \nwhat gradient descent tells you is move in a direction opposite to the gradient  so are \nyou fine with this  now with gradients i have come to scalars  but i will just explain \nwhat i have written here \nso this quantity is nothing  but theta t plus one right is equal to theta t  right and what is \nthis right so the new theta is equal to the current theta minus why because we want to \nmove in the direction opposite so it is basically theta t plus one is equal to theta t plus eta \ninto a negative directi on right the direction negative  to the gradient hence you get that \nminus one \nnow what are these quantities let me just take that carefully so this quantity is gradient \nof the loss function with respect to w  sorry the partial derivative of the loss function \nwith respect to w evaluated at w is equal to w t and b  equal to bt what does that mean  \nso remember when you are dealing with derivatives as always a formula and then a \nvalue add that at a particular value  so what is the derivative of x square  with respect to \nwhat does not matter twox \nso derivative of x square with respect to x is twox what is the value of this derivative at x \nequal to one two right so you see the difference you have a formula which is twox now you \nsubstitute in a particular value and you get the  value at that particular value  ok so that \nis what this means because you are already at w  t comma b t now you cannot subtract a \nformula from here right you have to put subtract a value so you know what the formula \nis you plug in the values of w t comma bt get that value and subtract it from your current \nwt is that fine so everyone completely understands what is the gradient descent rule is \nfine \nrefer slide time nineteenfourteen \n \nso now we have a more principled way of moving in the w  b plane what do i mean by \nthat remember this was our w  b plane this was our error  this is something what our \nerror surface looked like  it was this flying carpet  i was randomly moving on the w  b \nplane earlier right and trying to guess what the errors or trying to compute the error and \nthen settle for a particular value  now i have a more principled way of moving in the w  \nb plane i know what is the next step based on the current step  i just need to move in the \ndirection opposite to the gradient \nso let us try to so this is what it tells me for one step but i need to keep doing this till \nwhat is that golden word \nstudent refer time nineteenfiftyeight \nconvergence right i have to keep doing this till convergence ok \nrefer slide time twentytwo \n \nso let us create an algorit hm out of this rule  i will start a time step zero i will do this for \nsome max iterations instead of saying till convergence i will do it for some iterations at \nevery iteration i will this is how i will update my weights i will take the current weights \nsubtract the gradient from that and get the new weights i mean not subtract the gradient \nsubtract this quantity and get the new weights  so now is everything clear  is the \ngradient descent algorithm done can you do it for the toy network which i had is there \nsomething still missing \nstudent refer time twentythirtyeight \neta is fine we will take a small value zeroone or something  actually not told you what \nthese are  right i means to write it you know these are derivatives  but what is this \nactually ok so let us see that now so that is what we are going to see next \nrefer slide time twentyonezero \n \nso now we want to find out we are in the car quest is for this delta sorry the partial \nderivative with respect to w and partial derivative with respect to b  that is  the thing \nwhich we had plugged in the formula  but we do not know what that is  right so we \nneed to find that out  so now for simplicity let  us assume there is only one point of it \nwhich is x comma y  so earlier we had this x one yone and xtwo ytwo now i am just assuming \nthere is only one point which is x comma y \nrefer slide time twentyonefifty \n \nso now what is a loss function earlier i had this summation over i equal to one to two but i \nhave just one x comma y  so i will just use that  this is what my loss function a nd what \nare the quantities that i am interested in finding one is this the partial derivative of this \nloss function with respect to w \nrefer slide time twentytwoeleven \n \nso let us do this lets actually derive this  so this is what it looks like now you have to  \nhelp me in deriving this what will i do first \nstudent refer time twentytwoeighteen \ntell me the next step \nstudent refer time twentytwotwentythree \ntwo into f of x minus y and push the gradient inside  of course the derivative is that fine  \nanyone who has a problem with this  next y is a constant this is the true  i remember \nso that is why this is a constant is not the predicted y \nnow this quantity what is f of x actually \nstudent refer time twentytwofiftyone \nsigmoid function right so i will just write it  now this is the qu antity that i need the \nderivative for so i will just write it here  what is the next step  this is of the form one \nover x so what will it be \nstudent refer time twentythreefour \nminus one over x square and then you put the derivative and say it is that fine now the \nquantity inside is of the form e raise to x so the derivative is e raise to x and you push it \ninside is this fine so this on slide should come both these are coming together  so is \nthat fine now what is this actually \nstudent f of x \nf of x what is this \nstudent refer time twentythreethirtytwo \nthis is actually one minus f of x  just take my word for it for now  you can go home and \nwork it out right so this actually if you do one minus this and do some trickery you will \nget to this quantity right so what you get is a very simple formula f of x into one minus f \nof x into x  i am going to substitute back here  so now i exactly know what the partial \nderivative of w is \nrefer slide time twentythreefiftyseven \n \nso there is only one point  then this is  what the partial  derivative with respect to w is \ngoing to be of the loss function  right if there were two points what would happen  if \nthere were two points my loss function was this is a sum of two elements  and i am \ntaking some derivative of a sum i will get a sum of derivatives right \nrefer slide time twentyfourtwentythree \n \nso how many of you will not cringe if  i say this is the answer  anyone who has a \nproblem with this you get this how many if you do not get this how many of you get \nthis good fine now can you do a sim ilar thing for b  can you tell me the answer \nwithout actually deriving it \nstudent refer time twentyfourfortyone \ni can perfectly understand what you are saying \nstudent refer time twentyfourfortyeight \nx would not be there right because this last x that you see here came be cause w into x \nwas there but b we are not multiplying x so what we will get is this you can go home \nand check \nrefer slide time twentyfivetwelve \n \nso now we have everything that we need  now we actually have everything that we \nneed ok no more trick questio ns so now we will write code to do this  ok we will \nactually implement the code and see what happens  so these are the two data points that  \ni had zerofive comma zerotwo and twofive comma zeronine the first thing which i need is something which \ncan implement the sigmoid function so this is one over one plus e raise to minus w x plus \nb is that fine \nnow i need something which can compute the error  so this is summation of half into f \nof x minus y the whole square  i go over all the data points summation of half into f of x \nminus y the whole square is that fine now what i will do is i will take this try out a lot \nof values of w comma b and plot the error surface ok  but this is only for illustration  in \npractice i will not do this we just know that this error surface  exist i just want to verify \nthat whatever  algorithm i come up with does not  efficient navigation of this error \nsurface that is what i want to verify that is why i am plotting this \nnext time you need a function which can compute grad of b  we just saw t his on the \nprevious slide this is f of x minus y into f of x into one minus f of x  right simple \neveryone is fine with this  then i need a function which can compute the grad with \nrespect to w same thing except that  i have this x at the end  so i have all the ingredients \nin place now what would i do what is the next thing that  i will write the main loop \nright i will write the main loop now \nso this is what the main loop look  like looks like i start with some random initialize for \nw comma b remember that our initial theta which is composed of w comma b is going \nto be some random guess  so i started with the random  guess which is minus two comma \ntwo i have chosen eta to be one that means i am not going to be conservative  i am going to \nmove in the direct ion of the gradient  if i chosen at zeroone and zeroone i would have been \nconservative and  i am going to run this till one thousand epochs which is my notion of \nconversions \nnow in each epoch what  i am doing is for every data point  so remember that this \ngradient with respect to w was a summation of  i equal to one to two and that formula  right \nso for each data point  i am computing the grad adding it  right so that is  the \nsummation part similar thing i am doing for b once i have computed the gradient which \nis the summation quantity i am just moving in the direction of the gradient  is that fine \neveryone understands the code  it is simple python code and it does exactly what  i had \nshown in the pseudo code \nnow let us execute this code and see what happens  so  i will start with my random  \npoint which was minus two comma two and now i am going to actually run this code and \nkeep plotting what happens on the figure  so just pay attention fine  so now here is \nhow the code is running see what is happening what is happening actually so at every \npoint i am changing my w  so that i am moving in the direction of the gradient  i keep \ndoing that as i keep doing that my error keeps decreasing  why because that is exactly \nwhat we got from taylor series that if we do this the error is bound to decrease right and \nthen we keep doing this and after a few iterations we will actually reach almost the value \nwhich is the zero error  right and this same thing would happen if you start from \nanywhere else it will keep moving in a principle d way and reach the low error \nconfiguration \nnow some of you would say that maybe this was the shortest path  right it could have \njust rolled over from there  but that is not a principled way of doing that right we the \nprincipled way of doing it is to mo ve in the direction of the gradient  you might take a \nlonger route but reach your destination  taking shortcuts is always risky  in life as well \nas here so so do not please this is an advice for error assignments and  so on so this is \nthe more princip led way and we will reach the solution  so that is  what is happening \nhere so we have actually derived everything that we needed and this is all you need to \nwrite for gradient descent for this toy example that you had \nnow answer this question  now suppose i had hundred such variables  instead of w \ncomma b  i had hundred such variables  what would happen  you do not  have to \nvisualize it \nstudent refer time twentyninefortyeight \nin terms of the code \nstudent refer time twentyninefiftytwo \ni will just need to have these functions for all of those i will have to calculate it by hand \nbut still doable  it is just a lot of tedious work of course  later on we will see a more \nrefined way of doing this where we can do a lot of these computations at one go  so we \ncan directly start  operating in vectors as opposed to scalars here  i am treating w and b \nseparately here i could have actually had a function which tells me grad of theta directly  \nright and later on we will see something like this ok  but for now  the code is still \nrunning here \nrefer slide time thirtytwentyfour \n \nnow it suffices so later on we will see gradient descent in more detail in the course \nand we will also see a lot of variants of gradient descent  but for now it suffices that we \nhave an algorithm which can learn the p arameters of a sigmoid neuron  so just as we \nhad the perceptron learning algorithm  we have the gradient descent learning algorithm \nwhich can help us learn the parameters of the sigmoid neurons starting from random \nvalues and it gives a principled approach for doing that"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 3.5 Representation Power of Multilayer Network of Sigmoid Neurons.wav", "duration": 2094.18, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  threefive \nlecture  three \nrepresentation power of a multilayer network of sigmoid neurons \nbefore we move on to the next modulate some small corrections from yesterday\u2019s class \nrefer slide time zerothirteen \nso one was this partial derivative it should have been do u w square  so we already \ntaken one derivative with respect to w and now you are taking another derivative  it is the \ngradient of the gradient  and similarly should this should have been dou b square and \nthis should have been dou w dou b \nrefer slide time zerofortythree \n \nthe other small thing which  i wanted to say was  so when i was executing this \nalgorithm right so i forgot to m ention that just notice what is happening is the black \ndot that you see the black dots that you see  right and which are very close to each other  \nactually because you are just making small movements those are the changes in the w  \ncomma b values and the r ed dots are the corresponding loss to that w comma b values \nright just to clarify \nso that is why you see a movement on the w b plane which is this movement and as you \nkeep changing that your loss function changes and it becomes better and better right that \nmeans it goes closer to zero \nrefer slide time onetwentyfive \n \nso in this module we are going to talk about the representation power of a multilayer \nnetwork of sigmoid neurons right so i am going to compare these two things which are \nwritten in the title  so first tell me  what was the representation power of a multi layer \nnetwork of perceptrons  ok i roughly hear what you are saying and basically what you \nare telling me is that a multi layer of network of perceptrons with a single hidden layer \ncan be used to represent any boolean function precisely right no errors that\u2019s what we \nsaw with that illustrative proof where we actually constructed once its network \nnow what is the representation power of a multi layer network of sigmoid neurons so \nmultilayer network of neurons with a single hidden layer can be used to approximate  ok \nso just see the difference in the language  so this was a represent  that means exactly \nthis is approximate  that means  i will tolerate some error  any continuous function \ninstead of boolean function to any desired precision  so this was not this was precisely \nwith no errors this is up to any arbitrary desired precision \nso what does this mean  actually what is the  meaning of this  so there is a guarantee \nthat for any func tion ok which takes our original x from r n to r m what is the m that \nwe have been considering in all our examples  one right we just care about one output  \nbut it can be r m also  we can always find a neural network with one hidden layer \ncontaining enough neurons so that is the operating trace here enough neurons whose \noutput g of x  so that means you would have a network it would take as input n x it \nwould produce some y hat and that is what i am calling as g of x right that g of x would \nbe very close to the true function f of x \nso remember that we said that there is this true function f of x which gives us the true \ny\u2019s and we are trying to predict this y hat  so the true why i am calling by f of x and the \ny hat i am calling by g of x and you can come up with a neural network which can give \nyou values which can predict values which are very close to your true values  does that \nmake sense do you see the value of this theorem  what is it trying to tell me  tell me \ncan you can you give me an interpr etation of this why is this so useful do you know \nwhat this theorem is called  universal approximations here and we did that in the history  \nright \nrefer slide time threefiftyseven \n \nso this was one thousand nine hundred and eightynine ok what is the significance of this  why do we care about s uch \narbitrary functions and what does this theorem telling us actually  it is of course telling \nus something about the representation power of a multilayer network of sigmoid neurons \nbut why is this important so we will see that \nrefer slide time fourtwentysix \n \nso this the remainder of the lecture  i have borrowed ideas from this url you should \nactually read this  it is a very interesting book  it is available online for free  very \nillustrative so please take a look at it ok \nrefer slide time fourfortyone \n \nso now actually what we are interested in is we are interested in knowing whether a \nnetwork of neurons can be used to represent any arbitrary function like the one shown in \nthe figure ok so let me put some labels on this so they understand what i am trying to \nsay suppose this is salinity again i go back to my oil mining example and i say that my \ndecisions are based only on a single variable which is salinity and this is actually how the \namount of oil varies right as the salinity  increase it is a very  arbitrary function  it is \ndefinitely not a linear function it is not even a quadratic function it is not an exponential \nfunction it is just some arbitrary function but a mathematical function this is possible it \nis quite likely that salinitie s has this influence or in oil production or maybe it does not  \nbut i am just taking that as an example \nnow what do we want the network to learn  if i take some data and train the network at \nthe end of training what do i want so if i feed at this point after training what should \nhappen it should give me this value  right that is what training means and  that means i \nshould be able to approximate this curve  if i do that that means i have learned from the \ntraining data so let us see \nrefer slide time sixzero \n \nnow we make an observation that such an arbitrary function can actually be \napproximated by a lot of something that we call as tower functions  these are all single i \nmean pulse functions which you have many of these  and you could have an \napproximation right and you can see that this approximation is bad at many places  but \nstill it is an approximation it largely gives you the same shape as the original curve what \nwould happen if i increase the number of such tower functions \nrefer slide time sixthirtyone \n \nstudent refer time sixtwentynine \nthe approximation would improve right if i keep increasing it the approximation would \ngo more and more better  right so now just try to keep things in mind whether  i write \nin the theorem right you can make it arbitrarily close to the actual value that means you \ncan keep doing something  so that your approximation becomes better and better and \nyou already see something of that sort  this is still in the sense of a figure  we need to \nrelate this back to a neural net work but you see that as  i am increasing these tower \nfunctions i become approx arbitrarily close to the actual function \nrefer slide time sevenfive \n \nnow this is what is actually happening right  i have multiple such tower functions  i am \nadding them up all of them are shifted in time so this tower function is actually this one \nthis tower function is actually this one and  so on right and  i have not drawn the \nremaining ones i am taking all of these tower functions adding them up and getting my \noriginal fun ction right and the more such tower functions have the better is the \napproximation \nrefer slide time seventhirtyfive \n \nnow you make a few observations right all these tower functions are actually the same \nwhat is the only difference they just shifted and their magnitude changes right  but they \nare all tower function right so let us think of this that if i know how to make a rectangle \nthen i can make any rectangle  right i just need to change the size of the rectangle and \nmaybe shift it or oriented differently  or something right so they are all similar  i just \nneed to learn how to draw a tower right that is what my quest is \nnow if i take the original input salinity  pass it through multiple such blocks each block \nis capable of making a tower function and ea ch of these would give me one of these \ntowers that i am looking for and  i am looking for  so many of these  right if i have as \nmany such tower makers then i could get these towers i could just add them up and then \nget the original function back and the more these i have the better is my approximation \nright so i am taking as input the salinity and trying to predict the  oil does this make \nsense still we have not figured out a neural network way of doing this we are still \nbuilding intuitions of how to do this \nnow our job now is to figure out what goes in this black box that is the tower maker and \nhow does it connect to neural networks if you figure that out then our story i s complete \nthen we know that a neural network can actually do this and that p recisely proves the \nstatement which i had made that it can  it can represent arbitrary functions  so we will \nfigure this out over the next few slides \nrefer slide time ninetwelve \n \nnow if you take the logistic function and set w to w to a very high value  what will we \nget just try to think about it the answer is already written but i want you to imagine it \nw covers what \nstudent refer time ninethirtyone \nthe slope right as i make w very high what  will happen is  i will get the  so let us try \nchanging the v alue of w  ok i just increase the value of w and see what happens to the \nsigmoid curve \nrefer slide time ninefortyseven \n \nsome error here actually there is some problem the w value should have increased and  \nthat is how the sigmoid slope increases not the b val ue the b value comes later on  so \nactually sorry about this  the w value as  i keep increasing  so do not  think that b is \nincreasing think that the w is increasing  it will become sharper and sharper and it will \ncome very close to the step function  right it will not become exactly the step function \nthat will only become in the limit but if i keep increasing i will get very close to the step \nfunction everyone agrees with this \nnow what happens if i increase the value of b  it will shift everyone is conf ident about \nthat can you tell me why \nstudent refer time tenthirtyfour \nwhat will shift actually the point at which the transition happens  right so what is this \npoint actually \nstudent refer time tenfortythree \nthis is the point at which i get that half value right and let us look at our function this is \na function when will i get that half value when w x plus b is \nstudent zero \nzero right so that means x equal to minus b by w  that is why it is proportional to b so \nas i keep increasing the value of b  this will keep shifting  ok is that fine everyone  ok \nwith this \nrefer slide time elevensixteen \n \nnow what if i take two such modified sigmoid functions which are shifted differently \nand both are very close to the step function right so here is where one threshold is here \nis where the other threshold is and now  i subtract this one function from the other  what \nwill i get \nyou know the term \nyou will get a tower right is that fine everyone gets this right so these places up to \nthis point both are zero so zero minus zero will be zero at this for this small range this is one and this \nis zero so that one minus zero and then afterwards both are one so one minus one would be zero so you \nget that tower function so now i have my tower maker \nrefer slide time twelvefive \n \nnow can we  come up with a neural network to represent this operation i want a \nsigmoid neuron i was working with a sigmoid neuron with some arbitrary weights right \nso that i recover that step function  can you imagine  now given x i want this tower \nfunction and that is exactly what one of the blocks was  right so what i am asking you \nis oh god so i am asking you to give me a neural network  for this can you think of it \ncan you try imagining it \ntwo neurons in the hidden layer how many of you agree with that ok can you can you \ntake some more time to imagine what it would be \nand i have already ok right \nrefer slide time thirteenseven \n \nso this w one b one if i set it appropriately i will get this step function if this w two b two i set it \nappropriately i will get this step function now i needed to subtract one from the other  \nright so i will do plus one minus one this is just a simple addition and i will get this is that \nfine everyone agrees with this  this is just a adder  right this is just an aggregator \neveryone gets this so now i have given you the tower maker  if you put enough of \nthese tower makers and learn the w\u2019s appropriately what will you get \nthat function that we were needed  so you can approximate it arbitrarily to any \nprecision that you want a s long as you keep increasing the number of these units  right \nso these units actually give  you one tower more of these units that if you have actually \nthis much this is the input ring  the more such tower makers that you have the more is \nthe bars that y ou will get and then you can approximate everyone gets the intuition \nbehind this fine ok this all is always good in one dimension \nrefer slide time fourteentwentyone \n \nnow what will happen in two dimensions what if we have more than one input  what \nis the tower there do you do you guys all do all know what is the tower there \nif you say no  i will give you a  zero on the assignment  remember the last question of \nthe assignment did you all make a tower  did you all make a twod tower did you all \ncopy that no so what if we have more than one inputs suppose you had again trying to \ntake a decision about whether we will find oil at a particular location of the ocean  right \nand suppose now we base it on two two right so say this is  salinity this should be x one \nshould be x two should be y and this is pressure \nnow just observe about the red and blue points  so the red points are where you will \nnot for those configurations of salinity comma pressure you will not find oil and the blue \npoints are for which you will find oil what is the one thing that you can tell me about \nthe red points and the blue points  not linearly separable right but we still want to be \nable to learn this is that fine a single perceptron cannot do it  i will also make a case for \na single si gmoid neuron cannot do it and then  i will show you that  in fact first i will \nshow you that with a network of neurons we can do it and then  i will show that  with a \nsignal single sigmoid neuron you cannot not actually do that \nso now this is again a vali d problem you could have we could imagine that you will get \nthis kind of data where you have two factors and your function is some arbitrary function \nof these two factors it is not a need linear boundary between the blue and red points  \neveryone sees that  the blue and red points are not linearly separable you cannot draw a \nplane such that all your red points lie on one side and the blue points lie  on the other \nside everyone sees that ok  but the solution which  i have plotted here  that is  a good \nsolution it makes sure that all the red points are in this region and the blue points are \noutside \nso it will predict a high value for these red points and a  zero value everywhere for the \nblue points is that obvious how many of you understand that figure ok good \nrefer slide time sixteenfortysix \n \nso now i want to show that even in twodimensions i could come up with arbitrary i could \ncome up with a neural network which could actually approximate this and again what \nwill i look for a tower maker right i just want something which can make towers and \napproximate it \nrefer slide time sixteenfortyeight \n \nso this is what a twodimensional sigmoid looks like slightly incorrect because i have what \ni have done is  i have actually said w two to zero so if you actually i would want you to do \nthis go back and plot this for w one equal to three and w two equal to three \njust go back and plot this and see what  you get you will not get such a smooth such a \nnice looking s but you will still get something which looks like looks like a snakes \nhood right so in still get that s shaped function it just that it would be bent at some \npoints and it be thinner at some points and broader at the other points  so just go back \nand see and then you will realize what is happening right \nrefer slide time seventeentwentynine \n \nso here again what we want to figure out is from the single sigmoid  i was able to take \nyou to a tower function  right from a twodimensional sigmoid what does a tower mean \nhere and how do i take you to the tower  so that is what i want to do so i have said w \ntwo equal to zero and i will it will become obvious why  i have done that so just understand \nwhat the figure is doing right so this if you just look at this is like the cross cut  right \nso you are looking at the front view of this figure and  that is just the sigmoid function  \nwithout the w two right and now since i have said w two equal to zero no matter what i set x two \nto the same function will get repeated throughout that axis do you get that \nso that is why this entire function is just getting repeate d throughout this axis and then \nyou just get a similar s shape function everyone gets that how many of you do not get \nthat how many of you get that so this if you look at the front view this is the sigmoid \nof one variable  but since i have said w two to zero no matter what  i change x two to the \nfunction is going to remain the same so it will just get copied throughout the x two axis is \nthat fine with you \nnow what will happen if  i increase w two sorry w one same thing right  it will just keep \nshifting till it becomes almost like a twod step function ok now what will happen if  i \nincrease b shift i can do the same thing here also same logic applies here also ok \nrefer slide time nineteentwelve \n \nnow what is the next step that i am going to do take two of these which are shifted by \nsome point and then subtract what will i get everyone had this figure in mind so just \nsee right so this portion both are zero so zero minus zero would be zero this portion this is one but \nthis is zero so that would be one minus zero and again in this portion both of them are one so one \nminus one would be zero so you will get this kind of function would you like to live in such \na tower i am very serious  yes or no no why it is open from two sides right you \ncannot live in this tower so you want something which is a closed tower right so how \nwill you do that give me an intuition \nwe will do the reverse thing what will be set to zero \nw one ok \nrefer slide time nineteenfiftyeight \n \nand this is how it would look the orientation would change and again so notice that this \nis your sigmoid function and since  i have set x one to zero no matter what i change along the \nx one axis the same function gets copied and you get a nice looking a sigmoid function \nnow again i will do the same thing  i will increase the w  i will get a close to a step \nfunction i will increase the b i will move along this axis \nrefer slide time twentytwentyfour \n \nnext step take two of these subtract get what  another tower function this is also not a \ntower that you like to stay in  so what do i do now add them sure add this tower to \nthe other tower \nrefer slide time twentyfortyfour \n \nso now what will happen if  i add these two will you get a tower function  what will \nyou get \nyou will get a tower function with a parking floor  right is that what you will  get \neveryone understands why this is  so these portions both are  zero so you get zero same \nlogic applies for all the four corners right is that fine now for this portion or rather this \narea right so this guy is zero this guy is one so you will get a one the same logic applies for \nall these four corners in the centre both are actually one so one plus one would give you two so \nthis is two this is level two this is level one this is level zero is that fine \nso what am  i done so far i have taken my x  one x two passed it through some \ntransformations right this what are these transformations we will see  but transform it \nthrough these multiple hoops right where i adjusted a w\u2019s and b\u2019s and i have got some z \nright and this is how that z behaves  for different values of x one comma x two i will get \nthese different values and these values range from  zero to one to two is this pictured clear i \nhave taken my original x one comma x two passed it to some of these transformations and \nirrespective of what my x one to x two is this tells me the entire range of values that i will get \nfor some combination of x one comma x two i will get zero for some combinations i will get one \nfor some combinations  i will get two and some combinations also between one and two right \nso these places where it transitions is that clear is that picture clear to everyone \nso now i can treat this as the output z ok now from here how do i go to a tower \nthreshold it how will you threshold it  what do you want  you only want this much \npart to exist right this without the pa rking floor how will you do it  any output which \nis greater than equal to two you want to keep it any output which is less than two you want \nto make it zero if i do this will i get a tower right sorry greater than equal to one any value \nwhich is greater than  equal to one you want to keep it  anything which is less than one you \nwant to make it zero so this entire thing will get demolished  how do you do this this is \nan if else ok \nif else if something is greater than equal to  zero do something else do something else what \nis that \nperceptron right but we do not  want to use perceptron  we want to use sigmoid \nneurons have you learned an approximation from a sigmoid neuron to a perceptron  \nvery high w right you get the intuition let us see what we do on the next slide \nrefer slide time twentyfourseven \n \nso i take this any z  which comes from here  i will pass it through a sigmoid neuron \nwhich are very high slope such that the threshold is at one anything which is greater than one \nwill pass out as one anything which is less than one will go to zero so everyone sees how we \ntook this structure and converted it to a tower  we have this tower now  now what do  i \ndo with this \nrefer slide time twentyfourthirtyfour \n \ni lead multiple such towers and  i can approximate this  i could put a tower here here \nhere and so on i could have these multiple towers  and here of course  all my towers \nwould be of zero height right in this region  right so now i can cover the entire twod space \nwith a lot of tower functions and approximate this exactly that is a very weird statement \napproximate this exactly  i mean approximate this to arbitrary precision  everyone gets \nthis do you see why we constructed these tower functions and now we can put them \ninside this cone and approximate it \nrefer slide time twentyfivenine \n \nnow all this is fine  i was making some towers there  so can you now give me a \ncomplete neural network which does this i want you to imagine that remember you are \ntaking what i am asking you to do is this x  one x two give me this such that  i get this two\ndimensional tower i do not  know how to draw it  something like this ma ybe whatever \nso i want this twodimensional tower what is this network of perceptrons going to look \nlike just go back to all the operations that we did and try to imagine in your mind \nno we will not use perceptron because we can always use a sigmoid neuron instead of a \nperceptron with the high w i do not expect you to answer this i just want you to imagine \nright we just try with a there is something known as a pen  there is something on a \npaper ok so here is the solution \nrefer slide time twentysixeight \n \nso what is happening here you have this salinity and oh actually this is slightly wrong  i \ndo not know why you guys saying  it is correct actually at both places  i need both the \ninputs it is just that in one case  i do not care about that input because  i have said w two to \nzero so i learn these weights w one wtwo b wone wtwo b of course here the network should learn \nthat w two is equal to  zero right and then you get this one tower  do not needs this to be \nmodified this figure is incorrect \nso we need x one x two both as inputs we need to label it with w one w two equal to zero and b and \nso on it so we will discuss this later  anyways but you get the idea right that you take  \nthese two inputs make one tower take the inputs again make another tower add them up \nto get this function  pass it through this step neuron function step sigmoid function  so \nthat you get the tower  so this is one block  you will have many such blocks each of \nwhich will learn different w \u2019s and b\u2019s so that they get shifted and then you will place \nthem all together you have an aggregator on top of this which will combine them  just a \nminute how many of you get this ok good \nyes so that is  a good question  i am going to come to that  right so i have very \nconveniently given you a solution where  i have what is the bad thing that  i have done i \nhave hand coded these things  right i have hand coded w ones w twos and b \u2019s is that \nfine in practice no i mean that is where we started off and we do not want to hand code \nthese right \nso now you know a learning algorithm for a single sigmoid neuron  now what you \nhave is a network of neurons right  for this network of neurons  i need to give you a \nlearning algorithm driven by the objective function that whatever output it gives would \nbe very close to that arbitrary function that you are trying to model \nif i give you a learning algorithm then you would be convinced that if this has to be \nminimized and the weight configuration which need it needs to a rrive at as w  two is equal \nto zero then the algorithm should be able to do that  right because we saw we have some \nfaith in these algorithms in the case of a signal sigmoid neuron that with the right \nobjective function it will give me a principled way of rea ching that objective function in \nthis big network my objective function is to arbitrarily to approximate this  of this true \nfunction right \nso now if i give you that as the objective that whatever outputs the network generates  \nso the network might gene rate something like this  so that has to be very close to the \ntrue output that is the objective function that i am going to use in that learning algorithm \nand if that learning algorithm works which will prove then you should be able to arrive at \nthe necessary weights to make this approximation  right is that clear and  in fact there \nmight you might not even have to do these multiple towers in practice  all i am trying to \nprove is that there is one solution which exists \nif there is one solution which exists i can say that locate the network can learn that is the \nonly claim i make i am not saying this is the only solution  right same as in the case of \nthe boolean functions where i said that one solution exists where you have to raise to n \nneurons of the  hidden layer  that was a sufficient solution  that was not a necessary \nsolution for the and function we were actually able to do it with a single sigma neuron  \nright so just keep that in mind i am just giving you a sufficient solution \nand the network c ould actually learn something better than this all right this is again a \nvery bulky solution why it scales with the number of neurones\u2019 proportional to number \nof input variables that you have  so that is for a sufficient solution but you would want \nsomething better than that all i am trying to say is that it can  approximate i am just \ntelling you the representation power and just as we had the catch there that the hidden \nlayer is very large the same catch applies here also is this story clear to everyone \nso i have given you a solution i have not told you how to learn the weights  i have given \nyou a network now later on we will discuss a learning algorithm for this network  and \nwe will have some confidence that given a particular objective function th at learning \nalgorithm can strive to go to minimum error or minimize the quantity of that objective \nfunction that is going to come in two lectures from now is that fine \nrefer slide time thirtythirtytwo \n \nand that was for the tower function now i could have actually directly done this right  \nso i wanted to approximate these functions  so i could have placed a lot of these kinds \nof things here and approximated it right so that instead of that very high slope sigmoid \nfunction i could just use a normal sigmoid  function also ok and again there is a error \nhere but i hope you get the picture it is just that you feed both the inputs to them \nrefer slide time thirtyfiftysix \n \nso for one dimensional input we needed two neurons to construct a tower for two dimensional \ninput how many neurons did we need  i am just counting these because these are simple \naggregators right and this is one constant at the end  so how many did we need actually \no of two n i mean o of i mean so for n how many would we need let us try to work that \nout ok so i will ask you that in the quiz how many do we need for n dimensions \nrefer slide time thirtyonethirty \n \nnow why do we care about approximating any arbitrary function  we will again try to \nclose the loop now  we saw that we can arbitrarily we can  approximate any arbitrary \nfunction but now again i want to come back to the point why do we want to do this and \ncan we tie this back to the classification problem that we were dealing with \nrefer slide time thirtyonefortyseven \n \nand this is the data which  i had gi ven you which was there were some points some \nvalues of x and y sorry this should be x one and x two it is where this is pressure and salinity \nor salinity tendency and this is the output which is oil \nnow there was this is what the function actually looks l ike now what would have \nhappened if i had used a single sigmoid neuron to try to approximate this function try to \nrepresent this function and sigmoid neuron in two dimensions right so the two dimensional \nsigma what would have happened  can you give me one s olution for this  remember \nearlier i had said that perceptron cannot handle data which is not linearly separable  but \nthen i anyways used it for data which was not linearly separable  and we got some line \nsuch that we got some errors the red points and the  blue points are not clearly separated  \nso i am asking you for a similar thing here  i force you to use a sigmoid neuron  what \nwould you give me \nrefer slide time thirtytwofortyseven \n \nis this fine  this is one of the possibilities of course  it could have been orie nted \ndifferently and several things  what is happening here is that for these blue points it is \nacting correctly but for these red points it is not acting correctly  i am assuming red is  \npositive and blue is negative  i think that should have been the oth er way round  but let \nus assume red is positive and blue is negative again now for these red points this part is \nworking fine but it is misclassifying all these blue points \nso all these bad locations is actually saying that you can find oil and for al l these good \nlocations here it is saying that you cannot find oil  that is what a sigmoid neuron would \ndo and you could have multiple solutions are possible here right  but all of them would \nhave this problem that will make errors on some red points and so me blue points right  \nbut the true solution that we wanted is something like this  again there are multiple \nsolutions possible right you could have anything there are  you could have even finer \none side you could just have this much  there many things poss ible this is one such \nsolution what the illustrative proof told you is that  you can actually use a network of \nperceptrons and approximate this arbitrary function which exists between the input \nvariables and the output variable \nso if this is the functio n which exists between the input variables and the  output \nvariables now you could take these multiple two dimensional tower functions and \napproximate it with the catch that you might need many of these in the hidden layer but \nyou can still do that  ok so that is  why this in theorem important because now any \nproblem that you take right any problem that you will have in machine learning would \nalways want you to take an x learn a function of x which takes you to y this function will \nbe have some the function will have some \nparameters right and now what this theorem is saying is that you could adjust these \nparameters such that you can arbitrarily come close to the true function  right so that is \nthe significance of this any machine learning problem that you can think of in the sense \nof classification or regression you would find that this is useful and  i am giving you a \nvery powerful tool to do that  of course with the catch that  i am not giving you any \nbound on the number of neurons that you will need  i am just saying use as many as you \nwant"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.1 Feedforward Neural Networks (a.k.a multilayered network of neurons).wav", "duration": 1085.2, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  four \nfeed forward neural networks backpropagation \nso welcome to lecture four of csseven thousand and fifteen  the course on deep  learning today we  will talk \nabout feed forward neural networks and back propagation so quick recap of the story  \nso far it so we started with mp neurons we saw there were some problems with the mp \nneurons they  could  handle only  boolean  inputs  and boolean outputs  and  threshold \nneeded to be hard coded so from there we moved on to perceptrons which allowed for \nreal inputs real outputs and sorry real inputs and binary outputs and we also learned an \nalgorithm for  learning these  weights and parameters right so  we need there was  no \nneed to hand code these parameters anymore \nbut then we  found that for  a single perceptron there is a  limitation it cannot it can \nonly deal with functions which are linearly separable so then we went on to a  multi\nlayer network of perceptrons  and we  proved by illustration that it  can handle any \narbitrary boolean function whether linearly separable or not the  catch is that you will \nneed a large number of neurons in the hidden layer  right then we also observed that \nperceptrons have this harsh thresholding  logic  so  which  makes  the decisions  very \nunnatural it is zerofortynine it  is negative  zerofiftyone is positive  so you wanted something  more \nsmooth \nso \nthe smoothest approximation to this  step function  which is the perceptron  function \nwas a sigmoid function sigmoid is a  family of functions and we saw one such function \nwhich was logistic function and then we  saw that it  is very  smooth now it is \ncontinuous and differentiable \nnow for the sigmoid neuron on a single sigmoid neuron we saw a learning algorithm \nwhich was  gradient descent and  we  proved  principally that  it will  always go in the \ndirection where the loss decreases right so that is what is the basis for gradient descent \nand then we  graduated from a single neuron to a  network of neurons and made a case \nthat such a network of neurons with enough neurons in the hidden layer can approximate \nany arbitrary function right ok so i have told you that it can approximate any arbitrary \nfunction what does that mean and what is the thi ng in the network that does all this \nall the tower functions and the tower functions depend on weights and biases so there \nin that illustrative proof again we were adjusting the weights and biases by hand right \nwe knew that we wanted these very tiny tower functions and we were doing it \nnow from there where should we go  \nstudent refer time twothirtynine \nwe need an algorithm to learn these weights and biases right so that is what back \npropagation is so today i am going to formalize these feed forwa rd neural networks \nwe just did it by illustration the other day i will introduce you to the terminology and \nsee what the input outputs are and so on and then  we will look at an algorithm for \nlearning the weights in this feed forward neural network \nrefer slide time threethree \n \nlet us begin so this a lot of this material is inspired by the  video lectures by hugo \nlarochelle on back propagation he has a course on neural networks \nit is available on  youtube you can check it ok so let us first begin by  introducing \nfeed forward neural network right \nrefer slide time threeeighteen \n \nso what is a feed forward neural network the input to the network is an n dimensional \nvector so ok  that means my input belongs to r n that fine the network contains l \nminus one hidden layers where do you already know what hidden layers are right we \nhave been defining that terminology since  multi layered perceptron so you have these \nhidden layers and there are l minus one of these and then it has one output layer containing \nk neurons ok those are the feed forward neural network looks like what is missing \nhere  \nstudent refer time threefiftyseven \nthe weights right \nso each neuron in the hidden layer ok before that each neuron in the hidden layer and \nthe output layer can be s plit into two parts right  so i will call the first part as the pre \nactivation and the second part as the activation have you seen this plate before right \nwhat does the pre activation do  \nstudent aggregation \naggregation and what does the activation do  \nstudent nonlinearity \nnonlinearity right so we have this pre activation and activation at every layer and a i \nand h i are vectors is that correct because this entire thing or rather this part is h one and \nthis part is a one both of these are vector s right and for this discussion am going to \nassume that everything till here belongs to r n \nso the input was r n and all the hidden layers also have n neutrons is that fine so \nplease pay a lot of attention to this couple of slides because this is go ing to stay with us \nfor the rest of the lecture and perhaps two more lectures and even for the course alright so \nthis is very important that you understand this the way we are defining a feed forward \nneuron network \nrefer slide time fiveeight \n \nthe input l ayer can be called as zeroth layer what i mean by that is t hat i could refer to \nthis as h zero ok there is no a zero h zero here because there is no pre activation activation \nyou are just given the input so i just call it as h zero ok and the last layer can be called as \nh of l right whatever you get from this green part you  will call it as h of l ok what \nis the dimension of h of l r raised to k it belongs to r k because i have said here that \nyou have k neurons each corresponding to k classes ok \nnow we have weights between the input layer and the first hidden layer now can you \ntell me this belongs to r n this also belongs to r n so what is the dimension of w one n \ncross n right because it contains weights for connecting each of these inputs to e ach of \nthese hidden layers there are n here n there right so it is n cross n \nand what a re the dimensions of the bias n  one corresponding to each of the hidden \ninputs fine and this is only for up to this layer because till here i have assumed \neverything is n \nrefer slide time sixtwentythree \n \nnow what about the output layer n cross k and the biases k k dimensional ok so this \nis what the network looks like but now i have to give you some function so  i have \njust i have shown you a diagram but what  does it mean mathematically because \nremember that we are always interested in writing something of the form y is equal to \nfunction of x right and that is not well defined yet \nrefer slide time sixfortynine \n \nso let us start defining that ignore the red portion for now ok i will go over it so each \nof these activations right or rather the pre activations is given by b i plus w i into h i \nminus one so what it means is that these activations take inputs from the previous layer \nmultiply by them by weig hts and also add the bias is that clear so let us see it right \nfor example if i look at a one which is this vector so that is three dimensional and assuming \nit is three dimensional for simplicity \nso it is a one one a one one a one two a one three right and that is equal to how do you get rid off this b \none one b one two b one three plus this matrix multiplication is this clear to everyone i know it is trivial \nbut am still going over it right so let us not ok and then how do you do this matrix \nmultiplication row was multiplied by the column so this is what you get right and in \nthe end i can write it as this right and this looks very similar to what we have been \nseeing throughout it from a mp neuron to perceptron to sigmoid neuron and now this \ncase right \nso it is just an aggregation of all your inputs or weighted aggregation of all your inputs \nthat is the case which i want it to know  and that is obvious now so you understand \nwhat these are right \nso this is r n in our case we have assumed n equal to three what is this  i will keep \nasking till this is completely fine with everyone r n and this is \nstudent refer time eightthirty  \nn cross n and this is \nstudent refer time eightthirtyfour  \nn cross one n cross n i mean r n sorry is it fine  so everyone understands the operation \nhappening here it is a weighted aggregation of your inputs so every guy here is a \nweighted aggregation of all the inputs ok \nrefer slide time eightfortyseven \n \nnow after that i do h i of x is some function of a i of x ok what does this mean so \nthis is again a vector right i have assumed that it is three dimensional so these are the three \nelements of h i so these are the three guys now these are some function of these light blue \nguys ok now how does that function operate on the vector it operates element w ise \nnot all functions on vectors are element wise but this particular function we are going \nto do element wise \nthat means that h one one is equal to g of a one one h one two is equal to g of a two and h one three is equal to \ng of a one three right where if i take g of a one three  one of the functions that i could choose is the \nsigmoid function so it would just be one over one plus e raise d to minus here so what is \nhappening is i am taking this value and passing it to the sigmoid function to get oh sorry \nam taking this value and pa ssing it to the sigmoid function to get h one one taking this value \npassing it to the sigmoid function to get h one two right \nso the key thing to understand here that this is a element wise operation right it is not \noperating on the vector that does not make sense it is operating on every element of the \nvector right ok and g is called the activation function \nrefer slide time tentwo \n \nit could be logistic tan h linear anything right so we will see some of these functions \nlater on ok \nnow the activation at layer i sorry they are supposed to be activation at the output layer  \nthe activation at the output layer is given by the final function which is f of x is equal to  \no of a of so let us see  so this is a three in our case l was equal to three because we had l \nminus one hidden layers and the lth layer was the output layer right so this is a l so this \nis what i have computed here  that light green part of the figure that you see right now \nbased on that i want to produce an output \nso that is someone h ad asked me a question that why do we always choose sigmoid \nbecause sigmoid will clamp the output to zero to one what if i want to predict the amount of \noil which will not be between zero to one right that is why for the output we will use a \nspecial function that will call the output function and later on i will show you that i t \ndepends on the task at hand  so it is going to change with the task  that we are going to \ndo right so we are just going to say that the final output which is h of l is equal to \nsome function of the pre activation at that layer  is this terminology clear to everyone \nhow is each function operating is that clear to everyone \nrefer slide time eleventwentytwo \n \nand we will see some examples of the output activation function right  now just f or \nsimplicity am going to remove the x\u2019s from the brackets right s o instead of calling \neverything ai of x h i minus of x and  so on i will just call them ai h i and so and so that \njust simplifies things but we know that everything is a function of x be cause x is the \ninput and that passes through some functions and we get the final output right so this is \nthe notations that we are going to use is the dimension of everything that you see every \nvariable that you see here completely clear to everyone \ndimension of ai bi w hi x everything is clear ok and the output layer has a slightly \ndifferent dimension than the other layers because there we have k classes as opposed to \nn neurons everywhere else ok fine now i need to put this in the paradig m that we saw \nfor supervised machine learning what were the five components there data \nstudent model \nmodel \nstudent parameters \nparameters \nstudent learning \nlearning algorithm \nstudent refer time twelvetwentynine  \nobjective function right ok everyone remembers that ok \nso i said that we will do deep neural networks and we are trying to write this y hat as a \nfunction of x but then what i gave you is just a diagram from which this is not clear \nwhether y hat is actually a function of x how many of yo u think y hat is actually a \nfunction of x very few ok \nrefer slide time twelvefiftytwo \n \nso let us see what exactly is o ur model assumption here right  so the question let me \nrepeat the question just to be clear so i said that they are given some data we do not \nknow the true relation between y and x we make an assumption that y is related to x \nusing some function f right and it is has some parameters and then we like to try to learn \nthe parameters of that function so what is the function here \n so what is your model what have you assumed as the model can you write y as a \nfunction of x if yes what is that function how many of you have the answer i think \nyou have your answer ok i think i cannot wait more so i will give you the answer \nthen it will become very obvious ok so this is how y is a function of x right so let us \nsee what is happening i took the original x which was this i transformed it added b one \nthat was the dash at layer one  \nstudent refer time thirteenfiftysix \nno this thing \nstudent refer time fourteenzero \npreactivation at layer one i passed it through the activation function right ok \nnow again let us be clear about the dimensions what is the dimension of this  \nstudent n \nn what is the dimension of this n cross n so what is the dimension of this product  \nstudent refer time fourteentwentyone \nn what about this so what is the product the final dimension of this \nr n now you are passing it through a function g that function is operating element wise \nso what is the output dimension \nstudent r n  \nr n so this is again r n ok now this \nstudent refer time fourteenfortytwo  \nso now you see the whole story right so now this n cross n guy multiplies with this n \nguy again you get a vector again pass it thr ough a non linearity was it s o hard  it is \nobvious now right you just take an x just note down all the transformations that you \nhave done that is what a function does right it passes it through the  through first a \nlinear transformation this is a linear transformation then a non linear transformation \nthen again linear nonlinear and so on \nso just see how far we have come from where we started off right we started off with \nsimple things like w transpose x right that was the perceptron model where we were \ntaking decisions ba sed on w transpose x and we were saying y is equal to one if this \nquantity is greater than something y is equal to zero if this quantity is greater than \nsomething right that is why we started off with we made it slightly more complicated by \ndoing this this was sigmoid neuron \nnow from there where have we gone to this right so we have increased the \ncomplexity of the network with great complicity complexity comes great \nstudent refer time fifteenfortysix  \nno power right we have already seen the representat ion power of deep neural \nnetworks right so it comes from this complexity that you have you have a lot of linear \nand nonlinear transformations right that adds to the complexity of the network it has \nmore parameters at each linear transformation you h ave some parameters and you are \nalso using a lot of non linearity so that is the reason why deep neural networks are  so \npowerful right do you get that ok so just to impress again right \nso any machine learning algorithm that you have you should be a ble to write it in this \nform right that y is a function of x with some parameters and then your job boils down \nto learning these parameters right it just happens that here y is a very complex function \nof the inputs is that clear ok so i am not devi ated from the original story i am still \nbeing able to write y as a function of x with some parameters ok what are the \nparameters  \nstudent refer time sixteenfortytwo \nall the w\u2019s all the b\u2019s right so w one to w l and b one to b l \nrefer slide time sixteenfortyfour \n \nand the algorithm that we are going to see today for learn ing these parameters is called \ngradient descent but we will use it with back propagation where back propagation will \nhelp us to compute gradients it is ok  it does not it does not make sense at this point \nthat is what the lecture is supposed to be about right so and what is an objective \nfunction  \nstudent refer time seventeenseven \nloss function so i could just go with this loss function right  ok there is an error here \ni thought we corrected this there is a summation so actually these are vectors right \nso this does not make sense so you should hav e summation j equal to one to k yij minus \nyij does that make sense so this is the vector y hat ok for the i th example it will be \ncalled as y hat high i which will have k elements right so y hat i one y hat i two up to y hat i \nk right \nso that is what my predictions are and i will have the corresponding true vector also i \nam trying to take the difference between them which is going to be a n element wise \ndifference everyone understands the error in the slide how many of you do not get it \nhow many of you get it if you do not get it please raise your hands it is a minor thing \ni can correct it and how does deep neural networks fit into these this paradigm"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.10 Information content, Entropy & cross entropy.wav", "duration": 2627.02, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  fourtwo \nlecture  four \nlearning parameters of feedforward neural networks intuition \nnow we will move on to the nex t module where we wa nt to lea rn the parameters of \nfeed forwa rd neural network s a nd we first star t with some intuiti on a nd then \nmathematical details \nrefer slide time zerotwentysix \nso we have introduced feed forward neural networks and we are now interested in \nfinding an algorithm which can allow us to learn the weights of this network \nrefer slide time zerothirtythree \n \nso recall our gradient descent algorithm this is how it looked ok i had initialized  those \ntwo parameters w naught b naught and then i was iteratively doi ng this in a loop at every \nstep i was moving in a direction opposite to the gradient at that step \nnow can i write this a bit more compactly we can write using vectors  \nrefer slide time zerofiftynine \n \nso are you ok if i write it this way so these two was actually nothing but vector at every \npoint so i can just write it this way so theta is  the vector containing w and b  ok or \ntheta is the vector of all the parameters my network had it just so happened that \nnetwork had only two parameters so see where am going with this how many of you see \nwhere am going with this \ngood so where delta theta t  right just to remind you it was this the partial collection of \nall the partial derivatives with respect to all the parameters in this toy example all was \nequal to two right we just had two parameters now you see where am going with this ok so \nnow in this feed forward neural network instead of theta equal to w comma b what do \nwe have theta is equal to so many parameters ok so what would grad of theta t now  \nbe partial derivatives with respect to \nstudent refer time onefiftyeight \nall the weights but there is a problem here right this is the matrix how do you take the \npartial derivative with respect to the matrix who asked you to use the matrix how you \ntake the partial derivatives with respect to matrix so what i am interested in this right \nthe question i know there is some loss function which is a function of theta one of the \nelements of theta has this matrix w one which belongs to r n cross n right and now i want \nthe derivative with respect to w so see what i am trying to do this is scalar and we take \nthe derivative of that with respect to a matrix what is all that the derivative with respect \nto \nstudent refer time twothirtyseven \nevery element of the matrix ok \nrefer slide time twofortyone \n \nso we can still use the same algorithm except that del this grad of hat of so now i could \njust say that theta two hat i mean initialized all parameters and theta naught right \ncompute the gradient with respect to all of  them and then do this update right i could \njust instead of putting them in matrices i could just think of them as a large vector just \nhad initially i had just had w comma b now this vector is even more large in fact i will \nshow you actually how it is \nrefer slide time threeeight \n \nso this is the grad with respect to  theta looks very nasty now this is how nasty looks \nright so you have this weight matrix w one you have the derivatives with respect to first \nelement of w one all the way up to the last e lement last element so with respect to all the \nn cross n elements of w one what is the next entry going to be w two hundred and eleven to \nstudent refer time threethirtytwo \nwtwonn next after wleleven ok and then after this ok \nstudent refer time threefortyone \nwhat is remaining biases  right so you have bone one to bone n this slight error here but \nintentionally this actual is k because k is not equal to n right the last layer has only k \nparameters whereas  so that it looks ok  is this clear so is this are all the partial  \nderivative that we need right you do not need to worry about taking a partial derivative \nwith respect to our matrix it just boils down to taking the partial derivative with respect \nto all elements of the matrix \nso earlier you just had two parameters now you have these n cross n plus n cross n upto l \nright so l into n cross n plus l  into n that many number of parameters is what you \nhave you get the calcula tion right or rather you have l  minus one layers each of which \nhas n cross n parameters right and l minus one layers which also have the biases so these \nare the w\u2019s these are the b\u2019s then the output layer one layer which has n cross k \nparameters and k cross one bias so these are all the number of parameters that you \nhave and this is exactly what this size of this m atrix is right it has all these parameters \nand you need to compute the partial derivative with respect to each of these parameters  \nrefer slide time fourfiftynine \n \nso this is what grad theta is composed of it is composed of the partial derivatives with \nrespect to all  the parameters of your network  ok so now if someone gives you each of \nthese quantities same oracle give you each of these quantities then can you apply \ngradient descent r ight you can use the exactly the same algorithm that you are using \nearlier just the sizes of earlier vectors changes \nhow many of you are convinced that now you can use that gradient descent there is not \na trick ques tion how many of you convinced  how many of you not convinced \nassuming that someone has given you these q uantities right i know that it is hard to \ncompute we will see how to compute that but let us assume someone has given you this \nthen you can use gradient descent that is what the case i made in the previous slide right \nthat you could initialize with all  the parameters compute the gradients with respect to all \nthe parameters and just do this update fine so now we need to answer two questions  first \nis this is the key question \nrefer slide time fivefiftyfive \n \nbecause we are taking derivative of what  loss functions so we need to know what the \nloss functions that is the crucial question right  and then we are taking derivatives with \nrespect to all these elements so whatever i was told you that assume that oracle gives \nyou now you have to do the har d work and actually find it out  right so if you can \nanswer these two questions then we are done we have an algorithm for learning the \nparameters of feed forward neural networks we all agree that if you have these two \nelements then we have done \nso here i will end this module"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.2 Learning Paramters of Feedforward Neural Networks (Intuition).wav", "duration": 376.8, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  fourthree \nlecture  four \noutput functions and loss functions \nwe go on to the next module where we will be talking a bout output functions and loss \nfunctions  \nrefer slide time zeronineteen \nthe question that we are going to focus on is how to choose the loss function but i will \nshow you that it is tightly coupled with the choice of the output function also remember \nthat we had said that we have a special o function as the output function i have not told \nyou what that o is and now that is what we are going to define \nrefer slide time zerothirtythree \n \nnow the choice will be loss function actually depends on the problem at hand and that \nis exactly the question which had come up right that in some cases it is  to have sigmoid \nas the output function because your values are between zero to one but whatever there are \ncases where yo ur output is not between zero to one  right so it definitely depends on th e \nchoice of the on the problem that you are trying to solve so we will illustrate this with \nthe help of two examples and these two examples will cover a broad range of problems that \nyou will encounter or if you are working in machine learning right \nso the first problem is again you are given the input as movie you are using a neural \nnetwork with l minus one hidden layers and an output layer y hat right  so this is sorry \nthis is a true one so you have an output layer and the output layer is going to predict the \nimdb rating the critics rating and the rotten tomatoes rating \nis that fine ok so what kind of problem is this peo ple have done machine learning  \nthis is a  regression problem and notice that the output values that you want to predict \nare not bounded it by zero and one they are still bounded by one to ten but in general you could \nimagine that there could be problems so there are no bounds at all right it could be a \nvery large number is that clear now here yi belongs to r three \nso remember in all these cases we were assuming that we just want to predict one value \nbut nothing stops you from predicting multiple values at the same time so your output \nis now three dimensional you are taking an n dimensional input and trying to predict three \nvalues from it ok  fine the loss function should capture how m uch yi had deviates from \nyi ok so this is a valid or maybe we corrected on this way ok so this is the formula \nwhich was supposed to be in there right so you take you have predicted three values and \nyou kno w the true three values you just tak e the difference between these r ight is that \nclear the first element of the predicted value minus first value of the actual value and \nso on for all the three values that you want to predict \nrefer slide time twofortytwo \n \nnow you have a loss function but what should be the output function in this case can \nit be the logistic function yes no it will be bounded between zero to one and you know that \nyour output cannot be bounded between zero to one ok so in such cases then what is a  good \noutput function to use one option is to scale it so i will keep that aside why do that \nit is unnatural and you are actually clamping it and  then trying to scale it right  so can \nyou do something more natural in that just use a sum which is linear function right so \nwhat we could do is you could have o as a linear function \nso what that means is  again remember that this is a of l ok and i know all the \ncomputations that have happened so far a linear transformation non linear linear non\nlinear and then again linear so i have computed a of l  from that i want to compute the \nfinal output right so i could just have it as a linear function of the input which is a of l \nin this case \ndoes it make sense how many of you feel it makes sense  ok why because now it is \nno longer bounded  right you could this linear transformation your weights could be \nadjusted in any way to get a value whatever you wanted whether you wanted between one \nto ten or one to one hundred or one to one thousand these weights could be adjus ted to do that right so at \nleast you are not bounding it and it is free to learn what is the range from the data it \nshould be able to run but how should you adjust these ws \nso that you get the desired range now tell me why would it not happen that you learn \nws you start predicting values like one thousand ten thousand and so on in this particular case where \nyour input is bounded by one to ten sorry your output is bounded between one to ten  why \nwould it happen i this is my  argument and you prove me wrong  right i would say that \nif you have chosen a linear transformation which is not bounded i the n network could \nlearn weights which start producing a rating of  ten thousand twenty thousand and so on because it is not \nbounded \nbut you know that that is wrong because the ratings can onl y be between one to ten so why \nwould that not happen because you are  minimizing this loss function r ight so if you \nstart predicting values like  ten thousand when your actual rating was nine then you have a  ten thousand \nminus ninety whole squared loss that is a very high los s so it will start moving y ou away \nfrom that configuration right so the training is always guided by the objective function \nso if your training happens well it will try to prevent this \nnow suppose let us take a simple thing rate that  you are given a our same ball example \nfor probability so you are given an urn which has balls of three colors say black white and \nyellow \nrefer slide time fivethirtyeight \n \nand you have to put the balls in that so you know that the true probability distribution \nis actually zerothirtyfive zerotwentyfive and zerofour for red  black and white ok  this is the true probability \ndistribution you have put s ay thousands of balls in urn  now what you do is you just \nallow me to peep into the urn or you allow me to take some samples from there you tell \nme take these one hundred samples and you ask me t ell me what this probability is right  so \nthis is the true probability that you know is true right because you know it because you \nhave estimated \nnow you just give me a small sample from there and ask me to esti mate it and based on \nthat i actually estimate this ok so there was a true probably distribution and an \nestimated probably distribution now i want to find out how wrong i went right \nafterwards you tell me the answer you tell me that this is what the t rue was and this is \nwhat you predicted \nnow i want a way of c omputing how wrong i was right  so how do i do that you \nalready know this and these are two vectors what can i do  you could just do the  this is \nvalid anything wrong with this in principle n o you could just treat these as any two \nvectors you have a true value you have a predicted value you just take the square d error \ndifference between them  right but you know this is a probability distribution right \nyou should be able to do something better  than this you know this is a special quantity \nthis is not just any number that you are predicting you are trying to predict a \ndistribution so you should be able to do s omething better than that right  so that is \nwhat we want to see how to do something better than this that is what our quest is  \nnow again why we are at this right i also want to make a because this is something \npeople do not immediately understand so i just want to make a case for something else \nso i will just do that ok now suppose there is this ipl  ok and there are four teams in the \nsemifinal let us call them a b c and d o k now i was not in town after the semifinal \nso i just know the results up to semifinal and then the finals also happen and one of \nthese teams wins let us call it the b team right the b team wins can you express this in \nterms of probability can you express this in terms of  distribution what do you mean \nmy zero and one b has won \nso it is a certain event because it has one now so what is going to be the distribution zero one \nzero zero right so this event happen s with one hundred percent probability ok now the same case \ncan you ok so now let us do the same thing that is as i said i was not in town right and \nyou asked me tell me which team would win that is i know th ese four teams have qualified \nin the semifinals and i know who the players are and so on \nand with my limited knowledge of cricket i will predict something right so say i predict \nthis ok can you again tell me how wrong i was you know what the true label i s and \nyou know what i predicted you can tell me how wrong i was ok so the case which  i \nam trying to make is that even if the event is certain you can still write it as a probability \ndistribution where all the mass is allocated to the correct output  can you relate this to a \nclassification problem when you see training data you have already observed it \nsuppose there were four classes possible \napple orange mango and banana if you have seen it is apple and if you ask you what \nis the distribution wh at will you tell me zero one zero zero you will express it as this one hot \nvector where all the probability mass is concentrated on the guy which is correct right \nso even certain events which happen with certainty you can write them as a distribution \nrate where all the masse s are located on the true label  so that is how all classification \nproblems when you are dealing with multiple class classification problems it is often the \ncase that you will write it as this \nthat your true label is given to you in this format there were four possible events four \npossible classes or k cost possible classes ou t of which only one is correct and then you \nmake a prediction and you want to now find out how different was your prediction from \nthe true label you are trying to get the set of how this relates to a classification problem \nand this is that is why this is of interest to us ok \nso this so we will see this soon now the next thing that we need is  how many of you \nknow what is entropy forget about cross just entropy  ok that is why i have left two slides \nintentionally blank ok so so now let us see where i go with entropy ok how many of \nyou know what is expectat ion please fine so again the same thing now i knew that \nthis was the distribution which i think i am into gambling am not i am into gambling \nand i try to bet on these teams \nand i bet some amount on each of these can you tell me what is the expected reward \nthat i will get so what am i saying  wait suppose this is the case that if team a  wins i \nget tenk ru pees or my net profit is tenk  rupees if t eam b wins my net profit is twentyk  \nrupees and c and d so on right you get the setup for every even there is an associated \nvalue with it this is the value of event a winning b winning c winning  d winning  \nso the net profit in each of these case so what is my expected net profit no give me a \nformula sigma overall events  right h ow many events do i have here four  right so \nrather i should say i  equal to abcd  right probability of i  multiplied by  the v alue \nassociated with that event  so this is how you compute expectation  ok everyone gets \nthis \nso now suppose  say am doing this right there are suppose four symbols  i do not know \nwhat i am teaching  ok so and i am trying to communicate this from a source to a \ndestination ok and now suppose these are the four symbols that i give and if these one of \nthese symbols is say with probability one and if i transmit it what is the information that \nthis guy gets so this is assumed that a is that sun is going to rise today if i tell you this \nwhen you are sleeping in the night what will you tell me so basically are n ot gaining \nany information well it is a certain event you know this is going to happen right \nnow one of these events suppose i am going to say that this t here is going to be a \ncyclone tomorrow morning what is the probability of a cyclone happening in chennai \nalmost one but still it is a very rare event  so if i tell you something which is very rare \nthat message has a very high information content  right so if event which has a very \nhigh probability has a very low information content and an event which has a very low \nprobability has a very high information content  right so you can measure the \ninformation content of an event  \nso so the point is that what you can have is that the information content of an event you \ncan write it as how many of you get this how many of you have seen this before all of \nyou have seen this right so this is the value associated with an event ok now can you \ntell me what is the expected information content for every event now i have given you \nthe value associated with that even so what is the expected information content \nsummation p of i  into information content of i and this  like and this is of course log \nright so it would be so what is this called this is called the entropy \nnow what is cross entropy how many distributions are you dealing with here one which \nis the p distribution which tells you how likely these messages were and based on that \nyou are tryi ng to calculate the entropy of this  situation right so now what is cross \nentropy you have a true distribution say you  have a predicted distribution  ok this is \nwhat you predicted so that means according to your predictions the information \ncontent of every event is going to be log of qi bec ause that is what you predicted  right \nbut what are the actual properties which with these which these events are going to occur \npi\u2019s right so then the expectation has to be computed over pi\u2019s right \nso then wha t you will have is summation pi log qi so this is what you estimated the \ninformation content to be but the actual events are going to happ en with this probability \nright so this is your value associated with the event and this is the actual probability of \nthe event  right so this quantity is known as the cross entropy is it clear and this is a \nway of measuring when would this be  in when would this be minimized when both are \nsame that means if your prediction is very close to your true distributi on this quantity \nwill be low minimized actually \nso that is what we wanted actually you wanted to predict some distributions in all of \nthese cases and you wanted a measure which tells you that this prediction was good and \nwhat is the definition of goo d it is as close to the correct value so cross entropy gives \nyou a measure of telling how close a predicted distribution is to a true distribution \nso now instead of using the squared error which  was actually pi minus qi right  so pi \nwas my true distri bution and qi was my predicted distribution i can use cross entropy \nwhich is given by this model and it does the same thing it gives me a principled way of \nmeasuring how close my predicted distribution is to my true distribution do you get this \nrefer slide time sixteenfortysix \n \nso now so this was for whatever we have done so far right till this point  this was for \nregression right now i wanted to enter into classification for which i have built this set \nup of how to take the dif ference between two distribu tions so now  let us consider this \nproblem where we have this situation and which is a classification situation that you are \ngiven four possible classes out of which one is the correct class and this is the true data \ngiven to you this is the true distribut ion all the probability mass is focused o n one of \nthese classes \nnow we want to given an image class ify this into one of k classes  if you could again \nuse a squared error loss but since we are dealing with probability distributions here we \nwant to use something special so before we get to what the special is going to be what \ndo i first need to tell you in the earlier case my output was not bounded was it also \ndependent was there any condition on if the imdb rating is something the critics rating \nshould be something else or the  rotten tomatoes rating should be something else no \nnow in this case is there a tightly coupled behavior between the outputs why  because \nthey should sum to one we are trying to predict a probability distribution so the  sum \nshould one right so i need an output function which ensures this you get this setup \nrefer slide time eighteeneight \n \nnow we should ensure that y hat is also a probability distribution whatever we are \npredicting is also a distribution  so now can i u se a sigmoid function yes it will give \nme values between zero to one and probabilities are between zero to one but the sum would no t be \ny so sigmoid is ruled out \nrefer slide time eighteenthirtytwo \n \nso what we use is something know n as the s oftmax function how many if  you have \nseen this before please everyone raise your hands otherwise you will get zero on the \nassignment fine so what does this what does this function actually do let us look at \nthis function right so here you had a l which was say a l one a l two a l  three right suppose \nwe had three classes ok so from here i actually want to go to hl  or rather i going to want \nto go to y hat right which is going to consider y hat one y hat two y hat three right it is going \nto give me probability of each of the three classes \nlet us assume there are only three classes right so now what this function does is how is it \ngoing to predict y one hat suppose these values were ten minus twenty and thirty so what is \ngoing to be y one hat is going to be e raised to ten divided by e raised to ten plus e r aised to \nminus twenty plus e raised to thirty  so now you see how the output is comp com puted from \neach of these values right so why did we do this e raised to stuff why could  not i have \njust taken ten plus minus twenty plus thirty divided by  the sum because we have ne gative \nvalues \nso once we take the exponent even the negative values become positive righ t so that \nis why we need the s oftmax function i hope all of you wrote this in your assignment \nthey did ok so you get this we have a different output function n ow and this output \nfunction does it make sense it gives us a probability distribution now the summation \nwould be one and each of these values would be  between zero to one  that is exactly what we \nwanted \nrefer slide time twentytwentyfour \n \nrefer slide time twentytwentyfive \n \nand now that we have ensured that y and y hat both our distributions what is the \nobjective function that we are going to use cross entropy how many of you convinced \nit is cross entropy we have two distributions now we saw that a principled way of \ncomputing the difference between two distributions is the cross entropy so we will use the \ncross entropy \nnow can you tell me something about this sum there is something special about this \nsum what are these three true values and these are the predicted values wh at is so special \nabout this sum how many terms are there in this summation  k as many as the number \nof classes in this case four how many of those terms will go to zero all but one right except \nfor the correct class everything else will go to zero so this ju st boils down to the following \nloss function that  if l is the true class right for that class yc is going to be one it is \ngoing to be zero for everything else that is exactly what this vector tells you only that term \nwill remain so were actually trying to minimize this quantity \nrefer slide time twentyonethirtyfive \n \nlet us see so for classification problems this is your objective function you either \nminimize the negative log of y hat l or you can say you are maximizing this thing ok \nnow what is this quantity y hat l no it is a predicted  probability of the  correct event \nright so this is a probably  no wait this is an important question so you have y hat l \nhere and this is a function of i mean this optimization problem is with respect to theta is \nthis a well formed objective function does y hat l actually depend on theta yes it does \nso theta because why i tell is a function of all these things everything here and then a \nlog on top of that right so it is actually a function of all your parameters s o this is a \nproperly set objective function we are trying to minimize or maximize with respect to \ntheta ok and you told me that y hat l  is actually the probability of the predicted \nprobability of the correct class  ok hence this quantity is also known as the ml class  \npattern recognition class log dash of the data \nstudent all refer time twentytwofiftythree \nall good and fill in the blanks \nso it is a priority of the x belonging to the l th class and then hence y hat l because it is \nthe probability it is the l ikelihood of it is called as the log likelihood of the data log \nlikelihood \nso what have we done so far  we started with a feed forward neural network  we \ndefined the hidden layers and the input layers and the weights and the biases  we kept a \nprovision for the output layer to be something special right then we went to two classic \nproblems one is regression and the other is classification in regression we wanted to \npredict values of all sorts of ranges \nso we decided to use a linear layer there so tha t there is no bound on the values that \nyou can predict and your objective function should take care of where the bound lies it \nshould not allow values which are way off from the true values right and that is why \nwe use the squared error function there  the other problem that we looked at was \nclassification where we saw that the label actually can be treated as a distribution where \nall the mass is focused on the true label and zero everywhere \nand our job is then again to predict our distribution so we are given the true \ndistribution and we predict another distribution so the output again we want something \nspecial in this case which is a distribution so to ensure that use a spatial function which \nis called the who  said sigmoid softmax function  fine  and then we got a prediction \nwhich is a probability distribution and then how did we find what was the objective \nfunction what is the difference between the true and the predicted the cross entropy \nright so we use cross entropy as the objective func tion and then with some \nsimplification we realize that it boils down to maximize the log of the probability of the \ntrue class or other log of the predicted probability of the true class \nrefer slide time zerofiftythree \n \nso now let us look at the summary so if your outputs are real values what is your \noutput activation going to be  linear what is the loss function going to be  squared \nerror if your output is a distribution what is the output function going to be softmax  \nwhat is this loss function squared error cross entropy right now this grid light actually \ntakes care of a wide range of problems that you will see right think of any examples that \nhave been giving you so far movie prediction or sentiment prediction or image \nclassification or anything all of that you can fit into this frame of it \nand so if you know these two loss functions how to deal with them then you can deal with \na large class of problems that you are going to deal and for the rest of this lecture which \nwill happen tomorrow we are g oing to focus on this at this particular output function \nand this particular loss function how do we compute i have a loss function what i am \ngoing to compute now the gradient with respect to all the parameters \nso this i s what we are going to focus o n right so we have seen the loss function in \ndetail we have seen that the loss function is tightly coupled with the output function \nnow we are all set but given this loss function how do we start computing g radients of \nthis loss function"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.3 Output functions and Loss functions.wav", "duration": 1570.0, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  fourfour \nlecture  four \nbackpropagation intuition \nthis lecture is on backpropagation and feed forward neural networks so we i ntroduced \na feed forward neural networks  we saw the input layer hidden layer and the output \nlayers and we saw that the output layer actually the output function depends on the task \nat hand and we considered two main tasks one was classification the oth er was a \nregression \nfor regression it made sense to use a linear layer at the output because we did  not want \nthe outputs to be bounded they could be  any range and for the classification problem  \nwe realized that  we want a special kind of output because  we are looking for a \nprobability distribution over the output and for that we use the softmax function and in \nboth cases we used a different kind of a loss  f or the regression problem the squared \nerror loss made sense because we predict some values and  we want to see how far we \nare from those values \nbut for the other case the classification we realize that it is a distribution  so maybe we \ncould use something which allows us to capture the difference between the true \ndistribution and the predicted distribution \nrefer slide time onetwenty \n \nand therefore we had this figure emerging which was depending on the  output whether \nit is real values or probabilities  you will have different types of output activation \nfunctions and different types of losses \nand of these combinations today we are going to focus on softmax and cross entropy \nand our aim is to actually find these gradients  remember there are many of those we \nhave seen this large matrix which had many such partial derivatives and we want to find \nthat entire matrix i hopefully do it in a way that it is not a repetitive we could  compute a \nlarge number of partial derivatives at one go \nso before we look at the  mathematical details  we just get an intuition for  \nbackpropagation \nrefer slide time twoeleven \n \nand then we will get into the gory details of how to actually compute these gradients and \npartial derivatives so this is the portion that we are in we are intended to ask these two \nquestions and this is where we are \nrefer slide time twotwentyone \n \nso now this is what our network looks like this is clearly much more complex than that  \nsingle neuron that we had and which had only two weights w and b that was very easy to \ncompute the gradients there now imagine that i want to compute the gradient of the l oss \nfunction and let us assume it is a classification problem then what is the l oss function \nminus log of y hat \nso this is the loss function and we want to compute the derivative of this with respect to \none of these weights in the network and am delib erately taking something which is \nmuch farther away from the loss  but why do you say why do i say it is much farther \naway it is right at the input layer right and the loss is s omewhere at the output layer  \nso we want to compute this gradient \nrefer slide time threeten \n \nnow to learn sorry you want to learn this way to learn this weight we know that we can \nuse gradient descent  we are all convinced that this gradient descent algorithm which i \nhave shown here as long as we put all these variables or a ll these parameters that we \nhave into theta \nwe can just run the gradient descent algorithm and compute them the only thing that we \nwill need is this partial derivative with respect to all the weights in the network  and in \nparticular with respect to this weight that i am interested at \nrefer slide time threeforty \n \nnow so we will now see how to calculate this  we will first this is o nly to get the \nintuition so we will first think of a very simple network which is a very deep but the \nthin network it has many layers but it is a very thin network here you see what i mean \nby a thin network ok now this is what i am interested in can you tell me how to \ncompute this this looks like a chain so it is justified the user c hain rule of derivatives \nso what would the chain rule look like \nyou want to compute the derivative of this with respect to this and you have done this \nin high school right so you have functions of the form of sine of cos of tan  of e raised \nto sine of x  and this is exactly how this  chain is right you have some function of x \nfollowed by another function of x another function of x function of x function of x and \nso on you just keep making a composite function of the input we actually wrote down  \nthat function if you remember i t was just one function applied after the other function  \nor a very composite function so you just need to apply the sa me idea here so we take \nwe go step by step so i am almost accounting for every shade of color here \nso dl theta by d y hat then dy h at by d a  l eleven there is only one neuron here  then this \nwith respect to the sorry h twentyone then h twentyone with respect to a twentyone a twentyone with respect to h eleven h \neleven with respect to  a eleven and then a eleven with respect to w eleven \nso i just traversed down the chain in the r everse order this is how the chain rule works \nright anyone has a problem wi th this it is straight forward  right and now what i \nhave done is for convenience i have just compressed the chain you see the red part and \nthe green part i have just compres sed this weight so that and this is again something \nthat you have done in high school if you have this you could just write the chain as the \nfirst and the last it so this is what you can do and i also compress this other chain  ok \nand am going to use these kinds of compressions later on \nso what am trying to impress on you is that if i want to go from here to here right that \nis what my intention is if somehow i have already travels from here to here then i can \njust reuse that computation  that is the idea which i am trying to impress on it i do not \nneed to follow the entire chain every time i can do these partial computations up to a \npoint have you seen this something similar idea somewhere else dynamic program is \nsomething like that so you have just computed up to a certain point and then it is reuse \nthe value for further down the chain \nso that is what we are going to do and same for all the weights right for each weight \nthe chain size would be different depending o n where it lies in th e network right for the \nweights which are very close to the output layer the chain w ould be very small makes \nsense ok so this is the intuition and we will see the intuition a bit more \nrefer slide time sixforty \n \nso let us now understand this in the t erms of the wide co mplex network that we are \nusing \nrefer slide time sixfortyseven \n \nso what actually is happening is that  we are at a certain stage that means we have \nsome values of ws and b\u2019s ok at the initial stage we just have these w knots and b \nknots but let us assume that we have done some training and we are at a certain level \nwe are at wt at time step t and bt at time step t right for all the weights inverse now we \nfeed it a training example we do this entire compute computation what do we g et at the \nend we get y hat which is a function of this x that we have fed it but we also know this \ntrue y we know the true value we know y hat \nso we can compute the loss function so we compute the loss and to our surprise we see \nthat the loss is not  zero we are getting a non zero loss that means the netwo rk has not yet \nlearnt properly right the weights and biases are still not in the right configuration that we \nwant them to be in right so now what do we do we go on this path of investigation \nwe want to find at who in th is network is messing up things  there is someone who is \ncausing this problem because of which i am not getting the desired output and we are \non our quest is to now find out who this guy is who is responsible for this so what \nwould you do where would you start the output layer \nrefer slide time eighteleven \n \nbecause the output layer is the guy who give you the output right so go and talk to \nhim and we say that hey what is wrong with you why are you not producing the desired \noutput right now what is the output layer going to tell you in very civil language i \nwill say i cannot do anything boss i mean i was just given some weights and inputs \nfrom the previous layer and those weights and inputs were messed up \nso there is not hing which i can do go and talk to them so who will it directors do it \nwill say that i am just as good as wl hl minus one and bl because these are the guys that i \ncompletely depend on if these guys were ok then i would have been fine  so we then \ngo and talk to these guys that what is wrong with you \nrefer slide time eightfiftyeight \n \nso now they say  ok fine wn and bl take the responsibility they are the nice guys they \nsay we are the weights we are supposed to make a  we are the ones who are responsible \nfor the adjustments in the network so we have failed to do our job properly and i think \nwe should get adjusted right but then hl will resist it will say it is not my fault why \nwill it resist because it against again depends on the previous activation layer \nso till then point as to what the ws and b \u2019s in the previous layer right and you see \nhow the investigation is now proceeding where will we reach well keep going down the \nnetwork we are talking to everyone in the network we are talking to eve ry dark green \nguy every light green guy every dark blue guy every light blue guy we are also talking \nto all these weights and biases and in the end what do we figure out the responsibility \nlies with all the weights and all the biases they are the ones who are responsible for this \nnow but now we find out that this is also one of those weights which is responsible and \nthis is also one of these weights that is responsible but it was have been very difficult \nfor us to talk to them directly so then wh at are we going to do  instead of talking to \nthem directly which is this we will talk to them through the chain rule so we will talk \nto the output layer that is exactly how what we did maybe went to the first guy that we \nknew that guy pointed out to th e previous hidden layer that guy pointed us to the \nprevious hidden layer and then finally we get to the weights right \nso this talking to is fine but where do derivatives figure out in this why are why is the \nlanguage derivatives why are we not talk ing in english or hindi or something else  \nwhat does the derivative tell us  so talking about gradient descent like what we saw in \ngradient descent but in general what does the derivative tell us if i change this a bit \nhow much does my loss change right \nso that is how much this guy is responsible for the loss because if this is very sensitive \neven adjusting a bit of this i co uld drastically reduce the loss  right so that is what the \nderivative tells us that tells us how sensitive is the loss funct ion to the weight or any \nquantity with this with respect to  which am taking the derivative right that is why the \nlanguage is of derivatives right is that clear is the intuition fine to everyone \nrefer slide time eleventwentynine \n \nso now will convert this intuition into actual math and try to figure out how to compute \nevery guy along the way right and we will use this idea that we have  made some \npartial computations and then well use it for the rest of the chain so we have made this \nmuch at some point we  will reach where we have made this much and then you could \nuse it for the rest of the chain in fact  we will start right from here well start with this \nguy and then keep expanding the chain \nso the rest of the story is going to be about computing three qua ntities can you tell me \nwhich are these three quantities gradients with respect to the output units gradients with \nrespect to the hidden units and then gradients with respect to the parameters so these \nare the three things that we need to do if we do this w e have everything in the chain and we \nare done and the other thing that we need to do is we cannot sit down and compute this \nfor every element right we want to have it in a generic fashion where instead of talking \nabout w one one one w one one two and so on \nwe should at least be able to talk about w one  w two and so on  so that means we have \nonly three matrices and three biases right at least at that level so we have to do a collective \ncomputation instead of just computing for every guy so instead of looking at sca lars \nwhich is what we are doing when we are doing gradient de scent for w naught and v \nnaught we were just computing the update rule for w and b we want to now do it for  \nvectors and matrices \nso that is that is the transition that is going to happen and our focus is going to be on \nwhat cross entropy and softmax  why is that important  because that is the loss function \nso that is the quantity that am going to take the derivative if i change the loss function \nall the gradients are going to change are all the gradients going to change only the first \nguy will change in the change all this should remains still same right modulus some \nconditions but largely it should remain the same right unless you  change something in \nbetween"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.4 Backpropagation (Intuition).wav", "duration": 792.0, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fourfive \nlecture  four \nbackpropagation computing gradients w r t the output units \nnow we go to the next module where we w ill first see how to c ompute the gradient  \nwith respect to  the output units well that was the first guy in our chain right that is the \nfirst person that we need to talk to \nrefer slide time zerotwentythree \nso that is the part that we are going to focus on \nrefer slide time zerotwentysix \n \nso this is the output and when i say i want to  compute the gradient with respect to \noutput unit what do you actually mean what is the quantity that i am looking for i will \nhelp you out actually what i meant by output unit is t his entire thing right so i actually \nmeant al\u2019s ok but it is it is a fair answer and even y hat is a fair answer ok in fact am \ngoing to start with y hat and then go to al so i will have to start with this guy  and then \ncome to this guy \nrefer slide time zerofiftyfour \n \nso this is the loss this is y hat which is equal to y one hat  y two hat up to yk hat so these \nare the k values that we have here and we are looking at cross entropy that means we \nare looking at the classification problem right so we ha ve got a distribution over the k \nclasses that is what y hat looks like and we know that one of these guys is the right \nclass maybe say y two so the loss function is minus log of y hat two because two is the correct \nclass in this toy example that i am consid ering ok so the loss function i am just \nrepeating the definition right that is how the loss function is \nrefer slide time onethirty \n \nnow oh god so again this is what our y hat looks like ok now i want to compute the \ngradient with respect to any of the output units right so it could be y one y two y three y four up \nto yk right  so this i actually can take values from one to k in this case one to two right ok  \nnow can you tell me what is this loss ok this much is fine can you tell me what is this \nderivative minus one by minus one by y hat l if y is equal to l \nstudent refer time twoeight \nand zero otherwise how many of you get that cool ok so it is a very simple thing that you \ncan think of this as z and this is y only if z is equal to y then the derivative would ex ist \notherwise it is going to be zero right ok so how do i write this fe part using \nstudent refer time twotwentyseven \nhow many of you have seen indicator variables before  good so this i s what you are \ntelling me right it is going to be minus one by y hat l if i is equal to l ok and if i is not \nequal to l then these two things are not related it this is a function of something else and \nyou are taking a derivative with respect to a different quantity  \nso it is a constant with respect to that quantity and the answer would be zero ok now i am \ngoing to write this as this right  so this is the same as saying so this variable actually \nthis is known as the indicator variable it takes on the value one if the condition in the \nbracket holds otherwise it takes on the val ue zero so this is exactly i am writing exactly \nthis but in a more compact manner ok is that clear to everyone  \nrefer slide time threefifteen \n  \nso this is what the quantity this is the quantity that we have computed with respect to \none of the output units ok so this is what derivative partial derivative gradient how \nmany of you say derivative no one likes derivative partial derivative that is always \nthe safest choice partially fl right  and gradient oh  there is one brave soul who say is \ngradient do not worry well fix that ok \nso this is the partial derivative y because my y hat is actually a vector and i am taking \nthe derivative with respect to one of those guys ok now if i want the gradient with \nrespect to y hat what would that look like a vector which is a collection of \nstudent refer time fourone \npartial derivatives so let us see this is the quantity that i am interested in am interested \nin the gradient of the loss function with respect to the vector y hat so remember the \nvector y hat  is y one hat y two hat up to yk hat right so this gradient is going to be a \ncollection of the partial derivatives with respect to y one hat y two hat and so on \nnow wh at is each of these quantities so it is simple right so this quantity the \nderivative is either going to be zero or is it going to it is going to be one by y one hat right if l \nis equal to one right and that is exactly what i have done so now how many elements \nhere are actually going to be nonzero at a time how many of these going to be nonzero \none which one \nstudent refer time fivefour \nthe one corresponding to l right everything else is going to be zero so this is a dash vector \ny not vector ok so now am going to write one hot vector like this what have we done ok \nwhere el is what one hot vector such that it is l th entry is one ok that is what am going that \nis how am going to define e l is that fine with everyone ok  \nand so you see the story how did how we went about computing this we started with a \npartial derivative with respect to one of t guys right we found a formula for y i we saw that \nthis formula is generic enough and so now we can compute the gradient which is a \ncollective of all these yis where i ranges from one to k right and then we just put that in a \ngradient vector \nso this sto ry is going to repeat throughout the lecture where we try to compute the \ngradient with respect to one guy and then generalize oh sorry we compute the partial \nderivative with respect to one guy and then generalize and try to find the gradient fine ok \nrefer slide time sixeighteen \n \nrefer slide time sixnineteen \n \nso what if i what do i have so far i have this quantity what does till which part of the \ndiagram am i currently the dash green part dark green part  i am till here i need to go \ntill the light green party  that is collectively the output unit ok although i have divided \ninto two halves but when i say output unit i mean that output neuron right complete \nneuron so what i am actually inter ested in is these quantities  or more specifically ok \nthis is what i am  interested in what is this one of those gu ys right this al is actually \nalone up to al k right so this is one of those guys  so this is going to be the gradient or \nthis is going to be the derivative a partial derivative sorry ok  now what do how do we \nproceed from here \nrefer slide time seventwentysix \n \nnow i will again have to compute this  you already know that  good but before that i \nwant you to answer on e question right so y hat l  what is y hat l it is the output \ncorresponding to the correct class  does it depend on an arbitrary al i so in the \nprevious thing we saw that only when i is equal to l there is a connection in this case is \nthere a connection always or only when i is equal to l \nstudent refer time eighttwo \nalways why softmax so  \nstudent refer time eightfour  \ndenominator has all the ali \u2019s right so this is there it is y hat l in the numerator of \ncourse it only has this unit which corresponds to the l th  probably did not choose my \nvariables very well so l th component of a capital l r ight and but in the denominator \nyou have the entire sum which means that every output guy here  each of these dark \ngreen guys depends on each of the dash green guys light green guys good  \nso that is at least settled that we always the we can always com pute this partial \nderivative we do not n eed an if else here there is no thing like l is equal to i then what \nwill happen it will always have this partial derivative \nrefer slide time eightfiftythree \n \nso we will now derive the full expression for this so this is what we are interested in is \nthis fine so this is a function of the form so you are taking  how do i say this so  this is \nlog of a function so first you will take the derivative with respect to log and then push \nthe partial derivative inside right so that would be minus one by y hat l and then the \nderivative with respect to y hat l now what is y hat l the softmax function right  \nso it is the l\u2019 th entry of the softmax function applied to that output vector what is the \noutput vector al right so it is the l\u2019th entry of the softmax or l\u2019 th entry of the function \napplied to the output vector \nso this was our al what is our output right so now one of these guys here is the l th \nguy and one of these guys here is the l\u2019th guy right so what you do is you take this you \napply the softmax function to it which again gives you a vector and now you are \ninterested in the l\u2019th component of that vector  that is what this quantity means  it should \nbe clear now \nrefer slide time tentwentyfive \n \nnow i will just do some simple math stuff here and we should be able to derive this is it \nfine am just replaced by the actual softmax formula this is a derivative of the form u by \nv right what is the formula for that yeah  it perfectly right yeah so this is what it \nwould be right i mean it is you all know this i am not going to spend time on this \nso now am just going to substitute the values here  yeah it is getting a bit nasty but  it is \nnot very difficult right  so so this so this is our g of x so am taking the deri vative of \nthat then this is this one over h of x you can just figure it out right anyway it everyone just \nread this for a few seconds and let me know if this is not clear this is g this is h in this \nformula right have just substituted the gs and hs in this  now what is this quantity going \nto be it is derivative of the form e raise to x right so it is e raise to x always \nstudent refer time eleventhirtythree \nif i is equal to l right  s o now we have this dependence because we are looking at a \nnumerator but the numerator only depends on the l th entry right so now you are trying \nto take the derivative of the l th entry with respect to some arbitrary i th entry so only if \nl is equal to i you will get the derivative right \nrefer slide time elevenfiftytwo \n \nnow what about this how many terms in the summation would remain \nstudent refer time twelvetwo \none which one \nstudent refer time twelvefour \nwhere i dash  is equal to i right so the i\u2019 th guy would remain  the rest of it is \nstraightforward right this square  i have just divided into two parts  ok ah now let us see \ncan you simplify this because i cannot ok can you simplify this what is this \nstudent softmax \nsoftmax and which entry of the softmax \nstudent refer time twelvethirtysix \nl\u2019th entry i\u2019th entry l\u2019th entry with the saw with the indicator variable but what is this \nthis is our input hidden layer output  so ok now let us see  what is the next step   this \nis should have been y hat i but y hat is equal to f of x right so we  can fix this unit so \nok fine so we ha ve actually what do we have now  we have the derivative of the loss \nfunction with respect to  the i\u2019th unit of the  output layer right and which part of the \noutput layer the pre activation pattern ok now what am i going to do i have a formula  \nwhich tells me how to compute this what was i actually interested in so now how am i \ngoing to go from here to there i just put all the partial derivatives into a \nstudent vector \nvector and that vector is the \nstudent refer time thirteenfiftyeight \ngradient good \nrefer slide time fourteenzero \n \nso we have this one formula it is ok if some of you did not get this derivation right it is \nvery very straightforward if you go back and look at it i am pretty sure you will get it is \nnothing in this is very simple elementary s tuff right except  for some degree here and \nthere ok so now what would this look like  \nwe should add actually l theta here this would look like a collection of all the partial \nderivatives we have a generic formula what will we do now what is the fir st entry \nminus in indicator l equal to one minus y hat  one which is the variable that we are indexing \nover i right not l oh god oh we are indexing or ok have i goofed up oh that is wrong is \nit oh yeah that is wrong fine then this is fine we are indexing over i and then we can do \nthis \nnow can you simplify this i am looking for ok this is the element wise difference of two \nstudent refer time fifteenthirtyeight \nof the indicator vector and \nstudent y hat \nrefer slide time fifteenfortyfour \n \ny hat oh hey we should change all this y hat is equal to f of x right but i just want it to \nbe consistent as y hat so is this fine this is a simplification fine right so we have \ncome a long way right you have finish this part ok we have got the gradients with \nrespect to the out put units ok this much part is a clear to everyone moduler bit of the \nmath which you can go back and look at it this entire derivation is fine but you get the \nconcept right that we start with one unit from there grow the gradient then keep goin g \napplying the chain rule \nso we started with the dark green guys and then went to the light green guys now we \nhave the derivative with respect to the entire light green vector and that is what we had \nstarted off with that we wanted the gradient with respect to the output units"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.5 Backpropagation\uff1a Computing Gradients w.r.t. the Output Units.wav", "duration": 987.85, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 foursix \nlecture  four \nback propagation computing gradients w r t hidden units \nnow we will go to the gradient with respect to the hidden units \nrefer slide time zerosixteen \nso this portion so you already see there is a repetition here and i do not need to treat \neach hidden unit separately i can just have a formula for the hidden unit and then i could \ncompute it for all the hidden units so that is what our aim is so let us do some simple \nstuff first and then you will come back to it \nrefer slide time zerothirtythree \n \nso suppose you have a variable x you compute two functions from that one is x square the \nother is x cube i will call this as y one and i will call this as y two and i take y one and y two and \ncompute a z which is say a log of y one by y two now what i am interested in is this what is \nthe answer for this how do you get this this is a fair question to ask y one y two are \nfunctions of x z is a function of y one y two hence z is a function of x so i can compute this \nderivative and i can ask for this derivative how would you compute it  if i cannot really \ndo this right \nso if this path did not exist then it is trivial it is just  the chain rule along one path but \nnow you have two paths so what will happen add them right  so can you tell me a \nformula for that so let me know if this makes sense to you  ok does this make sense  \nnow let me complicate this a bit just let me just do it as y three now \nstudent refer time twofifteen \nwhat will happen \nstudent refer time twosixteen \nthat is all right so you see that if there are multiple paths you can just add up the chain \nrule across all these paths right that is what chain will across multiple paths does \nrefer slide time twotwentyeight \n \n  \nso with this we will go back to this figure so now i am interested in i am interested in \ngoing to the hidden layers again i will do this to bit calculation where i first asked for \nthis guy and then i will ask for the light blue guy right and am going to look at one unit at a \ntime now what is the  what am i interested in the derivative of the loss function with \nrespect to say d h two two right the second unit of the second hidden layer \nrefer slide time twofiftyeight \n \nnow what i am going to say here is exactly what i had written on the previous slide this \nwas our final function right which was z so z was  sorry again i have not chosen my \nvariables well ok but  if so we had exactly the same situation right whic h is which \nyou see here ok so we will just have to sum up the derivatives partial derivatives across \nall the paths which lead from this guy to this guy and there could be as many paths as \nthere can be but i do not care i will just sum across all those paths in fact actually here \nthere are not just two paths because we have always assumed there are k classes so there \nare actually k of these paths right \nso this form this is exactly the formula which i wrote on the next slide right this one but \njust written in terms of the net work that we are dealing with  so you can just go back \nand look at this but as long as you understand this figure you from my point of view  we \ncan go ahead  so everyone understands this figure that we just need to compute the \nderivatives across all the paths and add them up \nrefer slide time fourthree \n \nso now let us start we again the same  recipe we will compute it with respect to one guy \nand then go towards the gradient so what is this now let me explain right so dl theta \nthere are k of these guys between right  so there are k paths so this summation has to \nhappen over k paths just as you told me when there were two paths the summation was two three \npaths to three that is k paths of the summation over k guys the derivative with res pect to \neach of these guys and the k\u2019th the m\u2019th unit rate that is the index that i am iterating over \nand then the derivative of this guy with respect to whatever you are interested \nthat is just that there are only two nodes in the path in the chain but th ere are k such \nchains how many of you exactly get this ok how many of you have a problem want me \nto repeat this you have problem oh many of you ok good please do this  so i am \ninterested in this quantity that means i am interested in the partial der ivative of this loss \nfunction with respect to this guy \nrefer slide time fivesix \n \nand this guy is nothing but h ij that much is clear is the j \u2019th unit of the i\u2019th hidden layer \nin fact this is actually h two two so my i is e qual to two and j is equal to two  now i just made a \ncase on the previous slide that if you have such a function which first computes some \nintermediate values and then your final function is computed based on all these \nintermediate values right and now you are trying to find the  gradient the partial \nderivative of this with respect to the original input that you had \nso then what you will do is you will sum across all the paths that lea d from this guy to \nthe output how many such paths are there you already see two such paths here right but i \nam saying there are k such paths  because there are some other nodes here which i have \nnot drawn we have already said that in the output layer we have k nodes right so there \nare k paths so that takes care of the first bit that the summation is going to be over the k \npaths \nnow what is each of these paths composed of this intermediate value and this quant ity \nthat we are interested in  first we will take the derivative of the out of the loss with \nrespect to this intermediate value what is that tha t is the unit in the  that is the unit in \nthe previous layer or the next layer rather so i am interested in i so i am looking at the \nunit in the next layer hence i plus one right because that is what comes in my path the next \nlayer is what comes in my p ath we ha ve always the special case  that this guy feeds \ninto k guys but all the other hidden units before that feed into n guys right \nso that is let us just keep that complication aside for the minute and we just look at this \ncase ok is that fine  so we have agreed there are k paths and each path is composed of \nthese two nodes from the last loss function to this intermediate value and then from this \nintermediate value to the quantity of interest and why is this i plus one because the next \nnode in the path when i am at the i\u2019th layer \nso i will be feeding to the i plus one\u2019 th layer right and in fact i will be feeding to all the \nnodes in the i plus one th layer that is why i am taking or all the k paths right and then that \nnode which is this node with respect to the quantity that i am interested  in is this clear \nnow right this is very similar to the toy example which i did i just have k paths now \ninstead of two paths there \nso let us move ahead now what is  ok which of these quantities do we already kno w is \nthere any quantity that we know this one  why because in this special case i plus one is \nactually equal to l right because we are feeding into the last layer and they have already \nseen how to compute the partial derivatives with respect to the last layer \nso this quantity is known we do not know this for the generic case yet but we will get \nthat but for this special case when we are feeding into the last layer we know this does \neveryone get this ok now do we know this quantity so what you have told me is that \nwe know this quantity because that is what we have computed in the previous module \ndo we know this quantity we have to compute it can you compute it ok let us just do \nit right so let us assume that this hij that am dealing with is act ually h two two ok fine now \nwhat is a i plus one m actually which are the elements there a three one and a three two i am \nassuming that i only have two units in the output layer ok \nso my m is equal to two now is this fine  this is how the next layer is related to the \ncurrent hidden layer plus biases ok now what am i interested in one of these guys ok \nlet me take one of these guys so can you tell me what is a three one first row multiplied by \nthe first column there is only one right plus b two one  \nstudent refer time teneleven \nsorry \nstudent refer time tentwelve \nb three one  now let me just clarify something what is this in terms of variables i j k m what \nis this this is i this is j this is k this is m this is i plus one right  ok this is one of the ms \nthat i am dealing with no w i want the derivativ e of this with respect to hij  in fact i \nwant it with respect to h two two where this is i and this is j is this clear what is this \nderivative w three one two everyth ing everyone fine with this  now help me find this  what is \nthis ijkm and i p lus one what is this this is coming from the m  how many of you see \nthis because that is the unit that you are connecting to and this is j so what is the \nformula how many as many as the number of neurons in the next layer a bias will be \nconnected to all the neurons in that layer right everyone gets that right there are only two \nunits \nso there will be only two guys ok so what is the formula for this w i plus one mj everyone \ncomfortable with that  ok fine you can just go back and look at this and it shou ld be \ncleared right so whenever you are dealing with vectors and matrices right if you are \nreally good at it you can imagine the entries and figure out what is happening if you are \nnot good at it do not be lazy just work it out right you just need to write down this \nproduct and at the end remember everything is always element wise and you are never \ndealing with a vector or matrix now just dealing with the individual components of them \nso you should always be able to compute these derivatives or parti al derivatives with \nrespect to the individual components and that is exactly what i did here right if you \njust work it out if you just write it out then you will always get it if you cannot but \neventually try to get to a point where you can just visual ize it but if you canno t at least \ntry to work it out \nrefer slide time twelvetwentyfive \n \nso this is what it will look like ok now consider these two vectors one is this vector what \ndoes this vector look like this is a collection of all the  partial derivatives so this is just \na collection of all the partial derivatives nothing new we have already seen this now \nwhat is this vector actually in fact i have started with the matrix and  i am saying look \nat this vector what does this mean this i plus one is just th e layer in which the matrix is \nright so that index we do not really care about  for a matrix what we care about is the i \ncomma j index ok now what does this dot comma j mean all the i\u2019s belonging to j that \nmeans the dash column j\u2019th column everyone gets this this is all the i\u2019s or all the entries \nbelonging to the j\u2019th column \nso it is effectively just the j\u2019 th column so it is one comma j two comma j up to k comma j \nright so these are two valid vectors now tell me what is this quantity going to be th is is \nthe dash between two vectors dot product dot product between two vectors is a \nstudent refer time thirteenfortythree \nis a summation over element wise thing ok i have said enough now try to connect this is \na very simple maths the column that you will ever get in  your life try to connect this to \nsomething which is already there in the slide how many of you think the answer is this \nthis into this plus this into this plus this into this and just write it as a formula you will \nget this everyone sees that ok so no w i have a compact way of  writing one of these \nentries \nrefer slide time fourteenten \n \none of these guys i have a compact way of writing this it happens to be the dot product \nbetween two vectors one of them is the gradient but do i know this already do i kn ow this \nquantity already in this special case yes because i plus one is equal to l and that i have \nalready computed this of course i know right because these are the weights that i am \ndealing with where do i go from here this dot yeah it means anything from that \ncolumn so that means the entire column \nstudent refer time fourteenfortyeight \nah no these are weights right so this is a weight mat rix it has columns and rows i  am \ntalking about the j\u2019 th column so i fixed the value of j i am  talking about the j\u2019 th \ncolumn but i am not telling your given i\u2019th entry there am just telling you all the entries \nthere that just means the j \u2019th column you can take this offline ok this is very simple i \nwill take it offline ah now where do i go from here \nstudent refer time fifteensixteen \ni plus one \nstudent refer time fifteentwenty \nok no in this specific case are we done \nstudent refer time fifteentwentyseven \nwhere are we right now with respect to one unit where do we want to go the entire \nthing so what is the quantity that i am inter ested in gradient with respect to always say \nwith respect to h i right \nstudent refer time fifteenfortyfour \nwhere i is two in this case this special case ok what is that going to be collection of all \nthese guys that you have already computed ok now simplify this  what is this first \ncolumn of the matrix multiplied by the same vector the second column of the matrix \nmultiplied by this vector  the nth column of the matrix multiplied by this vector this \nreminds you of something very very difficult this is a very v ery complicated matrix \nmultiplication right \nfirst row of the matrix multiplied by a column the second row of the matrix multiplied \nby column how many if you get this right so this is can you tell me what this is wi \nplus one transpose \nstudent refer time sixteenfiftytwo \nperfect right so now you see that this entire quantity we can compute in one go by \nusing a matrix vector multiplication right so that is what i meant when i was saying \nthat we should not be doing these unusual computations but we able to compute that at \none row right so now we can just do this matrix vector multiplication and get this entire \nquantity ok now what is still missing in this module \nso what is the special case that i have assumed i told you that i already know these \nquantities but only if i plus one is equal to l  i need to tell you this in the generic case ok \nso we are almost there except that i do not know this when i is not equal to l or i is less \nthan equal to l minus one ok that is the case that i am looking for \nrefer slide time seventeenthirtyeight \n \n  \nso that is again very simple again what will i do i will compute it with respect to  ok \nwhat is this this is the guy that i am interested in the generic i not the l \u2019th one right \nthe generic i this is what the vector looks like the gradient vector looks like i want each \nof these guys ok now i will take one of those and i will write it as this ok what am i \ndoing am saying that  i already have the entries up to here ok at a very general level \neven here i could have said the same thing remember that i had said that the output layer \nyou can always write as hl right \nso even at the output layer i could say this chain rule always holds how many of you \nagree with that i want to go from the loss function to one of the lighter  blue guys so \nam saying that i can go through the intermediary dark blue guys that is all i am saying i \nhave just compressed this entire path into up to the dark blue guy remember i had said \nearlier that i will be compressing this chains now how many of  these quantities do you \nknow the first one is what we computed on the previous refer time eighteenfiftytwo the \nsecond one looks very difficult sorry \nso h ij is nothing but sigmoid of a ij or any non linearity of the a ij so i can just write \nthis derivative as i will just write it as sigma prime ok \nrefer slide time nineteenten \n \nor g prime is this fine now i have it with respect to one unit what will i do go to the \ngradient fit it all these values now simplify this what is this a vector  right what is \nthis another vector there is a one to one correspondence between them so you have two \nvectors and you are doing a one to one multiplication what is this \nstudent refer time nineteenfortythree \nhow many of you say dot product dot product is always a what is the output here \nstudent vector \ncan it be a dot product can it be a dot product no please empathic no ok so what is it \ngoing to be an element wise multiplication and this is how you denote that ok so what \nis this called you had a multiproduct right so thi s is every element of one vector \nmultiplied by the corresponding element of the other vector ok so now again the entire \nvector we can compute at one row right i am not i am when i am teaching this i am telling \nyou how to compute one element and then go to  the gradient but when you are going to \nimplement this we are just going to compute the gradient at one go"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.6 Backpropagation\uff1a Computing Gradients w.r.t. Hidden Units.wav", "duration": 1221.31, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fourseven \nlecture  four \nback propagation computing gradients wrt parameters \n refer slide time zerosixteen \nbefore we move on to  the next module  a quick summary of what we have done so far \nso we introduced feed forward neural networks and we wanted to learn the  parameters \nright from the last layer to the first layer and we figured out that what we can do is that \nwe can just us e the gradient descent algorithm as it is except that we have this small \nproblem that we have so many parameters now and located at differ different points in \nthe network right some at the initial layer some at the final year and you want to \ncompute the derivatives or the partial derivatives with respect to all of these \nif you can do that put them all in this large matrix then we can jus t use gradient descent \nas it is s o that is what we figured out and then we wanted to find out the gradients \nwith respect to or the partial derivatives with respect to all these parameters so then we \nrealize that this can be done using chain rule because there is a path from your output \nwhich is the loss function to any of these weights so we just need to follow that path and \napply this smart  this chain rule smartly and just sum up  the derivatives across all the \npaths that lead to that weight so in that process we started from the output layer we just \ntreated it a bit special because the output function is special and this is the last layer \nso we just first computed the gradient with respect to the output layers then we figured \nout how to compute the gradients with respect to  any of the hidden layers and now if \nyou are at a particular hidden layer now the  weights that feed into this layer we could or \nwe have not reached there \nso now the next thing that we need to do is that we have computed the gradients with \nrespect to any of these hidden layers and now we want to find the gradients with respect \nto the parameters which is the weights and the biases so it is the do you all remember \nthis or it is all long history or the story is back right  fine so now we are at the last \npoint which is computing gradients with respect to parameters \nrefer slide time twofour \n \n refer slide time twofourteen \n \nso again this is the overall picture we were in this chain rule and we have come all the \nway to the last point where we are ready to now compute these quantities  so now start \nby recalling that a k is equal to b k  plus w k h k minus one right this is our activation \nformula pre activation formula right so i am talking about these light blue guys ok \nwhich is clear in image \nand now i what have i done so far i have been able to come up with a formula to write \nthe gradient of the loss function with respect to any of these light green guys right that \nis what where we ended last time right where we are able to compute the gradients with \nrespect to the  sorry light blue guys ok  and now i want to compute the gradi ent with \nrespect to any of these parameters or any of these parameters \nso any parameter it does not matter am at some i\u2019th activation layer pre activation \nlayer and i just want to compute the gradients with respect to the weight s which feed \ninto this layer and that is what we are interested in so we are just taking any layer k \nand you want to find the gradient with respect to the weights there now can you tell me \nso can you tell me what is what is the thing that am going to do here or what is th e \nrecipe that we have been following \ni need to move what is the recipe that we have been following apart from yelling at \npeople who come late we find the element wise partial derivatives first and then put \nthem all together to get the gradient ok what  is the element here what is what am i \nlooking for right now i want to compute this fill this blank what goes here \nstudent w \nw any of these w is right and in particular say w k that is what i am looking for so \nwhat is the first thing that i am going to attack \nstudent wkij \ngood w k i j and once i have this for one of these guys i just know a generic formula \nwith respect to i j and k and i can just put it into a gradient vector ok is that fine ok so \nnow can you ok now from here to here if i wa nt to reach from here to here so this is \nwhat i am interested in right now how is the chain rule going to look on look like \nbased on whatever you have already seen till where have you already reached you \nalready know this quantity right now if i want this how am i going to write it \nstudent refer time fourfiftynine \ni will find up to the light blue guys which is this i already know how to compute it and \nthen from the light blue guys i will go to the this is fine right so this is the quantity \nthat i am looking for ok now what is one element of this guy dou a k by is it fine ok \nwhat is the dimension of this actually is it a scalar a vector a matrix matrix or a tensor \nwhat is the tensor what is it is it a matrix what are the dimensions w hat does this \nderivative mean or this gradient mean i change one element of w k how much does \none element of a k change how many elements are there in ak n how many elements \nare there in w k n cross n so how many partial derivatives which i have n cross n \ncross n what is this \nstudent tensor \na tensor right so this is going to be a tensor ok so when i say one element of this i \nmean this ok so this is one element of this gradient ok now can you tell me the \nformula for this what is this quantity hk minus \nstudent one refer time sixtwentyseven \nhk minus one or hk minus one j or \nstudent refer time sixthirtyone \neveryone gets this hk minus onei how many of you get this  \nrefer slide time sixthirtyeight \n \nso let us do it right so you have akone aktwo akthree that  is your ak vector ok you have \nbkone bktwo bkthree plus wkone one yeah i know again this is one of those silly things but if \neveryone does not raise their hands and compelled to do this  so h k minus one one hk minus \none two hk minus one three ok so let us take one of the se guys right so a k one can you tell me the \nformula for that \nstudent refer time seventhirty \nplus first row ok one two this one three now can you tell me this quantity so what is i here one ok \nso i want this by w k i j right so i is one so i can take any of the  j so let me take j equal \nto two so what is it going to be this will go off this is constant this is constant only this \nterm remains and the derivative is hk minus one two which is j right so that is what the \nformula says so i have a formula for one o f these guys  ok and that is a generic \nformula so always remember if you cannot figure out what it is just write it down in \nscalar terms just add up all the terms and you will get the formula right so now  this is \nwhat the chain rule is going to be  \nrefer slide time eightthirtyseven \n \nso this is what it is going to be  this is one element of that tensor this is how that \nentire thing is going to look i have just flattened it out and put it here \nrefer slide time ninefive \n \nnow let us take a simple examp le of wk belonging to r cross  three cross three everyone is fine \nso far right or anyone who everyone is  fine please raise your hands i mean fine i  mean \nnot in life but with the lecture  fine so this is what it looks like right for a three cross three \nmatrix \nnow let us see we already found out that this guy is equal to hk minus one comma j right \nso this is what this matrix looks like nothing rocket science here right so each of these \nquantities is actually can be written in this form where i appropriately substitute i k and \nj and i know that this quantity can be further written as this quantity right that this is \nour clear right so i have written it as this  \nnow can you simplify this i do use a lot of this ok can you simplify it is it look s \nsimilar to something  that you did on the assignment does this look like  matrix which \nhas some very regular patterns yeah i can see someone doing this and this everyone \ngets it \nrefer slide time tentwentyeight \n \nso let us see so this the first column the second term in the produ ct is all same \nthroughout all the rows right what i mean is all these guys are similar same thing \nhappens in the second row the third row right ah that is sorry the second column and \nthe third column what about the rows these are all equal  right so what does this look \nlike actually the outer product of two vectors everyone gets this raise your hands ok \ngood \nso i do not need to do an example so it is fine right this is an outer product of these two \nvectors one happens to the quantity to be the q uantity that we already knew right and \nthe other happens to be a quantity that we can figure out  i mean we already know this \nwhat is we know how to compute the hidden representations right the hk\u2019s we can \ncompute \nrefer slide time eleventwenty \n \nso fine  so finally we come to the biases this is what one entry looks like this is \nexactly the sum which i had written out now i take the derivative with respect to b k i of \nthe loss function so i could write it into as this chain rule where the first qua ntity is \nsomething i already know i have computed the gradient with respect to the pre activation \nlayers what about the second quantity anonymous roar is what i was expecting \nstudent one \none ok fine we can now write the gradient with respect to the bias  what would it be \nwhat is this what is this it is just the gradient with respect to the pre activation layer \nright simple so now we are done with all the gradients that we were interested in"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.7 Backpropagation\uff1a Computing Gradients w.r.t. Parameters.wav", "duration": 724.75, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 foureight \nlecture  four \nback propagation pseudo code \nso we move on to the nex t modu le a nd now we will write pse udo code to for back \npropagation  \nrefer slide time zerotwentythree \nso we have all the pieces of the puzzle  we have the gradients with respect to the output \nlayer that was the special layer because the output activation function is different they \nare the gradients with re spect to all the hidden layers that means i have the gradients \nwith respect to the activations as well as the pre activations \nso in the h\u2019s as well as the a\u2019s and i also have the gradients with respect to the weights \nand the biases and this is all  index agnostic right that means i am just using k as the \nindex everywhere i have a generic formula which applies at any layer for the weights as \nwell as the activations and the pre activations right ok now we can put all this together \ninto a full learning algorithm so let us see what the pseudo code looks like \nrefer slide time onethree \n \nso we have this t equal to zero well run this for some max iterations we initialize all the \nparameters to some quantity will randomly initialize them ok now for these max \niterations can you tell me what is the first thing that i will do so there will be two \nfunctions here ok tell me what those two functions would be \nstudent forward \nforward propagation and then backward propagation right  so you do a forward \npropagation and you compute all these activations pre activations output layer loss \neverything and then you do this backward propagation where you feed all these things \nwhich you have computed  these are the quantities which you have computed  you will \npass this t o your backward propagation algorithm it would not look so nasty as this it \nwill not take so many parameters you could write it smartly  and then you will just do \nthe parameter update \nso what will the back propagation give you actually all the gradien ts all the partial \nderivatives right and then once you have the partial derivatives you know how to \ncompute the update law so now let us look at these two functions more carefully the \nforward propagation and the backward propagation \nrefer slide time twonine \n \nso forward propagation is simple for all the hidden layers that means from layer one to \nlayer l minus one what will i do give me the code a k is equal to good then ok and what it \nwhat is h of zero you are  starting the loop from one right  so you will n eed h of zero that is x \nand then you will have a special treatment for the output layer and your final output will \nbe whatever output function you use ok this makes sense you can write this in python \nyou will have to write this in python \nrefer slide time twofortyfive \n \n  \nnow we have computed all the h\u2019s and the a\u2019s what have we computed all the a\u2019s all the \nh\u2019s and all and the y right now you want to do back propagation so back propagation \nthe loop will be from i equal to one to n minus one  good so the first th ing i will compute is \nthe gradient with respect to the output layer see even here the output layer was outside \nthe loop the same thing would happen here also in the back propagation also first you \nwill compute the gradient with respect to the output layer and this is the formula \nif you remember from last class right that is the formula which i have substitute here \nand note that f of x is known to you because you computed that in the forward pass and e \nof y one hot vector which with a correct label said to one and you know what the correct label \nis because we have given you the refer time threefourteen data right ok then what would the \nloop be l to one or l minus one let us see  first you compute the gradients with respect to \nparameters it is l \nso because we ar e using k minus one  then you compute the gradients with respect to the \nlayer below computes gradients with respect to the pre activation right this is exactly \nhow you will proceed this is clear to everyone the same three components that we have \nused you might be a bit confused about the ordering in which we have put them because \nwe computed the gradients with respect to pre activation first and then the weights but \nonce you go back you will realize because it is the way we have indexed it because this \nis already outside \nso this has already been computed so you can already compute the gradients with \nrespect to the weights of the outermost layer  is that fine so this is straightforward you \ncan go back and check this  ok now anything remaining or you have  everything can you \njust take a minute and see if you can visualize the python code and we will just assume \nthat you are done the assignment you can read you will have multiple these vectors and \nmatrices and so on and you are just doing a lot of  matrix operations using refer time \nfoursix or refer time foureight or whatever you prefer right \nnow what is missing here input is missing ok input we have given right the ominous \ndata set has been given is there something that yours i have still not shown you  how to \ncompute oh i did not update the parameters here is it no the parameter update will \nhappen in the outer loop  right so those forward prop back prop  and then update the \nparameters right so the main algo rithm was forward prop back prop  update th e \nparameters when we saw forward prob an o bvious seeing backward prop  so what is \nmissing one thousand iterations something in the last line before end of course do you know \nhow to compute this"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.8 Backpropagation\uff1a Pseudo code.wav", "duration": 318.91, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fournine \nlecture \u2013 four \nderivative of the activation function \nwe have that activation function and we were taking the derivative of the activation with \nrespect to pre activation and i just pushed it under the rug by saying we will write it as \ng dash so i need to show you what g dash is \nrefer slide time zerotwentynine \nwhat how to compute g dash so this is suppose g is the logistic  function ok so that \nmeans what is z actually it is one of those a \u2019s right so this is the activation that you \nare going to feed it right and then you are taking the element wise  sorry z is actually the \npre activation that you feed it and then g is the  activation function so i will do element \nwise activation function now what is the derivative of this  so i will just i will not do \nthis derivation \nit is there and you end up with a very neat formula which is g of z into one minus g of z \nso now that bit is also taken care of is there any more spoon feeding that i can do you \nare ready for the assignment now i will do one more bit you will also have used a tanh \nfunction so this is the derivative of the tanh function it again boils down to a very n eat \nformula which is one minus g of zd whole square so we will end this lecture"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 4.9 Derivative of the activation function.wav", "duration": 72.92, "text": "deep learning\nprof sudarshan iyengar\ndepartment of computer science and engineering\nindian institute of technology madras\nmodule \u2013 fourten\nlecture \u2013 four\ninformation content entropy  cross entropy\nso for the next module we need something known as cross entropy so we will just try  \nto make some develop some intuitions for cross entropy and get to the formula for that \nand then i will tell you how it relates to the problems that we deal with ok so first let  \nus start with something simple that what is it that we are trying to do ok so with that i \nwill give you an example and i will ask you a few questions and then from there we will \nslightly\n try to go towards cross entropy so now suppose you have an urn which has \nthousands of balls and these balls are of three different colors which are red black and  \nwhite\nrefer slide time zerofiftyone\nso you have an urn which has three different types of and there are many such balls\nwhich you have put in it and since you have put in it you know how many red balls are\nthere how many blue balls are there and how many white balls are there so for you it\nis very easy to compute the probability of each of these things\nso that is say our probability is zerotwentyfive zerothirtyfive and zerofour now talking more formally what is\nhappening here is that you have a random variable x which can take on the values red\nblue or white right and this is the probability of each of those or the random variable x\ntaking any of these values ok so this is the setup now i am your friend and you tell me\nthat you can peep into the zone you cannot actually count take out all the balls and count\nthem and estimate the probability we can just take a look into this and try to give me an\nestimate of what these actually what are these probability values that means what is the\nvalue\nso what is the probability that x is equal to red or x equal to white or x equal to blue\nso i just take a look at it turn it around a bit and try to get some feel ok i see a lot of\nred balls but a fewer blue balls or white balls and so on and based on that i make my\nbest estimate right so i will just say that maybe these probabilities are zerothirtyfive zerofortyfive and\nzerotwo right so this is actually the true distribution i will call it as p right because this is\nthe correct one and what i have estimated i will call it as q\nand remember now p has you can think of p as a vector which has these three values p one p\ntwo p three because there are three possible events here and similarly q has three values q one q two and\nq three so in this case i clearly know that i am wrong or when i give you these values you\nknow that you are wrong so you tell me that whatever you have estimated is wrong\nthen i obviously ask you tell me how wrong was i so how would you give me that\nnumber that is the thing that we are interested in\nso the general problem that we are interested in is that there is a true probability\ndistribution and there is an estimated probability distribution and we want to find out\nhow bad was the estimation now can you tell me a simple way of computing this it\nmay not be correct but still it makes sense \nstudent squared error \nyou can just take the squared error so what you are essentially telling me is that you\ncould just treat these two as any other vector right and you could take the squared error\ndifference between these quantities  so what you are telling me is this where i goes\nfrom one to three ok so this is one valid way of doing this but then we are ignoring the fact\nthat this is a distribution and hence it has certain properties that the sum of the elements\nis one and so on all of them are positive and things like that so we are ignoring those kind\nof things we are completely ignoring the fact that we are not dealing with a normal\nvector but a spatial vector which happens to be a distribution so now we want to find\nout a more principled way of computing the difference between two distributions and in\npractice why are we interested in this because we will always have a true distribution\nand a predicted distribution so that is what we want to do we have some way of\ncomputing it but you want a better way of computing it\nnow let me make a case for why do we care about such differences right so let me take\na  simple  case  of a  classification  problem  and  to  motivate  that  i  will  start  from  a\ndifferent example and then i will come to the classification problem suppose there is a\ntournament going on and there were four teams which leads the semifinals let us call them\na b c d ok now you were following the tournament up to the semifinals and after\nthat you didn\u2019t watch the tournament and you do not know who eventually won well\nthe tournament is over and someone has won it\ni actually watched the tournament and i know that b has won it now can i express this\nin terms of a probability distribution right so first let us look at what is the random\nvariable here what is a random variable here the team which won right so that is my\nrandom variable and it can take one of these four values\nnow i know that team b won because i saw the tournament and i have seen that they\nwon so now how can i write this as a distribution what is the distribution comprised\nof it comprised of these probabilities assigned to each of these events and there are four\nsuch events here so how do i write this distribution so what you are telling me is i\ncould write it as zero one zero zero\nso essentially they are telling me that all the probability mass is focused on one of these\noutcomes because that is the certain outcome that is already happened no one can\nchange it so that is the outcome for this tournament so i know that the probability of\nthat even is event is one and everything else is zero so in other words the probability that the\nrandom variable x takes on the value b is one and everything else is zero\nso what i am trying to tell you is that even for a certain event you could still write it in\nterms of a distribution where all the mass is focused on that event now again i will\nbring the same setup that i did not watch the tournament after the semifinals so now\nyou  ask  me  give  me  your  prediction  what  which  team  would  win  or  this  is  the\nprediction which i made before just after the semifinals or just before the semifinals that\ni think one of these teams is going to win the tournament and the chance of each of them\nwinning is something like this so i know the teams i follow this sport and i probably\nknow that ok b has a very strong team and they have a very good record in the past few\nmonths and so on so maybe they have a higher probability of winning so these are the\nnumbers which i assign\nnow again i have made an estimate was my estimate perfect when would it have been\nperfect if i had predicted with certainty won that b is going to win but i was not willing\nto bet everything on b so i said there is a very high chance it will win but there is still\na chance that there could be some surprises now how wrong was i now again tell me\ncan you tell me what is p and what is q here this is the true distribution and this is my\npredicted distribution and what am i interested in again the difference between them\nhow wrong did i go and what again what is a simple way of doing this again square\nerrors so again this is what my formula would look like\nso this is fine in this toy case but why do we care about in real life examples that we\nare going to deal with in machine learning so in watching learning will deal with a lot\nof  problems  which  are  classification  problems  and  in  classification  problems  you\nwould again have this setup where you have a label the good thing of the label as a\nrandom variable and it could take off one of many values so i will again assume that it\ncould take suppose you are trying to take a picture of fruits and you are trying to\nclassify them\nand i could again think that i have four fruits say apple banana cherry and dragon fruit\nok and this random variable can take one of these four values depending on the image that\ni am seeing ok now i have been given some training data so for every training data i\nhave been given an image and i have been given the correct label so for that training\ndata what is this distribution suppose i have been given the image of a banana what\ndoes this distribution look like zero one zero zero right again i have seen it so i know it is\ncertainly a banana\nso i do not have any confusion all the probability mass is focused on that now the same\nimage we are going to show to one of our models ok and it is going to make a\nprediction and will again ask it to give us a distribution the model will give us values\nperhaps like this ok so this is the models prediction again the model has given us a\ndistribution and we have a true distribution and we are interested in knowing how wrong\nthe model was so that\nstudent correct the refer time ninetwentysix\nwe can correct the parameters of the model so this is our dash function loss function\nright so a loss function is some notion of difference between p and q right and so far\nwe have been dealing with a very simplistic notion of this difference which is just the\nsquared error loss ok and we want to do something better than this right so what i\nhave told you is that you could always have a true distribution always have a predicted\ndistribution and you would be interested in finding the difference between them that is\nthe one first part the second part is that even when you are given something with\ncertainty you could still write it as a distribution such that all the mass is focused on that\nevent which was which has happened right which was the label was banana in this case\nand then you could still predict this from your model and now you are interested in\nknowing how wrong you are model wind because that is the loss function that you will\nuse and then you will try to update your parameters with respect to this loss function\nmeans that is the setup that we are interested in so that is so i made a case for why we\nneed to find differences between two distributions how to do it in a more principled\nmanner we have not seen that yet we will get to that ok so before i get that i also need\nto tell you something about expectations so let us written to as sports example where\nthere were four teams and say based on pundits and that sport they have said that these are\nthe probabilities of winning\nand now you are into betting and you bet place your bets on these teams and you place\nour bets in a way that suppose team events then you end up winning ten k rupees if team\nb wins then probably will end up winning five k rupees and if team c wins probably ten k\nand if one of the other team wins maybe will end up losing money or something like that\nnow you want to know what is your expected reward so now let us see what is\nhappening here this was a random variable which could take on one of these four values\nthese are the probabilities of the random variable taking that value taking on the value\na or taking on the value b c and d ok this is your value or the gain or the profit\nassociated with the random variable taking one of these values\nso you have a random variable you have a probability associated with every value of\nthe random variable and you have some gain or value associated with every value of the\nrandom variable now how do you calculate the expected gain or expected profit which\nis this there is a thirty percent chance that you will earn ten k there is a forty percent chance\nthat you will earn five k there is a twenty percent chance that you will earn ten k and ten\npercent chance that you lose thirty k\nso the way you will compute it is that and this is the simple expectation formula which\nis the probability of now the event here belongs to abcd right this is one of the four\nteams that will win probability of that event happening in to the value associated with\nthat event happen this is a fair computation you get the intuition that this is how you\nwill compute the net reward that you have so this is how you compute the expected\nvalue with respect to a particular distribution so this is the background that we need\nnow i will just go on to the next slide and now we will talk about entropy first perhaps\ninformation content be first then entropy and then cross sector ok so now what is\ninformation content\nrefer slide time thirteentwentyeight\nso now again let us take the same case that we have a random variable which can take\non values a b c d now let us what we are trying to say is that if i know a certain thing\nwhat  is the information  that  i have gained  so you and i are  talking  you tell  me\nsomething  and  i  want  to  see  whether  my  information  was  enhanced  whether  my\nknowledge was enhanced that is how we will quantify information content\nso if you are talking to me and you tell me that my name is mitesh the zero information\ncontent for me right because i already know that there is no surprise in that ok but if\nyou talk to me and you tell me that today there is going to be a lunar eclipse then there is\na possibility there is some information content gain for me right because that is not a\nevent which happens every day if you just tell me you will see the moon today and you\nlive in a region where it is not typically cloudy and there is no information gain there\nright so what do you see here when is the information gained high when the event\nwhich happens is a very surprising event and how do you say supplies in terms of\nprobability\nstudent refer time fourteentwentynine\nit is a very low probability event right so if there is again this tournament and say d\nwas the weakest team in the tournament and a was the strongest team in the tournament\nif  you  come  and  tell  me  that  d  won  then  i  would  be  really  surprised  that  some\ninformation which i had gained but if you tell me that a won then probably i already\nknew  it  at  the  back  of  my  mind  because  a is  clearly  the  strongest  team  in  the\ntournament and there is no information gained for me\nso one thing that we are trying to establish that the information content i see ok the\ninformation content of an event is inversely proportional to the probability of the event\nthere is a that is a fair intuition fine now i want am still talking in terms of vague\nthings i am saying it is inversely proportional but i still need an exact function so that i\ncan compute it so i want something i want a function where i plug in the probability of\nan event and i get the information content of the event\nright now i do not have that function i am just building some intuition towards that\nfunction  ok but  this is one  requirement  that  i want  the  function  to satisfy  this is\nsomething that all of us agree with ok now think of two events which are independent a\nand b ok so a is the event whether the ac is on here or not and b is a event which\ntells maybe so let us consider two different random variables\nso x is the random variable which can take on values zero and one sorry so x is again this\nrandom variable which can take on these four values abcd whether who won in the\ntournament and y is this another random variable which can take on the value on and\noff depending on whether the a c\u2019s on in this room or not what can you tell about these\ntwo random variables they are independent random variables\nso this is on or off and this is which team won the tournament now i come and tell you\nsomething about the random variable y and i come and tell you something about the\nrandom variable x ok so now i want you to tell me this what is the information\ncontent of x and y i tell you something about x and i tell you something about y and\nthese two events are in these two random variables are independent then what can you tell me\nabout the information content what is the condition that you would want you gain\nsome  information  by  knowing  things  about  x  and  you  gain  some  information  by\nknowing things about y so what can you tell me it should be the sum right\nbecause these two are independent events so whatever information i am getting from\nthis random variable and this random variable which together enhancing my information\nright it is not cancelling out anything or is there is no common intersection there right\nif the two events were not independent  then i would not expect  this to hold because\nknowing something about the first event only tells me something about the second event\nright because they are dependent\nso then that case the information gained would not be additive ok so now let us see i\nalready made a case that this function which tells me the information gain is actually\nproportional to the probability ok so that means this is what the input is going to be\nright and then what is the other condition that i want this is a fair thing right i just\nreplaced information content by a function and i know that the function should depend\non the probability because that is what we have established here\nso we know that the function depends on the probability we still do not know what this\nf is exactly but i am trying to impose some conditions on f one condition of f f is that\nthis condition should hold ok now let us look at this condition which i have underlined\nthis is f of is this fine because it two events are independent you can write them as the joint\nprobability as p of x into p of y this is clear to everyone right you seem to be a bit lost\narvind clear ok\nnow what is happening here i have a function f of a into b and that is actually equal to f\nof a plus f of b what family of functions do you know which has this characteristic log\nright that is why log is a good choice for this that is why information content is going\nto be the log of the probability but i wanted to be inversely proportional right so it will\nbe log of one by the probability ok so that is why information content of this thing is so\nyou see this how did we arrive at this log formula\nand this log can just be to any base it does not matter so all of you get how we arrive\nat the formula for information contained now just give me a minute i need to think of\nwhat is the next thing that i have to say ok and so we have found out the information\ncontent of one of these events happening which was the x taking on the value a\nnow let us think of this random variable x so here actually i should have said x equal\nto a probability of x equal to a ok it makes sense because the random variable is x and\nthe event is x taking on the value a how much information content is in that so if i\nknow that x was a how much will i be surprised by it ok now let us take this event this\nrandom variable x which can take on values a b c and d as i said with each value there\nis a probability associated with it such that this sums to one now i did not need to draw\nthis diagram ok i should refer time twentyoneone\nso x is a random variable which can take these four values which each of these values i\nhave a probability  associated  ok so these are the values these are the  probability\nvalues now what do i also have i have the information content associated with each of\nthese right and the information content actually tells me the surprise of that evening\nnow if i ask you what is the entropy of this random variable x so remember i had this\ncase where i was betting i am with every poor outcome i had a value associated with it i\nhad the same situation here with every outcome have a probability and i also have a\nvalue associated with this and the value is the information content\nnow if i ask you what is the entropy or the expected information content of this random\nvariable then how will you compute that i am asking you for an expectation\nrefer slide time twentytwosixteen\nso i will compute summation i belonging to a b c d p of x equal to i information can\ntake what is the formula for that\nstudent minus refer time twentytwotwentynine\nminus i will just take the minus outside ok so this quantity is called the entropy of the\nrandom variable right it is the expected information content in the random variable\nnow  if  you  see  what  would  be  the  expect  entropy  of  a  random  variable  if  it  is\ncorresponding to a certain event that means say the sun rises always in the east right\nso what is going to be the entropy of that zero why you will have one of the sums in that\nsummation as one log one right and every other sum would be zero into log of something\nso zero into anything is going to be zero even though that quantity is not defined zero into\nanything is going to be zero so the total entropy is going to be zero ok so this is entropy\nnow what is it that we are actually interested in cross entropy so we have not gone\nthere yet ok so we need to perhaps add one more slide so far everything is clear ok\nso now we are interested in something known as cross entropy\nrefer slide time twentythreethirtysix\nso there again the situation is that there is something which is the true distribution and\nsomething which is the predicted distribution  now actually before going there so let\nme just erase this off how many of you have thought that entropy is related to the\nnumber of bits that you need to transmit something do you know why that connection\nexists no now again let us think of this that you are trying to transmit a message\nrefer slide time twentythreefiftynine\nand that message is again a random variable which can take on four values a b c d so\nthink of these as four commands that we are trying to send to someone right and then\nbased on that command someone will take some action now in the digital case how will\nyou transmit this encode it to bits so what is the encoding that you will use zero zero yeah\nwe will come to that zero one one zero one one so how many bits are you actually using for every\nmessage\ntwo bits ok for every bit so maybe this is a this is b this is c and this is d so for every\nmessage you are using two bits let us see actually what when you are doing this what are\nyou actually assuming so actually assuming that all of these are equally likely if all of\nthese are equally likely can you tell me the information content of any of them it is\ngoing to be minus log of one by four ok that is actually equal to minus log and this is to the\nbase two ok one by four is two raise to minus two that is equal to two\nso the information content is actually equal to the number of bits that you are going to\nuse to transmit that message now let us see if this is just in this special case or in a\ndifferent case also suppose this could take eight values how many bits would you use three\nbits right so you will have zero zero zero zero zero one ok and this would be a to h now what are we\nactually assuming here each of these is equally likely what is the probability one by eight\nwhat is the information content two raise to minus three that is equal to three\nso the number of bits that you actually use to transmit something this can you can talk\nof it in terms of the information content of that now suppose i want to transmit this over\nthe long distance so i need to bit be a bit efficient in terms of number of bits that i use\nright so now in this one of these cases suppose it is of the following form right that\nlet us look at the case where x can take one of four values and say let me just put the right\nvalues so i will say one by two one by four one by eight one by eight ok now what is the information\ncontent of each of these one two three three and this is the message that i am going to send\nso what am i doing here i am using a different number of bits depending on the\nprobability of that event why does this make sense why is this a smart thing to do if\nyou want to transmit something which you are going to transmit a lot of times you better\nuse less number of bits for that and this is exactly what is happening here a was having\nthe highest probability and you are using the lowest number of bits for that now what is\nthe expected number of bits that i will use up if it is a i will use one if it is b i will use two\nif it is c three and d three\nso what is the expected number of bits that i will use again i have the same situation\nright i want you to cast it into the same situation i have the probability values and with\neach of these guys i have a cost or a value associated what is the cost one bit two bit three bit\nthree bit so what is the expectation now can someone compute the expectation oneseventyfive\nactually let me just write it down it would be again i belonging to a b c d p of x equal\nto i into the number of bits that you will use so that is just equal to log of log to the\nbase two of p of x equal to i minus one what does this quantity actually this is the entropy\nwe just saw this a this is the entropy of the random variable and what is it telling us\nactually that the entropy is oneseventyfive\nso what is the meaning of this actually so on average you will be needing oneseventyfive bits\nwhereas if you are assuming everything is equally likely on average you are using two bits\nright so you see that on average you are making some savings here right so that is\nwhat the entropy tells you if you know what the probability of these events is then you\nbetter use that to decide the number of bits that you are going to use to send each of\nthese\nso  now  let  us  complicate  this  a  bit  more  now  we  have  the  entropy now  let  us\ncomplicate it a bit more so there is some true distribution which exists there is some\ntrue distribution from which these messages are coming right but you do not know\nwhat that true distributions we never know the true distribution that is the entire problem\nthat we have been dealing with in machine learning\nso what you will do is we will somehow try to predict this distribution and this then the\nand the recipe that you will use is the same as that i used for the example where i had an\nurn right so there are these thousands of tenzero\u2019s of messages which has going to keep\ncoming  on  you  do  not  have  access  to  the  entire  stream  but  you have  seen  some\nthousand of those messages just as i had peeped into the urn and i had seen some balls\nand i had made an estimate that i think based on these messages that i have seen so far i\nthink these are the actual probabilities\nso the true probabilities are say p one p two p three and p four corresponding to a b c and d i\ndo not know what this two properties are but i can estimate them looking at some samples\nor basically using my domain knowledge right maybe i would know that if one of these\nmessages is stopped and i am actually trying to talk to a computer or a computer\nprogram that maybe stop is something which are used very rarely only at the end of the\nprogram or something so you have some either some domain knowledge or based on\nsome samples i can estimate the value of this probability\nand i just try to relate it to the exact example of urns where you had these tenzero\u2019s of\nballs but you could not see all of them you sampled some and estimated a probability\nhere again there is a continuous flow of message you cannot have access to all of these\nbecause they are going to continue but i have seen some of those and based on that you\nestimate these probabilities now based on this estimation how many bits\nso now this is the estimation that we have now based on this you will decide the\nnumber of bits that you will use for each of these messages right because you have\nsome estimate so you want to be smart you do not want to keep two bits for all of them\nso you will just say that i will use log qi bits for the i\u2019th message\nrefer slide time thirtyonetwentythree\nthis is fair thing because i know that the information content is proportional to the\nprobability in fact it is exactly given by this formula minus log of qi so based on my\nestimated probability i am going to do this ok and this is the number of bits that i have\nreserved now do you see a problem with this this is my estimation but the data is\nactually going to come from the true distribution it is not going to follow the distribution\nq it is going to follow the distribution p\nso now what is actually happened is this right this is the situation that we are dealing\nwith we have p which was a true distribution that is the rate at which the data will\ncome but with each of these events the value that we have associated is now related to q\nbecause q is what i have access to i do not have access to p i just have access to q so i\nhave associated a value based on q does this make sense i should have actually used\nlog p one bit\u2019s log p two bits and log p three bits but i do not know what p one p two p three are\ni just estimated them based on some samples so that is q one q two q three and these are the\nnumber of bits that i am using now if i have to compute the expectation how will i do\nit i have to use p because that is the true distribution from which the data is coming\nso  what  would  the  expectation  now  look  like  everyone  gets  this  the  actual\nprobabilities are this but because i am poor at estimating them i ended up associating\nthese values which could be wrong right because i would have overestimated the\nprobability of one of these messages and hence i have reserved lesser bits for that or\nunderestimated the probability of one of these events and hence reserved more bits for\nthat or vice versa\ni could have assigned a wrong number of bits to them right p no so do we have access\nto p in the sense so someone knows that right i mean there is a again in the same case\nas in the label case right we have access to the true p there and we are estimating a q\nwhen we are given these images for the training data we know that the distribution is zero one\nzero zero if the image is b for banana\nstudent refer time thirtythreefortynine\nthen it is validated right so now this is what is this quantity called this is called the\ncross entropy ok you get why it is called the cross entropy because now you have two\ndifferent distributions involved here ok you have the q distribution based on which you\nmade your decisions you assign values to these events based on the q distribution but\nthe true distribution is the p distribution\nso the actual number of bits that you use up on average is going to be based on the true\nprobability they try to understand that now what will happen is for event a you have\nassigned a certain number of bits now how many bits will get used up it depends on\nthe actual probability of p if that message is repeated many times then that is how this\nsummation would be computed\nso  this  is  called  the  cross  entropy but  now  why  is  this  the  difference  between  two\ndistributions that is what we wanted given two distributions we wanted to be able to find\nthe difference between them now am telling you that cross entropy is a way of finding\nthat difference why is it so so what would you want this difference to what is the\nproperty that you would want this difference to have if p is equal to q then if p is equal\nto q then\nstudent refer time thirtyfiveeight\nnot zero maybe it should take the lowest possible value right so this function right this\nis actually telling you loss of p comma q right this is what this is and we are calling it\nas the cross entropy this function you take it is minimum value when p is equal to q\nright because now at that point you are not really making any loss that is the best you\ncould have done does this function take it is minimum value when p is equal to q yes\nwhy how is that obvious but why there could be something else which is lower than\nthe actual entropy right why how you have to we are trying to minimize something\nso you have to give me answer\nso yeah so let us do that ok so how many of you it is obvious that q is the answer i\nmean the answer is p is equal to q it is not ok now this is the part which i am a bit\nworried about but i will just do it anyways so let me see how do i put this ok so\nremember that we had a p and we had a q and we want to find a q such that this quantity\nis minimized that is what our objective is\nso we want to minimize this with respect to qi ok now how do you find the minimize\nsuppose i have this problem how do i find the minimum value how do i find the value\nof x which minimizes this take the derivative and set it to zero ok and then in this case i\nwill get x equal to zero is that value can i do the same thing here and suppose it was this\nso now this is a function of two variables again i could do the same thing i could take the\npartial derivatives and set them to zero\nand i will get the minimum value now here this is actually a function of how many\nvariables k in general right so q one q two q three up to qk ok now can you try doing the\nsame thing can you can you take the derivative and set it to zero this is again a sum right\nit is very similar to this situation it is actually let me just write it down it is p one log q one\nplus p two log q two up to p k log qk\nnow i want to take the derivative with respect to one of these guys say q two what would\nit be p two by q two is equal to what will i do that is the derivative p two by q two i will set it\nto zero do i get anything what is it that i am doing wrong here there is something that i\nam deliberately doing wrong is this an unconstrained optimization problem there is a\nconstraint on the variables what is the constraint so why my true optimization problem\nis minimize with respect to q i\u2019s such that summation q i\u2019s equal to one\ndo you know an easy way of dealing with these problems how many of you know the\nlagrange multiplier how will i use it here what will my objective function become\nthen summation of qi minus one lambda then minus ok how many of you understand the\nintuition behind this that is a good answer now let us let me try to explain why this\nmakes sense right this is the constraint that we have to operate within this constraint\nwhat i have done is i have taken the so now if the constraint is not satisfied what will\nhappen to this quantity if the constraint is not satisfied that means my summation is\nnot equal to one that is what means whether the constraint is not satisfied what will happen\nto this quantity it will be a nonzero quantity right fine then what will happen to my\noverall objective and i think we have made a mistake this should be plus i should add\nit right should be plus no it does not oh the lambda can be ok sorry so let me assume\nthis is plus\nso what i am trying to do is that this is my objective function which i am trying to\nminimize i have added another quantity to it if this quantity is not equal to zero then i will\nnot be the absolute minimum i will be at the minimum plus something right but if this\nquantity if the constraint is satisfied then this quantity will go to zero then am actually at\nthe minimum of the function do you get this right so this is the function that i want to\nminimize i have added some quantity to it now that quantity is actually related to the\nconstraint that i do not want to violate if i violate the constraint this is going to be non\nnegative right\nso whatever minimum value i achieved i will be slightly higher than that because some\nnonnegative value has got added to it ok is that fine but if the constraint is satisfied\nthen i can achieve the minimum value so that is roughly the intuition behind using this\nlagrangian multiplier it is a very crude intuition but there is of course a lot of math\nbehind that but i am just giving you the intuition behind this which one\nstudent refer time fortyonethirtynine\nyeah that is what you could adjust the lambda and ensure that it is not negatively ok so\nnow now can you do the same thing can you equate this to zero can you take the derivative\nand equate to zero what will you get now this term will give you p i by q two as before oh\nsorry p two by q two as before plus lambda times yeah plus lambda times one ok fine so\nequal to zero so then what will you end up getting p two is equal to i think it is something\nwrong here now this should be minus p two by k\nso p two is equal to lambda times q two ok and then further actually you can show that\nlambda is going to be equal to one how can you show that your constant is fine so do you\nsee how we will get lambda equal to one so what does it actually tell you then p two is\nequal to q two that means all in fact you can show that all p i\u2019s are equal to q i\u2019s that\nmeans the distribution p is equal to distribution q\nso this cross entropy term will be minimized when your true distribution or when your\nplated distribution  is  the  same  as  the  true  distribution  and  hence  it  captures  the\ndifference between the two ok and that is exactly what we were interested in we were\ninterested in a quantity which can allow us to capture the difference between the true\ndifference between a true distribution and the predicted distribution\nso we have arrived with that quantity and that quantity is cross entropy so therefore\nfor all our classification problems where we have this scenario that we are given the true\ndistribution where all the masses focused on one of the labels and you are estimating a\ndistribution where you could give non zero quantities to many of those and you want to\nfind out how wrong your estimates were with respect to the true distribution you can use\ncross entropy as a measure for that right so now your loss function which you wanted\nto depend on the difference between p and q it can just be the cross entropy between p\nand q"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.1 & Lec 5.2 Recap\uff1a Learning Parameters\uff1a Guess Work, Gradient Descent.wav", "duration": 510.86, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  five \ngradient descent gd momentum based gd nesterov accelerated gd \nstochastic gd adagrad rmsprop adam \nwelcome to lecture five of the course  on deep learning and so today we look at  some \nvariants of gradient descent so we will just quickly do a recap of gradient  descent and \nthen look at  some variants  of it or some  ways  of improving  it which  is momentum \nbased gradient  descent nesterov  of  accelerated gradient  descent stochastic  gradient \ndescent adagrad rmsprop and adam \nso just to set the context so we started with this gradient descent algorithm for a single \nsigmoid neuron and  then we  saw  how to extend  to network  of neurons  with back \npropagation so we realized that all  we need is the gradients or the partial derivatives \nwith respec t to all the weights a nd biases once we compute that we ca n just use the \ngradient descent update rule \nnow today what we are going to see is are there better update rules which lead to faster \nconversion or better performance in various ways so that is why we are going to look at \nall these differe nt variants or methods of improving on gradient descent so that is the \ncontext \nrefer slide time oneeighteen \n \ni will just quickly rush through  so for most of the lecture  i have borrowed ideas from \nthe videos by  ryan harris on visualize back propagation and some content is based on \nthis course by andrej karpathy and others when i talk about some tips for  learning rate \nand so on so you can just look at those also  so we will just quickly rush through the \nfirst two modules which we have already done \nrefer slide time onefortysix \n \nwhich was  we were interested in learning the weights and  biases for this very toy \nnetwork with just  one input and one output and we started by doing something known as \nguesswork where we were just trying to adjust these weights and biases by hand \nrefer slide time onefiftytwo \n \nrefer slide time onefiftysix \n \nrefer slide time onefiftyeight \n \nand we realize d that its clearly not good and  but we still try to do a very  smart guess \nwork where we were driven by this loss function  which was telling us whether this \nguess the current guess is better than the previous guess or not  and we just kept \nfollowing our guess work and try to reach to some solution and for this toy network it \nwas very easy to do that \nrefer slide time twosix \n \nrefer slide time twoseventeen \n \nand what we were actually doing is  there is this error surface which  exists which can \nbe plotted for all possible values of w comma b and what we were trying to do with this \nguesswork is trying to find path over the error surface so that we enter into the better \nregions so red is bad blue is good the darker the shade of blue the better  and this of \ncourse becomes intractable when you have many parameters and so on \nrefer slide time twothirtyeight \n \nso we wanted to have a better way of navigating the error surface  so this is exactly \nwhat we were doing with the guesswork algorithm \nrefer slide time twofortyeight \n \nso then this better way actually we realized that  we could arrive at it from a very \nprincipled solution from starting from taylor series \nrefer slide time twofiftyfive \n \nand we went to this derivative  where we finally came up with this rule that move in the \ndirection opposite to the gradient \nrefer slide time twofiftyeight \n \nrefer slide time threethree \n \nso that is the rule that we have been sticking to since then  and we also along the way  \nrealize some of these things which we defined carefully which was what is what exactly \nthis quantity means  which is the partial derivative with respect to w evaluated at a \nparticular weight comma bias configuration and because this is an iterative process you \nare at a certain value of weight and bias and you need to change it from there \nrefer slide time threethirty \n \nand we then created an algorithm out of this and when we ran this  we actually derived \nthe full derivative and so on \nrefer slide time threethirtyeight \n \nand then when we finally ran this algorithm  so this is where  now i will slow down  \nso when we ran this algorithm  so let us see what was happening here right  so i will \njust start the algorithm from the beginning \nso we are now going to run this code and you tell me somethi ng that you observe ok  \nso i am just clicking  so there is no change in the pace at which  i am clicking this \nright so every click of this is  one time step and  i am just continuously clicking this  i \nwill start now do you observe something fl ok do you observe something \nit was initially slow then suddenly picked up and then it again became slow  why did \nthis happen the slope is small  why ok how many of you completely understand why \nthis slow and fast moment was there  please raise your hands good  so that is what we \nwill focus on now right so we will try to see this \nrefer slide time fourthirty \n \nso we will i hope this has been fixed ok  so let us take a simple function which is f of \nx equal to x square plus one right this is how it will look lik e now in these portions of the \ncurve the curve is actually very steep right and in these portions the curve is a bit gentle \nand of course it becomes very gentle over here right  all of you can see the pen marks  \nproperly \nso now let us see what this mea ns this steep and fast and small  so let us look at a \nregion which is steep ok now what i am going to do is i am going to change my x by one \ni move my x from  one to two how much did my y change  all you need to do is just \nsubstitute in this formula right for two it evaluates to five for one it evaluates to two so when you \nmove from one to two your function changed from two to five ok so there is a large change in \nthe function for one unit change in your value of x everyone sees that \nnow let me do the same at a gentle portion of the curve  i will do it here  now when i \nchanged the x by  one unit again one unit right it is the same change which  i did earlier i \nchanged from zero to one how much did my y change \nstudent one \noneok now actually what is this quantity delta y one by delta x one \nstudent slope \nit is the slope it is the derivative at that point so what are you inferring from this what \nhappens to the derivative when you are at steep slopes \nstudent it is high \nderivative is high  because the change in y is much fa ster than the change in x  what \nhappens to the derivative when you are at the gentle slopes \nstudent smaller \nsmaller because the change in y is small or relatively smaller as compared to the change \nin x or it could also  be missing but just these two ar e relatively different  is what i am \ntrying to impress upon right  and so that means the derivatives at the steep slopes are \nlarger in magnitude whereas for the gentle slopes they are smaller in magnitude \nnow can you relate it to the observation that  you had on the previous slide  when we \nwere at the plateau it was a very dash slope gentle slope what would the derivatives be \nstudent small \nsmall now what are our updates  you have w is equal to w minus  the derivative right \nnow the derivative is small what will happen to the updates \nstudent small \nthey will be small what would happen if the derivative is large \nstudent the updates would be large \nthe updates would be large therefore in the gentle areas you are moving slowly and in \nthe steep areas you are moving fast  you get this picture very clearly  now this is going \nto be the basis of a lot of things that we do today  so it is very essential to that you \nunderstand this perfectly all of you get this properly good \nrefer slide time seventhirtytwo \n \nnow now you might say that  this was only that special point again and  i always get \nthose questions so let us see what happens if you start from a different point \nrefer slide time sevenforty \n \nso now again the same gradient descent algorithm  i am going to run  but instead of \nstarting at this point which was my random initialization  i just happened to choose a \nvery different random initialization which is here everyone sees that \nnow let us see what happens  what do you expect initially fast movement  because the \nsteep the slope is a bit steep now what would happen it will become slow because you \nhave entered a gentle slope region and then again fast right  so and then again it will \nbecome slow \nso see in this gentle region right the changes in w are so small that all your black points \nare actually indistinguishable from each other it is almost like a snakes body whereas in \nthese steep slopes you can see a large change in the w  you can see gaps between the \nvalues of w  right so this is irrespe ctive of where you start from  gentle means slow \nmovement steep means fast movement that is the basis"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.3 Contours Maps.wav", "duration": 629.38, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  fivethree \nlecture  five \ncontours maps \nso we look at something known as contours \nrefer slide time zerosixteen \nso now visualizing things in threed can sometimes become a bit difficult  especially for the \nperson who is making the slides  so we can  can we do a twod visualization of this \ntraversal have i done this in the  ml course no good can we do a twod visualization of \nthis traversal along the error surface \nso for that we need to understand something known as contours how many of you have \nlooked at contour diagrams before how many of you know how to read them  all of you \nknow how to read them \nrefer slide time zerofortyseven \n \nso let us see now suppose this is what my error surface looks like and  i have a single \nscalar variable so this is just a function of w for example and this is what my error \nsurface looks like \nnow what i am going to do is  i am going to take horizontal slices on  this error surface \nfine now can you tell me how this is going to look from the top  sorry let me  you \nshould start answering before  understand the question  this is this error surface is \nactually so i was wrong in saying this is theta  assume this is w  comma b and you are \njust seeing the front view of the error surface  what you are seeing here is just the front \nview \nthis error surface is actually like a dementors hat  so right so imagine that it  is a hat \nplace like this and you are just seeing the f ront view of this  otherwise a top view does \nnot make sense right so now i am going to slice this hat at two vertical positions  and \nnow you are looking at it from the top what are you going to see \nstudent ellipsis \nellipsis everyone agrees with that \nrefer slide time onefortynine \n \nso we will see something like this  do you see something peculiar about this  is this a \ncontour map is this no ok and all of you raise your hands when  i asked do you know \ncontour so do you see something peculiar about t his what is it  how many if you get \nthat so what you are seeing here is  this portion right where the slope was very steep  \nthe difference between the two circles or the two ellipses is small and you can visualize it \nif you try to look at it from the to p this distance is actually going to be small right  and \nin the areas where the slope was gentle  relatively gentle the distance is more and you \ncan again visualize it right  from if you look at from the top this is the distance that you \nare going to see  and what do you say about these guys  what does that indicate  they \nare the same \nstudent refer time twofortysix \nvalue across that entire region the value is same  because you have taken a verticals  \nyou have taken a horizontal slice at a particular vertic al position right  so you have \ntaken a horizontal slice at this position  that means the error is going to remain the same  \nthroughout that rim is this clear to everyone ok it is very important that you understand \nthis \nso there are only two things tha t you need  to understand if you want to read contour \nmaps one is a small distance between the contours indicates that the steep slope exists \nalong that direction  and a large distance between the contours indicates that a gentle \nslope exists along the di rection so everything today is going to be about steep and \ngentle slopes and the other thing that you know need to know is that whenever you see \none circle the error is the same along that circle or ellipse  whatever you boundary that \nyou see the error is the same  because you are taking these vertical slices  so we are \nready with this rule everyone understands this perfectly \nrefer slide time threeforty \n \nso i will just give you a couple of exercises and you have to tell me whether you \nunderstand this or not \nrefer slide time threefifty \n \nso i have plotted a three d surface or two d i have what is this \nstudent refer time threefiftyfour \nno \nstudent there is a contour \nthere is a contour everything is not going to look like clean circles always right ok  so \nthis is a contour  every line that you see here represents one cut along the vertical axis \nright that means the error is the same there  now what you are seeing is a contour  i \nwant you to guess the three d surface from this  you just guess it  i mean just k eep it to \nyourself fine the color is the same right blue is good red is bad \nso blue means the darker  the shade of blue the lesser the value of the error  the darker \nthe shade of red the higher the value of the error ok  i want you to imagine the three d \nsurface if you can do that then  i will be sure that you understand what how to read a \ncontour how many if we can imagine this you can just say yes right i can never figure \nout whether you actually speak it \nso let me help you with the first one and then  we will do a few more  so let us start \nwith the extremes right so let me see how to do this so this portion i also need to do it \nfor the video ok so let me just do it here  so this portion what do you think about the \nslope there very flattish wh y because this is the line that you see and the other line is \nnot even in the figure right  so it is basically very flat  the slope is very gentle  is it a \nlow region or a high region high region fine ok  now what is actually happening here  \nwhat is the slope here \nstudent high \nvery high that is why these two regions are very close to each other  so from this high \nregion what is happening  suddenly there is a slope and you are going down and you \nknow you are going down because you are reaching a blue region right ok now what is \nhappening here \nstudent very flat \nvery flat and this also flat  but slightly upper than the lower guy  is that fine now can \nyou all imagine this ok  and is this what you thought it is  perfectly yes right is exactly \nwhat you thought ok just a minute so the orientation here has been changed a bit right \nso this portion actually corresponds to this portion are the two this is clear this portion \ncorresponds to this portion right the just orientated fine \nso you start o ff this high plateau region which is here  then you start going down go \ndown and then you see a fold here right  that is this fold so you went to a darker shade \nand then you came up to a slightly lighter shade the shades are ok \nguess the threed surfaces how many if you want to play this forever now  start with the \nextremes the bad guys the good guys the plateaus and the valleys and then see how do \nyou go from the plateau to the valley ok tell me the corners first this plateau or valley \nstudent plateau \n plateau this plateau higher than this or lower than this \nstudent lower than this \nlower than this this \nstudent valley \nthis towards the valley  it is still between red and blue right  it is not like right down \nthere and what happens to all these guys all are very steep slows  all converging down \ninto the valley so can you perfectly imagine this \nand you will tell yes when i say when i show you the three d surface right again you need \nto reorient yourself so this corner here is this corner  this corner here is this corner  so \nwe had these two plateaus at the top  we had this slightly higher valley slightly lower \nvalley and then all of them going into a very deep valley  you see that everyone gets \nthis how many if you have a problem with this if you have a problem with this you \nwill just sleep off in the rest of the lecture  so i want you all to understand this very \ncarefully i do not mind repeating it  how many of you understand this  you understand \nthe regions with gentle slope \nstudent yes \nthe regions where you have a steep slope and you end up into that valley  which is the \nvalley here can you point it out fine ok so we will move ahead \nrefer slide time eightten \n \nso now we know what contour maps are and how to visualize them and  so on right so \nnow we will try to see the gradient a descent algorithm  instead of running it on the threed \nerror surface we will try to run on this twod contour map \nrefer slide time eighttwentythree \n \nso this is what i already showed you right i started from here and i showed you how it \ncomes here or something like this  right that was the gradient descent  let me just erase \nthis ok that is something like what the gradient descent algorithm \nnow again you just need to reorient yourself  so let us see this corner is this corner  \nthis corner is this corner and  so on right so you get the reorientation right  it just \nshifted now i am going to start my gradient descent algorithm from here from this point \nok everyone see is that ok i am going to start from there and you have to help me and  i \nam not going to just keep clicking you have to tell me what is going to happen so what \nwill happen initially fast movement slow movement \nstudent slow movement \nslow movement right  so  i am running it one two three four five six seven eight  it just keeps running very \nslowly now what will happen \nstudent fast \nfast ok now you see actually you can see the arrows these arrows are the quantity  the \nmagnitude of the movement right so earlier this movement was so small that you could \nnot even se e the arrows  i have been drawing arrows right from the beginning  but you \ncould not see them at the beginning  now you can see them right  now what will \nhappen \nstudent slow \nslow right so you see the exact same movement that  i did on the three d surface now you \ncan visualize it on the two d surface right and you can easily tell me where it will go fast  \nwhere it will go slow right and where it will just keep moving very drag its feet and so on \nok so this is where it starts dragging its feet  and the same thing happened when we \nwere in this region right  so just you just make the connection that we are in the \ncorresponding three d region there ok fine so we are moving very slow and it just keeps \nrunning \nso that is where we lend this module  so we just revised gradient descent we saw that \nthings are proportional to the gradient that is why gradient descent  and the smaller the \ngradient the slower the movement  the larger the gradient higher the movement  gentle \nthe slope \nstudent smaller \nsmaller the gradient steeper the slope larger the gradient"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.4 Momentum based Gradient Descent.wav", "duration": 1084.82, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fivefour \nlecture \u2013 five \nmomentum based gradient descent \nin this module we will look at momentum based gradient descent \nrefer slide time zeronineteen \nso what were the observations about gradient descent that  it takes a lot of time to \nnavigate regions having a gentle slope so what is the practical implication of this in \npractice why it what does this need to what does this mean right it takes more time so \nremember we had said this max iteration equal to one thousand \nnow if you  are initialization happens to be such that you are stuck in this large flat \nregion then those one thousand iterations just keep moving around that flat region right  you \nwill not enter into one of the valleys and valleys is what you are interested in right \nbecause values is where you will have some minima for your function right \nso if you have a very gentle slope then for one thousand iterations you will keep moving around \nthat gentle slope right that is why this has a practical implication  now this was because \nthe gradient in these regions were small can we do something better that is the question \nright so yes we can and we will take a look at momentum based gradient descent \nrefer slide time onefourteen \n \nso here is the analogy which i give my ta\u2019s have heard this at least ten times so i will \njust repeat it the eleven time for them  so i hope that is the one which  i want to use here \nyeah ok so now suppose you are standing at the velachery gate and you want to go to \nphoenix market city something that all if you can relate to today  so you want to go to \nphoenix market city and you ask the security guy at the gate that where do i go right \nso he will say take a left  no take a right  so i am slightly dyslexic actually i have a \nleft right dyslexia  so take take a right  ok so you will say  he has told me to move \nright but you would still be a bit  cautious right we will just keep moving slowly in that \ndirection that is  how we find ask for directions  you keep moving  slowly in that \ndirection \nnow one hundred steps later or one hundred meters later you find another guy and you ask him or her \nwhere is phoenix market city he again points to in the same direction  keep moving left \nright so now you will what will happen  you will increase your space and then you \nask again someone  when you read the signal where it is and he again points in that \ndirection what will happen move even fast \nso what is happening here if a lot of people are pointing you in the same direction  you \nbetter start taking larger and larger steps in that direction does that make sense that is \nhow we find directions and move around  so just like a ball gains momentum as it goes \ndown a slope right it is constantly moving in that direction so it starts moving faster so \nnow can you tell me a way of incorporating this i have been moving in a certain \ndirection these directions are nothing  but the gradients and now at this point  someone \nasked me again to move in the same direction what should i do \nstudent take a bigger step \ntake a bigger step  so can you think or try to imagine  how would you do this \nmathematically \nstudent refer time threeten \nso it is probably there are a few ways to do it  so let us see so what i am doing here \nis this is my current gradient right so i asked that guy at the signal  he asked me to \nmove in that direction so that is this direction and this is all my history  whatever i did \ntill step t minus one ok so now what i will do is i will so earlier i was moving like this \nthis is what my update rule was wt plus  one is equal to wt minus in the direction of the \ngradient right i will moving in the direction opposite to the gradient \nnow what i have is in addition to that  i have this gamma  update t minus one so that \nmeans whatever i had done up till step t minus  one i will also take that into account  so i \nwill end up taking a larger step  is that clear if it is not clear it will become clear on the \nnext slide \nrefer slide time foureighteen \n \nso let us see what this means right so it basically means that in addition to the current \nstep also look at the history  there are three guys who earlier pointed you in the same \ndirection so maybe this direction makes sense right so start accumulating that and \nmove faster \nrefer slide time fourthirtyone \n \nso let us just break this down and see right so this is what the update rule is sorry this \nis all my update s and this is the update rule  so at time step zero my update is zero  \nbecause not started yet  at time step one this is what it will look like right and this is \nnothing but just move in the direction of the opposite to the gradient  because this minus \nsign will come later on right in the next equation  \nnow what will happen  update two so its gamma times update  one plus the gradient at \nthe current step  so remember here everything is positive  i am adding the gradients  \nbecause my final negative sign is going to come in the next equation  ok so do not get \nconfused with that eventually i am going to move in the direction opposite that opposite \nwill come from this negative sign \nso what is happening  i am moving in the current direction plus a fraction of the \ndirection which was pointed earlier right  ok then does this make sense so can you tell \nme in general what is happening here at the  t\u2019th time step what is happening  what kind \nof average am  i taking weighted average but it is a dash weighted average this is an \nexponentially weighted average ok so let us look at this right \nso when i am at step four i have most faith in the current gradient right and this gamma is \nalways i will just set it to less something less than  one right so i have a fractional trust in \nthe previous gradient  even smaller trust in the previous guy and even smaller trusts in \nthe previous guys so  i am taking an average of all my gradients  but it is an \nexponentially weighted average  does that make sense  my maximum faith lies in the \ncurrent guy and then decaying faith in the previous guys \nand as i move further and further away from the last guy that  i checked right i will give \nlesser and lesser weight age to that  so everyone understands what is happening here  \nanyone who has a problem is just raise your hands if you understand this good \nso in general this is going to be the formula and you see that as  as i form problem here \nno as t is larger this fraction is going to become smaller and smaller right so you are \nfirst the first step that you take  will have lesser and lesser weight age as t inc reases \neveryone gets this fine \nrefer slide time sixfiftyseven \n \nso now this is the code for momentum  based gradient descent i will just give you a \nminute to stare at the code and see if it makes sense so this much part is ok you are just \ncomputing the gradients with respect to all the points right and now we are keeping this \nrunning sum ok which is the previous gradients and the current gradient right and then \nyou are just subtracting that running sum  \nnow this looking black curve that you see here  that is gradient this  this guy ok this \nblack curve that you see here  that is gradient descent when i have run it for around one hundred \niterations now i am going to run momentum base gradient descent and each click is \ngoing to be one step  ok and i want you to o bserve what happens  ok so slowly a red \ncurve will start appearing on the figure \ninitially it will not be visible so do not worry there is nothing wrong with your eyesight \none how many if you already see the red part i see it two three four five six no now you can see it \nas is nothing great aboutseven eight nine i want you to observe something here eleven twelve thirteen fourteen came \nback right so gradient descent i ran it four hundred iterations it was just stuck here right this \nwas a point and i ran this for less than like around fifteen or twenty is what we counted right and  \nso already entered into the valley \nso momentum base gradient descent is good  you see that wicked smile on my face and \nyou know it is a trick question so we are moving fast right \nrefer slide time ninetwo \n \neven in the r egions where the slope was gentle right that is  the beginning of the  \nbeginning of our trajectory right this was the gentle region even that i was very quickly \nable to navigate right within five to six steps i was away from that part right so even in the \nregions where the slope was gentle  i was able to move fast  but is moving fast always \ngood \nso would there will be a situation where momentum would cause us to run fast ago \nsame thing now instead of walking you are in a car  you ask the person at the secu rity \nwhether i should go there  he says yes go in the right direction you keep moving there  \nsomeone else you keep accelerating  what will happen eventually  you will go fast \nphoenix market city then what will you do \nstudent take a  \n take a u turn come back again while taking a u turn what will you do \nstudent refer time ninefiftyseven \novershoot and come to the signal and then go back again right so you see this you will \nend up taking a lot of u turns so let us change the input data a bit and see what h appens \nto momentum based gradient descent \nrefer slide time teneleven \n \nso this is what my data looks like now  so this is not what my data looks like  this is \nwhat my error surface looks like  so earlier we had this error surface something like a \nflying carpet now i have a very peculiar error surface  this is again for the two \nparameter problem right w comma b  that means  i want to learn a sigmoid function  \nwhere i have these two plateaus at the top  the dark red regions that you see and then a \nvery sharp valley can you tell me how i would have come up with this kind of an error \nsurface what are the points that i would have chosen just hold on to that part \nso i have this kind of an error surface fine  the error is high on either side of the valley  \nnow could momentum be detrimental in this case  yes no maybe i do not care i do not \ncare fine \nrefer slide time elevenseven \n \nso let us see this is the  is this the two d equivalent of that three d surface everyone gets it i \ncan perfectly verify that you get it everyone gets it i will assume right so these are the \nvery high plateaus where the error is very high  very sharp and narrow valley where the \nerror is low \nso now again this sorry looking black curve is what  i have done with gradient descent \nafter s ome one hundred iterations or something  now i am going to run momentum based \ngradient descent and you have to help me understanding what is going to happen  again \nyou will soon start seeing that red curve appear one two three four five six  what will happen now it is \nalready fast that is known it was that black curve was after one hundred iterations or so it is fast \nnow tell me what will happen \nstudent refer time twelvezero \nhe will go out is actually almost come out of the valley right  it is almost at the top of \nthe valley now what will do take a u turn now what will i do again take a u turn \nnow i will keep doing this  i will take now smaller and smaller u turns and it will \nconverge right so what happens here is because of this speedy movement and which is \nvery analogous to that car movement which i described \nthis overshoot your goal you  will have to take the u turn come back if you are again \ncareless you will have to keep taking these u turns but you will finally end up at the \nlocation that you want righ t it takes a lot  of u turns before converging despite these u \nturns it still converges faster than gradient descent right  because gradient descent can \njust not move at those gentle slopes right it just can not move from there  because the \ngradient is almost zero because the slope is flat right and it just can not move but even with \nthis lot of u turn and lot of rework  after one hundred iterations momentum base gradient descent \nhas reached an error of almost  zero whereas gradient descent is still stuck at the plateau at \nan error of zerothirtysix ye so see you have reached the minima now \nstudent ye \nnow you will be navigating there right  but you know that now your loss is very slow \nlow so you could end that right  you know that your loss is very close to zero so you \ncould have a condition that once you have reached something very close to zero you could \nend that even if you are making these very small movement s now you could just stop \nthere \nstudent but in the plateau regions is also zero \nbut the loss is high right so if the loss is high and you are not moving you cannot stop \nbut if the loss is low and you  are not making movements you can just stop there  right \nso you can just end you can define that as your convergence condition \nrefer slide time thirteenfortyfour \n \nso let us look at we will come back to  three d now we look at a  three d visualization and a \nvery different interpretation of what is happening  i really want you to understand what \nexactly is happening in this example which i had picked up right \nrefer slide time thirteenfiftynine \n \nso this is what the three d surface looks like view from a different angle you have these two \nplateaus and the very sharp valley now this is the corresponding sigmoid function \nwhere i started with so what i am trying to tell you is that  this is a sigmoid function \ncorresponding to w equal to six oh no sorry w equal to two and b equal to six \nthis is the sigmoid function that i got once i plug that value so sigmoid is one over one plus \ne raised to minus w x plus b and  i have plugged in the values of w and b and plotted it \nfor all the values of x and this is the sigmoid that i got ok so that is my starting point is \nthis good how do you define good or bad \nstudent refer time fourteenfortythree \nwhat do you expect at the end of training it should pass through all your training points \nand these are my training points ok  is it passing through them no its way off right  ok \nso now let us start this momentum based gradient descent and what just  see how my \nsigmoid function changes  so right now i am on the gentle slope  even that momentum \nbase gradient descent i t is going to be fast  but not dramatically fast  because still  \nbuilding up the momentum \nso it is you see that these sigmoid that i am drawing here  they are almost \nindistinguishable from each other  i have already drawn three sigm oids here so i will \njust go back so there was this initial guy then i draw drew a red one then one more and \nthen one more but they are all very close to each other \nnow keep viewing both these sides in parallel  what happens here on this figure and \nwhat happens to this sigmoid  ok and i will ask you questions so still i am moving a bit \nslowly because i am still building the momentum right  it takes time to build that \nmoment now i have slowly started building the momentum my sigmoids have started \nmoving towards where they should be everyone gets this what is happening here ok \nnow tell me what will happen  as i enter the valley  i am almost entering the valley \nwhat will happen  i have gained this momentum now  so my w comma b values are \ngoing to change much faster now so what will happen to these sigmoids they no longer \nstick to each other  we will start seeing a difference they  are already moving away from \neach other ok so that is what is happening to the function ok now you see even faster \nchanges ok now what will happen  i have entered the valley  this is how my sigmoid \nlooks at this point now tell me what will happen \nstudent refer time sixteenthirtyfour \nit will go fast what will happen to your sigmoid how many of you know what will \nhappen to the sigmoid ok i will tell you what happens and then it will be obvious right \nso now i am entering the valley all of us know that i am going to come out of the value \nof the other side right so let us see what happens when  i come out of the valley fr om \nthe other side the sigmoid changes that is why you have this situation that your error is \nhigh on both sides right  because on this side you have these kind of sigmoids  on the \nother side you have the other sigmoids and somewhere in between lies the so lution \nwhere does the solution lie at a very flat sigmoid right \nso now i start this is where the oscillations will happen  so notice what will happen to \nthe sigmoids they will toggle between these two orientations ok  just see what happens \nto the sigmoids you see it again moves keeps moving keeps moving it keeps oscillating \naround the solution and then finally you reach the solution  so you see that  should i \nrepeat this \nso when i am on one side of this valley i have one kind of sigmoids right now when i \nmove to the other side of the valley i have this others kind of sigma and take a u turn so \nwhen i u turn take a u turn i again overshoot and go to the other side  and this keeps \nhappening and i keep toggling till i reach my final solution \nso these are all the oscillations that you are seeing  so can you visualize this  what is \nhappening do you understand all these relates to the actual function that you are trying \nto learn so that is why we will end this module this was on momentum base gradient \ndescent now we will see a nesterov accelerated gradient descent"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.5 Nesterov Accelerated Gradient Descent.wav", "duration": 678.94, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fivefive \nlecture \u2013 five \nnesterov accelerated gradient descent \nlet us look at nesterov accelerated gradient descent \nrefer slide time zerosixteen \nso now we know that momentum based gradient descent is good at these gentle regions  \nit moves really fast  but we do not  still do not  like it  because it has this problem of \noscillations it has this problem that it overshoots its objectiv e its goal and then it has to \ntake a lot of u turns  so can we do something about reducing this oscillation so the \nanswer is always yes so let us look at nesterov accelerated gradient descent \nrefer slide time zerofortyone \n \nso the idea here is very sim ple look before you leap ok  now remember that this was \nthe update rule for momentum based gradient descent  ok and i will write it down again \nwt plus one is equal to wt minus gamma into update t minus one minus eta into the gradient at \nthe current point \nso you see that actually  i am taking two steps  one is this step and then one more step \nand i could just this is one way of visualizing right  that i move according to the history \nand then i move a bit more according to the current gradient so everyone sees that there \nis a two step movement happening here \nnow can you think what could have been done  look before you leap  so we will see \nwhat we can do \nrefer slide time onethirtyfour \n \nso we know that we are going to move at least by this one and that is fixed we know \nthat our history is telling us to move at least by this one and then we will mo ve a bit \nmore by the gradient \nso now can you think about it  i am at least going to move this much  what if i had \nsome way of looking ahead  and then do something at th at point  this is what you are \nsaying of course i can verify it  but  i am sure it will become clear once  i show the \nequations but i just want you to think about it a bit wait  it is very simple it will become \nabsolutely clear once i show you the answer but just think about it a bit \nso here is the answer it  why not compute the gradients add this look ahead point right  \nso you are again adding it in two steps  minus the history and then minus  the current \ngradient so take this value call it the look ahead point i know that i am going to move \nby this much so let me not compute the gradients at the current point  let me move by \nthis much then compute the gradients and see what happens at that point \nrefer slide time twofiftytwo \n \nso this is the equat ion right that first  i move by that one step  i had to make a two step \nmovement so i will move by that one step right then i will compute the gradient at that \nposition not at my current position right  this was earlier gradient at point t  now i have \nalready moved a bit so i can compute the gradient there and then move in the direction \nof that gradient \n so you understood this that there is a two step movement right  wt minus history minus \nthe current gradient  gradient computed at time step t ok  now you know that you are \nalready going to move by the history right  so why not just move there  and then \ncompute a gradient at that point you are anyways made some movement you compute \nthe gradient at that point and then decide which is the direction to move in right \nso that is what this look ahead value is  i know it is still not clear to many of you and  i \nam very confident it will become clear in the next five minutes we will show you one more \nvisualization for this but this stay with stay with me for a while as long as you get the \nintuition i am fine i will move ahead and then  i will explain it again in a different way  \nthis is fine ah that should become clear good that you asked that question ok  so ask me \nagain on the blank slide that i have and then i it should be complete \nso for right now let me just show you what will happen with the code  and then i will \nagain explain it with a different way \nrefer slide time fourthirtyfive \n \nso this is what momentum based gradient descent it ok  now let us see what nested or \naccelerated gradient descent will do again the code is simple you can just read it up and  \ni have started executing  you see this blue curve  coming over there fine  ok and now i \nkeep running this now what will happen you see that all the u tu rns of the blue curve \nare inside the u turns of the red curve \nso the objective is being achieved at least empirically  i have showed you that right  its \ntaking shorter u turns what is probably not clear to all of you is  why is this happening \nis it clear to everyone  why is this happening  can everyone visualize that ok  so let us \nsee why this is happening i will give you an alternate explanation for this \nrefer slide time fivethirtyeight \n \nso suppose this is my error surface right on a two by  and i have a s ingle variable with \nrespect to which i am trying to optimize  so this is my w i started off with some initial \nvalue w naught \nnow what is the gradient at this point  positive negative negative right because when i \nam going to increase w the function i s actually going to decrease right  so right so the \nslope is negative  so where will  i move this is the number line right  so this line is \nactually the number line  because it is a single variable  so where will i move positive \nside of the number li ne or the negative side of the number  positive side the derivative \nis negative i am going to move in the direction opposite to the derivative so i am going \nto move in the positive direction right  so i will end up somewhere here  is that clear \nfine with everyone \nso now i am somewhere here  what is  the derivative at this point  now what is  the \nderivative here  positive negative  negative right when i am increasing w my loss \nfunction is decreasing  so my dl by dw is going to be negative  this is pos itive this is \nnegative so again i will move in this direction \nso what is happening a lot of negative updates are getting  sorry a lot of positive \nupdates are getting accumulated  and now because of my momentum  i am not going to \nmove only by this deriva tive i am also going to move by the history right  so i will end \nup somewhere further \nso now at this point what is the derivative  again negative when i am increasing w the \nfunction is decreasing  so what is my update positive or negative  positive so now \nyou see that a lot of positive updates are getting accumulated right  my momentum is \nbuilding up so now what will happen now if i just move further then again i will get \na let me just put it here right  so i am again moving largely in the pos itive direction \nbecause this guy is also positive  all my history was also positive  so i have moved in \nthe positive direction \nnow what will happen at this point  what is the derivative here no it is still its negative \nsorry so again i am going to mo ve in the positive side of the number line ok  now at \nthis point i want you break down the movement into two points  one is what my history \nwas telling me which was all these positive updates  but of course i will not make such \na large update  because i am waiting them exponentially right so  but its telling me to \nmove in the positive direction ok  and i know that the gradient at this point is negative  \nbut i want you to ignore that for now  i just want you to focus on the history  if i just \nmove according to the history where will  i end up i will end up somewhere here right  \nbecause the history is very positive  so i will keep moving in the positive direction  and \nthis is my w look ahead \nnow what will happen if i compute the gradient here \nstudent positive \nthe gradient is \nstudent positive \npositive so where will i move \nstudent negative \nnegative so you see now why momentum works because you are able to look ahead to \nthis point  instead of what should  i have actually done is  i should have look ed at the \ngradient at this point  the history is positive the gradient is also telling me to move \npositive so i would have moved a large positive and  i would have ended somewhere \nhere instead i just moved by the history i checked where i end up i end up here \nnow let me see whether what is the gradient at this point  have i already overshoot my \novershot my objective  when would  i overshot my objective  it has the sign of the \ngradient changes right  it became from negative to positive and now since its  positive \nbecause as i am increasing w  the loss is also increasing  so now where will i move \nnegative  \nso now what is the second step actually its again bringing me close to here  so instead \nof taking this large u turn  i end up taking this small u  turn is this clear to everyone \nnow how many if you still do not get it how many if you get it now  good sure ok so \nthis is what and now we can relate it to what was happening on the figure \nso let us go back right  so you saw that  i was making these  smaller u turns  because \nwhen i was at this point right  i already moved by the history  i knew i would land up \nsomewhere here where i would need to go back right  so i already accounted for that \nand made a very small movement  is this clear everyone gets this how the nesterov of \naccelerated gradient descent works sure raise your hands \nrefer slide time tenfortytwo \n \nso looking ahead helps nag in correcting its course quicker than momentum based \ngradient descent right  so it is already looking ahead whe re do i land up and already \nmaking a correction if required  if not required it will again move in the right direction \nright so the update is this guy plus the gradient and my  update happens on the original \nvalue not on the look ahead value \nso her confusion was perhaps that  i am doing w look ahead minus  update where this \nupdate again has this quantity you know  that is what your confusion was  but i am not \ndoing w look ahead i am using wt there everyone gets this so that is where ah now it \nis clear that why the oscillations are smaller in the case of nag and it  is able to correcting \nits course quicker"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.6 Stochastic And Mini-Batch Gradient Descent.wav", "duration": 811.22, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fivesix \nlecture \u2013 five \nstochastic and minibatch gradient descent \nnow we look at stochastic and mini batch versions of these algorithms \nrefer slide time zerotwenty \nso we will digress a bit  actually we should have ended up somewhere else  but i was \njust going to digress a bit \nrefer slide time zerotwentyfive \n \nso this is the original gradient descent code that we had  and i have high lighted \nsomething in this red box \nso notice that the algorithm actually goes over the entire data once  before making an \nupdate it has going over this entire for loop  which is over all the data points of course \nin this toy example  i had only two data  points but in i practice i will have many many \ndata points i go over all the data points compute the derivatives and then make this one \nupdate \nstudent refer time zerofiftyseven \nbecause that is the right thing to do ok  this was the exact formula that we pa infully \nderived right that the gradient with respect to the loss function right  which we had the \nsummation i equal to  one to n remember  and the true derivative was a sum of the \nderivatives with respect to all the data points  that is what we analytically d erived and \nhence we are doing that  it was that is the right thing to do  not for any other purpose  ok \nthat is what it should always be right  so that is the right thing to do  because this is a \ntrue gradient and we actually derived it \n refer slide time onetwentyeight \n \nand hence this was not an approximation  so all the theoretical guarantees hold  if i do \nthis i know that now this is the true gradient or the true derivative and if  i move in the \ndirection opposite to the gradient everything falls in place  because i proved it using \ntaylor series  \nbut what is the flip side of this  this is the right thing to do  but what is the flip side if \nyou have millions of point  we will go over all these million points and make this one \nupdate now imagine the consequence when you are in a plateau region right  even that \nmomentum or whatever your movement in the plateau is going to be relatively smaller \nright you are going over these million points and making that tiny delta update right  \nso imagine how much time it will take your algorithm to converge you get the problem \nso the algorithm will take a million calculations and then make one tiny update to your \nw ok this is going to be very slow  can we do something better always right  so let us \ntake a look at stochastic gradient descent fine \nrefer slide time twothirtysix \n \nso i have done a very subtle change to the code what is it do not tell me indentation but \nthat is what i have done so you can tell me that  so what is happening now for every \ndata point i am making an update to my w values \nnow the algorithm updates the parameters for every  single data point  if you have a \nmillion data points how many updates will be make in one pass over the data  a million \nfor every data point will make an update right  so that slowness factor in what is known \nas batch gradient descent right batch gradient descent is when you look at the entire data \nand then make one update \nrefer slide time threenineteen \n \nwhat is the flip side  what does this module titled stochastic gradient descent so what \nis the flip side these are not the true gradients  the true gradient is summation over all \nthe points now this is no longer the true grading this is just a point estimator this is just \na approximation of the gradient right  and stochastic because we are calculating the \ngradient based on a single data point right  it is a sampling one data point and computing \nthe gradient that this is what the entire population looks like right \nthis is almost similar to tossing the coin once and saying that this is what the probability \nof heads is if it lands at heads then the probability is one  otherwise its zero right you see \nthe error you see the problem with that right  as opposed to tossing the coin a thousand \ntimes and then deciding the probability  is just tossing it once  so this is always going to \nbe a erroneous right this this is going to be bad \nso now there is no guarantee that each step will decrease the loss why  because the \nguarantees were only when you are doing the right thing which was to compute the \ngradients over all the data points now there is no theoretical guarantees right  because it \nis all stochastic now  so it is possible that in a particular data point your loss might \nincrease also the overall loss on the data  with respect to that point it might decrease  \nbut the overall loss right \nso now let us see this algorithm in action and  i want you to make certain observations \nabout this so this is the code that i am going to run now so let us see \nrefer slide time fourfortyfour \n \nso i will start and you have to observe and let me know and this is really becoming an \neye test for all of you  but that is good so for nothing interesting to observe or already \nmaybe \nremember i am running  gradient descent this is not momentum  not nesterov this is \ngradient descent ok i have already given you the answers what do you observe \nstudent  \n i can still pretend an answer a let us do that we see many oscillations why why do we \nsee the oscillations are these oscillations the same as the osc illations that we see in \nmomentum no these are different  everyone gets that right  why are there oscillations  \nwhat is each click here correspond to one data point right  so what is happening here  \nbecause we are making greedy decisions right we are loo king at one point  this point  \nsays to decrease the loss with respect to me move in this direction and we blin dly move \nin that direction \nnow we look at the next point  it says oh no no wait you need to move in this direction  \nso we again move in that dir ection so all these points are actually trying to just make \nthings better for themselves  they are not thinking about what is happening to all the \nother points in my data right so all these points are actually competing with each other \nso some decisi on which  i took with respect to where to move  which was locally \nfavourable for one of these points may not be good for the other point right  hence i \nkeep these tiny oscillations which  i make these are the stochastic noise that you are \nseeing now \nnow can we reduce the oscillations by improving the stochastic estimates always yes \nfine so let us see what do i mean by that \nrefer slide time sixfiftyseven \n \nso we look at a mini batch version of this  so what i am going to do is  instead of so \nthis code is  actually for mini batch stochastic gradient descent  it is a very minor \nalteration on the stochastic gradient descent  i will just let you stare at it for a minute or  \nso what i am doing here is  i am instead of doing it for every point  i am waiting for a \ncertain number of points and then making the update right that is what i am doing here \nnow for this i have kept k equal to two what does that mean i look at two points compute \nthe derivatives with respect to them and then make an up date for two point s at a time  \nwhat do you expect no what do you expect with respect to this code \nrefer slide time sevenfortyseven \n \nso let us see we will try to run this now and you will start seeing a red curve here and \nmake some observations about this  so this is the red c urve yeah its visible oops i do \nnot read any of those \nstudent refer time eightseven \nif you need to fix this right  these bullet us should come only after the curve has finished \nthis journey ok do not read any of that ah so what do you see about the red curve it is \ncompletely contained inside the black curve  that means its oscillations are smaller than \nthe black curve right does that make sense why this is happening  because now you are \nnot listening to just one point  you are listening to two poin ts and then at least you are \ndoing something better right instead of just taking one \nso what is the analogy with respect to our coin toss experiment you are tossing the \ncoin twice and then deciding what is  the probability are heads or tails right  so it is \nalways going to be slightly better than tossing it only once right  and now what would \nhappen in the limit if  i keep increasing this  you will end up with a batch gradient \ndescent where you look at the entire data \nso looking at only one data point is  bad because it is very noisy  looking at the entire \ndata is bad because it is very time consuming so you need to do something in between \nwhich is mini batch gradient descent ok  and typically you look at values of sixteen thirtytwo sixtyfour \nbut it also depends on the  amount of data you have and if you have a billion points you \nmight actually want to look  because if you have a billion points and you have a batch \nsize of sixtyfour you will take one billion by sixtyfour times to finish the data once \nso you might want to keep a larger batch size at that point right but just ignore that but \nyou will try different batch sizes and see which one works better  so in the assignment i \nwill be asking you in to experiment with bad sizes  yes ok no sorry wrong question  i \nwill be asking them to implement stochastic and mini batch also or only vanilla \nstudent mini batch \nmini batch fine that is fine ok so you will see this in your assignment  so everyone \nsees what was the difference between  stochastic and mini batch  you have better \nestimates now and therefore this red curve is contained inside the black curve fine \nrefer slide time teneight \n \nso you have some things to remember  one epoch get used to this terminology  one epoch \nis one pass over the entire data  one step is one update to the parameters n is equal to the \nnumber of data points and b is equal to the mini batch size  now you have to fill in the \nsecond column in vanilla or the batch gradient descent what is the number of steps that \nyou take in one epoch \nstudent one  \none in stochastic gradient descent \nstudent n \nn n \nstudentn \nin mini batch gradient descent \nstudent n by b \nn by b everyone gets that so get used to this ok  so this epoch step batch size all this \nis something that you will see regularly when you are reading pape rs on deep learning  \nfine \nrefer slide time tenfiftythree \n \nso similarly we can have the stochastic versions of momentum based gradient descent \nand nesterov accelerated gradient descent \nrefer slide time eleventwo \n \nso these are just the codes it is very easy to see what is happening here again basically \nthis is just an indentation right so if you look at the difference between the two codes  i \nhave just indented it inside  that means  i am making these updates for every data point \nright and same thing you could do for nesterov also \nrefer slide time eleventwentyone \n \nnow let us see ah this guess what is it this is the gradient descent stochastic gradient \ndescent now let us see if you have really understood nag and momentum based gradient \ndescent one of these curves here corresponds to stochastic nag  the other one \ncorresponds to stochastic momentum tell me which one is which \nstudent blue pill  \nblue pill red pill blue is \nstudent refer time twelveone \nhow many of you say that  ok i am confused ok  how many of you say that blue is \nmomentum how many if you say that red is momentum  oh there is so many you do \nnot have an opinion \nstudent sir not clear  \nnot clear  i will buy that  so ok so look at this  who is taking longer u turns  \nmomentum or nag momentum roughly which guy is taking the larger u turns \nstudent red guy  \nred guy right  i mean roughly speaking there is only one point to judge by this here  \nbecause here they are almost same and that could happen in practice right because this is \nnow noisy so the red curve corresponds to \nstudent momentum \nmomentum because it is taking a larger u turn  we saw that momentum takes larger u \nturns and the blue curve is corresponding to nag ok  so no i remember this was an error \non the slide yeah so this has to be red and this has to be blue  so ok so the momentum \nis actually red and the nag is blue  because it is taking a shorter u turn and the reason you \ndo not see it very clearly is because both of these are running in this stochastic mode \nbut you still see the relative advantage of them that nag still takes shorter u turns both of \nthem are faster still faster than vanilla gradient descent \nrefer slide time thirteenthirtyfive \n \nyou see that black curve at the top and both of these are faster than them  both of them \nall three have run for the same number of iterations after sixty steps you see what happens \nto stochastic gradient descent and what happens to nag and momentum basically gradient \ndescent and of course  you can have the mini batch versions of momentu m and nag \nalso"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.7 Tips for Adjusting Learning Rate and Momentum.wav", "duration": 760.2, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fiveseven \nlecture  five \ntips for adjusting learning rate and momentum \ntips for adjusting the learning rate and the momentum \nrefer slide time zeroseventeen \nso before moving on to these slightly advanced optimization algorithms  we will revisit \nthe problem of learning rate in gradient descent \nrefer slide time zerotwentyfour \n \nso one could have argued that we could have solved this problem of this  slow \nmovement on the gentle slope by increasing the learning rate  remember that we have \nthis eta and we deliberately chose to be conservative  that we will take a small value for \nthe eta but what if i just blow up the eta i could just take a ver y large eta what would \nhappen it will overshoot right \nso what will happen is i will see what happens when  i take eta equal to ten ok so so i \nwill see what happens when i take eta equal to ten \nrefer slide time zerofiftyseven \n \nso this is step one step two step three its moving very fast on the regions where the slope is \ngentle but it also moves very fast  much faster on the regions where t he slope was \nalready steep \nso when the gradient was actually high  you ended up blowing it further by  multiplying \nit with the eta which is  ten so it is again going to have this effect that you will move \nmuch faster in the steeper regions and again you will see these oscillations  because you \nwill overshoot your objective  does that make sense right  so it is not that you ca n \nalways choose a high eta and get away with it \nso what do you actually want what is your wish list regulate theta you want a adaptive \neta right  that it somehow figures out that  i am on a gentle slope  so  i should move \nslowly i should move fast and i am now on a very fast loop  so i should move slow so \nthis having this  one eta is not working for every point on the error surface right  for \neverywhere on the error surface  is that clear ok  so ok so we will see such algorithms \nsoon where we try to adjust this learning rate \nrefer slide time twosix \n \nnow here are some tips for the learning rate  so how do you  if you are just going to \ndeal with this gradient descent or nag or momentum  how do you adjust these learning \nrate so how do you fix a learning rate so a learning rate is typically something known \nas a hyper parameter  so why is it called a hyper parameter so what are your \nparameters \nstudent which i learned \nwhich i learned using the objective function  eta is not a part of the objective  function \nyou are not computing radians with the respective to it is a hyper parameter so you will \ntry to tune this hyper parameter  so what you will do is  in practice you could try these \ndifferent values on a log scale  next what will you do  run this all these for a few \nepochs note down the dash just note down the loss function \nso run all of these with different learning rates  for say five epochs you will get some loss \nright now which one will you pick the one which led to the maximum decrease in the \nloss i will keep that learning rate and now what you will do  you just stick to that  i \nstarted off with a dash scale \nstudent log scale \nlog scale now what will you do ok so now run it for a few epochs figure out which of \nthese learning rates on the log scale works well  now do a finer search around the best \nlearning rate that you discovered right  so say zeroone was the best on the log scale  so \nnow look at zerotwo zerothree zerofour zerofive look at values around it and see which one works better \nso this is how you will tune the hyper parameters  otherwise there is a very wide range \nright if you put tune from zeroone to zeroone there are just too many values to consider  so \nwe will have to do this log scale and then a linear scale will that make sense \nthese are just heuristics  there is no guarantee that will always work or which of these \nare clear winner strategy  but you have to try this  so tuning a learning rate is an \nimportant part  when you are working in deep learning  so at least when you are \nworking with gradient descent or nag or momentum based gradient descent \nrefer slide time fourtwo \n \nnow here some tips for annealing the learning rate  so there is something known as \nstep decay so what you can do is halve the learning rate after every five epochs can you \ntell me the intuition for this  what do you expect after five epochs that you have moved \nenough and now you are closer somewhere to the solution  so if i closer to the solution \nif i closer to phoenix market city you want to move fast or slow \nstudent slow \nwhat will you do \nstudent refer time fourthirtytwo \ndecrease the learning rate right so after every five now this is again what is so sacrosanct \nabout five it is just a magic memory  so this is again hyper parameter  so you could fix \nsome number of epoch and after these  i will just halve the learning rate ok  now this \nsecond one is what my favourite is and  i typically use this  what i do is i compute the \nloss after epoch t i run epoch t plus one i compute the loss again if the loss has increased \nwhat will i do i will just throw away all the updates that  i have made in this epoch  i \nwill decrease the learning rate and again learn  again start this epoch what do i mean by \nthrow away all the updates \nstudent refer time fiveseventeen \nso after epoch t i will save my model  i will save all the w values that  i have computed \nand i will let it run for one more epoch  after this epoch if my loss function actually \nincreases i reload this model which  i had saved half the learning rate and then run this \nepoch again does that make sense so i have run till epoch t i have some values of w\u2019s \nand b\u2019s i will save this values i will just save it as a numpy array \nnow i will with the same learning rate that i have been using so far i will run the epoch t \nplus one ok and i get some new values of w comma b  right i will plug this into the loss \nfunction i will plug this into the loss function i will get two loss values if this loss value \nis greater than what  i was at the previous time step  that means things did not work out \nwell in this particular epoch \nso i will throw away all these updates  i will just reload the model which  i had saved i \nwill just start from where i was at epoch t i will decrease the learning rate i will make it \nhalf and run this epoch again right and hopefully now i should do better because there is \nsomething i am just making a hypothesis that the reason  i did not get to a better loss \nfunction was because my learning rate was not adapting to it \nso i will just halve the learning rate because this solution was good this was a low loss \nfunction i just want to be something around it i do not want to make any drastic steps  \nso i will just half the learning rate from there  so then you not see this drastic change \nthat your loss funct ion should not improve  so first of all local minima is  known \nproblem in deep neural networks \nso what happens is that  in deep neural networks you do not have something which is \nlike a neat convex function as your loss function right it is a non convex function which \nmeans there is no one unique minima  there could be several minima  and there are \nseveral analysis which show that a lot of these minima are equivalent  so in practice \nthese are the things that you do  either once you reach a minima you jus t stay there the \nsecond thing that you could do is  you have trained your algorithm trained your \nparameters for say one hundred epochs and you have stopped now \nnow again go back and start with a different initialization  you started with some w \nnaught b naught a nd you have reached to some solution  keep this solution  now start \nwith a different initialization  that means if you look at your wb plane  you have started \nfrom some other point  that means you started from some other error location right and \nrun this algorithm again and see if you reach a different minima \nso the only thing you the way you counter this is you just try different stochastic things \nright should try to start with  ten different initializations every time  reach a minima and \nthen at the e nd select the lowest possible of these  did this make sense to most of you  \nhow many of you got this oh cool i thought i was just rambling but yeah fine does that \nmake sense to you at least ok does it fine \nyes a local minima is a severe problem in lot of deep learning optimization and typically \npeople get away by that  by just picking up one of these minimum  fine now the other \nthing is you could use exponential decay  where with each time step you just keep \ndecreasing your learning rate  and if this ca se two that means at every time step you are \nhalving the learning rate so you just get with something like this \nbut the reason i do not like this is that you have one hyper parameter which is eta which \nyou are trying to tune and now to tackle that proble m you have introduced one more \nparameter which is k  hyper parameter which is k  so it becomes harder to tune that \nnow and there is a similar thing which is  one by t d k where you try to use this formula to \ndecay or learning rate  so both of these  i typically do not  use in practice  i use the \nsecond one i prefer the second one \nrefer slide time nineeleven \n \nnow tips for the momentum  can you make sense of this  you just stare at it it looki ng \njust come back ok let us see what happens at t equal to zero this becomes zero \nstudent log one \nlog one is zero this is two raise to minus one minus zero which is just two raise to minus one which is zerofive \nso what is your mu t at t equal to zerofive does that make sense is it fine with everyone or \nis it confusing no ok mu max is typically this let us assume mu max \nnow what happens at time step two hundred and fifty this is two hundred and fifty by two hundred and fifty so this becomes one one plus one is two \nthe best thing that you learn in this course log of two is one so this become two raise to \nstudent two raise to minus two \nminus two which is zerotwentyfive so what is this \nstudent zeroseventyfive \nzeroseventyfive let us do one more i had t equal to seven hundred and fifty one minus one by eight so that is what is going to be \nright ok so then what is happening as my time steps are increasing  what is happening \nto what is happening  i am having more an d more faith in the history or the current \ngradient what am i increasing actually i have made a mistake  actually this is mu is \ngamma there is not we did not use mu anyway what you guys just went along  so this is \ngamma actually right that was a momentu m term that we had  so as a number of time \nsteps is increasing  my gamma is increasing  that means  i am having more and more \nfaith in my \nstudent refer time elevenfortynine \nno history learning rate is eta momentum is gamma so its gamma into update t minus \none and eta into gradient at the current time step right and here gamma is actually equal to \nmu is there any more confusion that i can add so when i say gamma i mean mu and so \nthat is how it is  so as i am increasing the number of time steps  i have more and more \nfaith in the history  that means  i do not  want to now get distracted by this one update \nwhich i am making right i want to go by the history and i am not increasing this gamma \nor mu indefinitely i am capping it by a max right max i will have this much faith which \nis zeronine hundred and ninetynine in the history does that make sense this is again just a heuristic do not worry \ntoo much about it so that is how it is"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.8 Line Search.wav", "duration": 520.48, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 fiveeight \nlecture \u2013 five \nline search \nso we were looking at these different variants of gradient descent  we saw that gradient \ndescent has this problem that  it finds it difficult to navigate the gentle slopes  so we \ncame up with tricks on momentum based gradient descent and also  nesterov accelerated \ngradient descent  \nthe trick in momentum was that if lot of your history is telling  you to move in a \ndirection then just continue to gain momentum in that direction  so instead of just \nupdating based on the current gradient  you also update based on the history  right and \nthere we saw that this is always going to be a problem that you will end up taking uturns \nand we had this analogy of how you look for directions and you just overshoot your \ndestination and have to come back and take a uturn and come back and so on \nso to prevent that we realize that the update done by momentum base gradient descent is \ntwo step update you actually the first step is based on the history and then  another step \nbased on the gradient at the current time step  right so then instead of doing th ese two \nsteps at one go why not just update based on the his tory see what the gradient that tells \nyou and then we saw this  nice figure i hope it was nice and where you saw that if you \nlook ahead point then you will be immediately corrected with respect to your errors  so \nthat was about nag and momentum \nthen we saw the stochastic versions  of these algorithms where we realize that if we do \nthe batch version  then you go over a million points and then  make only one update \nwhich could be very slow in cases where you have large data  so we then decided to the \nstochastic version where we just update  for every point  that again had these oscillations  \nbecause we were taking greedy decisions  we were just relying on one point to tell us \nwhich was the right direction to go o n and you saw that these esteem has become better \nas you increase the value of this k \nso k equal to one is the most stochastic version and then  k equal to two you get the mini \nbatch version and then  you could just have different  values of k so that you have more \nreliable estimates of the  gradients and in the limit if you have the entire data  then you \nare just doing the  full batch gradient descent  right this is the vanilla gradient descent \nanything else did we cover  then we had some tips on the learning rate and the \nmomentum these are again he uristic i gave you some ideas and you could try these in \nyour back propagation  assignment and see which one works better for you  you could \nsee you have any peculiar  observations while implement the back propagation  \nassignment \nso now there are a few more  things left in this lecture  so i will start with the line \nsearch first  so this is one more thing before you move on to  some more interesting \nalgorithms which are the current state of the art and lot of deep learning solutions \nrefer slide time twofortyfour  \n \nso most people that you read would look at  would have algorithms that we will see \nafter ten minutes \nrefer slide time twofiftyfour \n \nso now this is where just to contest contextualize things right so we are still trying to \nsee what is the light  right learning rate to use a line search is one such method where \ninstead of just doing one learning so you can look at the code and just focus on this part \nand tell me actually what are we trying to do how many of you get what the algorithm is \ntrying to do so far what we were doing is we were just having a single learning rate  ok \nand we saw that this learning rate can make a lot of difference  right because if you are \non the gentle part  you want larger  learning rate and if you want steep part  you want \nsmaller learning rate \nso just fixing the learning rate  to one value does not really help because then you will \nmake you will suffer on one of the two cases are either on the gentle case or on the steep \ncase now what line search does is instead of just using one learning rate  at every step \nnow whether it  is vanilla gradient descent which is the batch one or mini batch or \nstochastic right just use a bunch of learning rate  so i have used five different learning \nrates here  and i have computed the  gradients th at part remains the same  t he \ncomputation of gradients does not change \nnow you have the value  now you want to be conservative  you want to multiply the \ngradients with this eta right  but you know that you do  not always want to be \nconservative in fact in some cases when you are on the gentle slope  you do not want to \nbe conservative  at all  y ou want actually  blow up the gradients  so now  try these \ndifferent learning rates and update w and b  ok so if you have five learning rate s you \nwill get five different updated values for w b \nnow plug in all these w  b values into your loss function  right and see whichever is the \nminimum retain that w  b value and repeat the process  t hat means  again you will \ncompute the gradients with respect to th is new value of w  b and the new loss function  \nagain try out these five different learning rates and continue everyone gets that \nso now are we using a fixed learning rate at every step  no and now do you see that  if \nwe are at a gentle slope  it would pick probably this as the learning rate and if we are on \nsteep slope which should probably pick one of these as the learning rate and even  lesser \nthan that if you have  the disruption it does not make sense  you see the advantage of \nthis now  y ou are in so me way heuristically trying to adapt to the slope of the error \nsurface right by just giving a different learning rates  so try all of these and whichever \nworks best pick it up ok so that is about it we are trying different values \nnow what is the flip side of this now if you have k different learning rates that you are \ntrying then at every step you have now increased your computation k times so earlier \nyou just add one  learning rate u just going by that  but now i have k  so now  this is \nagain a trade off which is you have to see  now i will give an example where this trade \noff clearly works \nso now if you are at the gentle slope now making k more computations and moving out \nof that slope is definitely worthwhile as compared to just sticking to that slope where \neven after hundred more computations  you will not really move out of that slope  so \nremember that gradient descent  algorithm that we have seen where you just stick to the \ngentle slope after hundred iterations also right but instead if i tried five different learning \nrates and there is a high chance  that i could have moved out of the gentle slope  does \nthat make sense you see the advantage of this \nrefer slide time sixthirtyone \n \nso this is something that i have to talk about when i back to second order optimization \nso i will see when to teach that  so let us see line search  in action so this is again \ngradient descent t his black curve  which is visible there  t his is the one i a m talking \nabout which is run for few iterations and is just stuck on the steep curve  you know this \nstory now and it is just get stuck there \nnow let us see what happens if i run so now i will start running the line search based \ngradients descent so what do you expect now so it will just move very fast right so \non the first step itself it is crossed wherever gradient descent was stuck  after fifty iteration \nor so i will keep moving fast \nnow here is an interesting question  would you see oscillations here so when you see \noscillations it is w hen your loss is actually increased from whatever it was currently  \nwill that happen in line search the answer is always no \nit could happen  when could this happen  so it depends on the learning rates that you \nhave chosen right so if you have chosen the lear ning rates so suppose at one point to \nreally be effective you needed the learning rate to be zeroone ok and now if zeroone learning \nrate was not in your set right that means everything that is there in your set is faster than \nzeroone so that it will again hav e the same problem as momentum because you will move \nfaster than what  you should actually move so it depends on this careful choice of the \nlearning rate set \nso that is all i have to say so there is a slight convergence would be faster than vanilla \ngradient descent that is obvious and we see some oscillations  ok and the statement is \nactually wrong we need to remove that ok we see some oscillations and these could be \nthe similar wants to the once with that we see in  momentum because we overshoot \nbecause we have not chosen the right set of learning rates \none of the learning rates which was actually needed at a particular point right  say at this \npoint suppose i needed to move very slowly and that very slowly say zeroone and that was \nnot in my set  then any other learning rate is always going to  be much faster then so \nyou could see oscillation"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.9 (Part-2) Bias Correction in Adam.wav", "duration": 571.26, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  fivenine \nlecture \u2013 five \ngradient descent with adaptive learning rate \nin this module  we look at gradient descent with adaptive learning rate so first we \nwill see motivation or intuition  for why we need this and once you get the motivation  i \nbelieve the rest should be straightforward \nrefer slide time zerothirtyone \nso far what we have been doing is  please pay attention on this slide  i need to define \nsome notations and you should  not get confused with that  so far we have been dealing \nwith the situation  where we had just one feature which was x and one weight  \ncorresponding to it which was w and one bias which corresponded always on i nput \nright now we are going to look at the situation where we have more than one inputs  \nthat means earlier we were basing our  predictions only based on the director  and now \nwe are the director actor genre imdb ratings and so on \nso here x one x two x three x four these are four different features or four different inputs that i \nhave and this is not x square  just i know it is obvious but i am just making it clear  \nright so this is x one x two x three x four ok it is n ot probably the best choice of notation  but i \nwill just stick to that so now each of these has a corresponding w one w two w three w four ok and \nthis is how you r decision looks like it is the dot product between the weight vector  and \nthe input vector ok this is how i am going to decide and that is a single sigmoid neuron \nagain \nnow given a single point xy do i need to again go through this computation sorry w p \noh sorry ok i will just erase this so this w is actually the vector w  so it includes w one \nw two w three w four and i am trying to take the derivative with one element of that vector  do i \nneed to show you how to compute this  have you seen this before  can you tell me i \nwill show you the derivative with respect to w one can you tell me it will be a product of \nsome terms can you tell me what is the last term going to be \nrefer slide time twofortyone \n \neveryone gets this  yo u remember this form so only thing which is changing is this \nguy right so this part is exactly what we have derived and when we had one input  we \njust call it x and now we have multiple inputs so it will depend on that particular input \nright which ever w one corresponds to \nnow make an interesting observation there so sorry before that yeah this is obvious if \nthere are n points we will just take the sum of the gradients with respect to the n points  \nok now what happens if the feature x two is sparse what do i mean by that it is mostly \nzero ok what does that mean so i am looking at lot of movie data ok amir khan acts in \na very few movies so if i have a feature which says actor amir khan then that is going \nto be zero for most of the movies in my data scene right that is what i mean by sparse \nso if i have tenzero movies then probably only fifty of them would have this feature as \none ok does that make sense  so it is going to be very sparse  now if the feature is \nsparse why do we care about  it what will happen what do we really care about when \nwe are talking about optimization  in this course  the gradients right that decides how \nwell we move in the plane that we are c onsidering w b plane or the  other in the end  \ndimensional region that we care about \nso now if x two is sparse what would happen to this  it will be zero lot of times because x two \nis zero lot of times  right so now  just take a minute to understand this  right so now \nremember let us talk about stochastic gradient descent  or mini batch gradient descent  or \neven batch descent  y ou are  going over all the tenzero points that you have  you are \ncomputing the gradient with respect to all the parameters  \none of those par ameters happens to be w  two right you have gone over tenzero points \nbut in how many of those you will actually get the gradient for this only in the fifteen which \nx two was present right everywhere else the gradient would be zero so that means your \nsum of the gradients overall the endpoints is going to be small or big \nstudent small \nsmall for this particular feature or for this particular weight  it is going to be small right \nbecause you do not have enough samples where you are seeing this so now what would \nhappen to the  update y ou started with a random value for w two a fter one epoch or \nmaking one entire pass of the data  what would happen to the updates for w two very \nsmall very few updates compare this to a feature which is dense  do you get a lot of \nupdates so you see there is something unfair happening here if a feature is sparse it is \nnot getting updated enough  \nnow that was ok  i n one situation if this feature was not really important  b ut now \nconsider the exact example which i gave you which is this an amir khan movie or not  \nbut suppose i am doing a classification whether  this movie is going to be hit or not  i \nwould believe this feature is very important because almost always when he is the actor  \nthe movie is a hit  right so you really cannot ignore this feature you want to learn the \nparameters correctly for this feature  do you get the setup right  t here could be cases \nwhere your feature is very sparse  but at the day at the same time very predictive of the \noutput that you are trying to  learn right and in this case the output is whether the movie \nwould be a hit or not \nthe other example could be is christopher nolan the director so yes probably directed \nless than ten movies but all of them have been at some point in the imdb top two hundred and fifty or \nsomething right so that is a very important feature  but you will not get it very \nfrequently in your data  right so you cannot really ignore these features  that means \nyou still want to learn these features properly  so you have sparse features  you have \ndense features we understand that for the sparse features  the updates would be slower \nand for the dense features  the update would be faster  the sparse would be zero in most \ncases no no so you will do this zero mean thing \nno but if it is a same v alue and you are going to zero mean the data  right so the value \neven if it  is one it is going to be very close to zero right so you always assume zero means \notherwise all this does not make sense  right because if your features are not in the same \nrange then anyways you are in trouble  right fine so this is what i was trying to say  \nthat the gradient with respect to w t is going to be zero for most inputs and hence  w t will \nnot get enough updates  and as  i said if this is an important feature  we cannot really \nignore it we have to make sure that it learns better \nso what is the case that i am making for what do we actually need can you relate it to \nthe discussion on learning rate that we have been having so if the feature is sparse you \nknow it is going to get very fewer updates  so can we change its learning rate  so that \nfeature gets updates a bit  faster as compared to the other features  so you get the \nmotivation right how to do this is a separate story but at least we need to do this  \nrefer slide time sevenfortynine \n \nso the intuition is decay the learning  rate for parameters in proportion to their update \nhistory so you have been recording the update history  you have been looking at the \nparameter you know  all the gradient w two that you had calculated  so far right how \nmany times you had computed the gradients and what those values were actually now for \nthese sparse features those are going to be zero \nso your cumulated history is going to be small  right for a dense feature it is going to \nbe high so why not make the learning rate inversely proportional to this history  that \nmeans if the feature has been updated fewer times  give it a larger learning rate  if it is \nnot updated if it is updated many times  give it to a smaller learning rate  can you give \nme a mathematical formula for doing this  this is the intuition  just think about it for a \nminute learning rate inversely proportional to update history ok good how many of you \nget that but most of you will get it once i show you the answer \nthis is my gradient which i had computed so far i mean at this time step i will keep \naccumulating it in a history vector  so at time step zero i will take the magnitude of this \nagain i am taking the magnitude right because  it does not matter whether you made a n \nupdate in the positive direction or the negative direction  you just matters that whether \nhow much by how much it move so i will just square this quantity so that i can get rid \nof the sign  so i am taking the magnitudes and i am storing all that  so at time step t  \nwhat would vt contain  it is grad w zero square plus w one square grad w one square and so on \nup till time step t \nnow this was my  if i ignore this quantity  this was my normal gradient descent update \nrule now do you see what i have done i have divided the learning rate by whatever \nhistory i had accumulated so for the dense features what would happen  is the learning \nrate will increase or decrease with time  the learning rate will decrease  right and for the \nsparse features relatively less in fact if you have written  gotten zero updates so far so \nwhen you have to updat e the first few times  you will have a very  high learning rate  \ndoes that make sense right because this quantity would be zero so our eta would actually \nbe very large so you see how that intuition got converted into some reasonable formula \nrefer slide time tenthirteen \n \nnow can you tell me a way of actually realising this  i  want to show you that what \nhappens when you have sparse data  and i want to do this with the  toy example that we \nhad where we had only one feature and other feature was always on  right so how do i \ncreate this sparse  data so you should think about these because these are things you \nwill have to do when you  are practising machine learning and if you are  working with \nthe problem and you want to create some simulated data  so that you can  verify some \nhypothesis that you have so how would you do this \nsee i am going to create thousand data points  right which is x  y points and of course i \nhave this x zero which is always on  right so x zero is always on i cannot make that sparse  \nwhat about the other feature if i am creating thousand data points what should i ensure \nis that eighty percent of them or some ninety percent of them is always zero right just as the amir \nkhan case and most of the data it is going to be zero so what we will do is as i said we just \nhave two parameters w and b  b cannot make sparse is always going to be on  so what \nwe will do is  we will make x sparse  we just create random x  y pairs and then for eighty \npercent of those we will set x to zero right so now this x feature is going to be very \nsparse  \nso now i have created some data which is sparse one of the features is sparse and now  \ni want to see what happens when i run gradient descent momentu m and nesterov \naccelerated gradient descent and how does the algorithm behave and now  if i apply this \nalgorithm which i did not name  it is called adagrad ok t his algorithm is called \nadagrad if i apply this algorithm then what how does the situation change \nrefer slide time elevenfiftyfive \n \nso this is what  gradient descent momentum  and nag do now at least the d ifference \nbetween momentum and nag should be clear  nag blue curve is inside the red curve \nright so oscillations are slightly smaller this is how they behave \nnow there is something very interesting that these algorithms are doing for this \nparticular data set that i have created  c an you spot it  w hat is the interesting thing \nhappening here i want you to take some time and think about and relate i t to the \ndiscussion that we just had how many of you see what is happening here very few i will \ngive a hint  ok it is almost as  if these algorithms went to a school where they did not \nteach pythagoras theorem n ow related to the discussion that we just had what is \nhappening initially so initially what is happening is you started from here ok and this is \nthe w b planes so you have w on the horizontal axis and b on the vertical axis \nwhat is happening to all your updates initially  where are you movin g you are moving \nalong the b direction  are you making any movements along the w direction  no why \nw was sparse  its gradients are mostly zero it was not being able to make any updates in  \nthe w direction or it was able to do make updates in the b direction  it did as much as it \ncould do after reaching here it realizes that there is no point in going to b e further right \nit actually took uturn because it realise that there is nothing i cannot really go ahead  i \nhave to now start working in a direction of w \nso now in practice although in this toy example it does not it still converges fast but in \npractice what will happen is you have just moved in one direction  reached a point and \nnow from there again  you are going to take right turn and reach to your destination \nright so you are taking you are doing something which is not fast this is not how you \nwould go from this point to this point  t here has to be a better wa y right and this is \nhappening because w is not getting updated frequently  all the updates are initially done \nfor b \nnow when it is no longer possible to change b because you reached the optimum value \nfor b then only you start changing w and that to very slowly because  it will have to wait \nfor many updates  to happen for that to happen h ow many of you get this  so this is \nexactly what is written on the slides because in our data the feature corresponding to w is \nsparse and hence w undergoes very few up dates and b is very dense and it  undergoes a \nlot of a updates \nnow such sparsity is very common in large neural networks which have thousands of \nfeatures right so you can imagine this  n ow if i have thousands of features  now \nsuppose i am doing credit card fraud detection  ok now say one of my features is \ncorresponding to some education that the person had and suppose he has done some  very \nless sort after degree or less sort after curriculum \nso that feature is going to be sparse where most of the cases  but i cannot ignore it may \nbe this is the most predictive feature that i might ha ve right so you could think of \nvarious cases where you have thousands of features out of which many are going to be  \noff for a given example  right everyone sees that this is the real world scenario  where \nlot of your features are going to be sparse and in many cases  you cannot ignore the \nsparse features ok fine now let see what adagrad does any guesses \nrefer slide time fifteeneighteen \n \nso i am running this we should start seeing something a green curve starting from here \ndo you see what is happening  expected now try to guess if you are going  to run into a \nproblem i  have deliberately  halted the algorithm  i  just want you to think if you are \ngoing to run into a problem  ok a ll of you think you have  something which makes \nsense so now i have run it for in this case again this is the toy example  hence you do \nnot see a lot of difference between these algorithms  in terms of number of steps taken to \nconverge but in real world  application it would be very different  but now what has \nhappened is i have run the algorithm for as much i can and i am then stuck here i am not \nbeing able to move forward why is this happening \nwell i am the histories accumulating it is growing now what am i doing to the learning \nrate i am just killing it right it is eta by a very large constant now that is going to be \nvery small so no matter how big my gradient is  it is going to get multipl ied by a very \nsmall learning rate and i cannot just move any forward  anymore right so see that will \nhappen that is  why in th is case this is some point here which i do not want to go over \nnow and it is this \nin fact i do not have an explanation for that but this one observation which people have \nmade that remember we have the square root in the denominator  i f you remove the \nsquare root in principle you are still doing the same thing  right you are still making it  \ninversely proportional to a cumulated  history but it does  not work well  when you do \nthat that i do not know why it happens and i just read these comments at several places \nthat it does not work when you remove the square root from the denominator  but that is \nnot important for this discussion that is just point for reference later on \nso right now what i am trying to say is that it did the right thing  it  started making \nupdates for w also and started making larger  updates hence we se e this simultaneous \nmoment in both w and b direction but the flip side is over a period of ti me the effective \nlearning rate for b will decreas e so m uch that we no longer be able to move in the \nvertical direction right and if i am not being able to move in the vertical direction  we \nwill not reach the minima  in this particular example  not always but in this particular \nexample you need to move further in the direction of b  but a le arning rate is not \nallowing you to do that so that is what is happening \nso now can you avoid this  yes how multiply by  so  first divide it  so  that the \ndecreases then multiply it so that does not decrease all of these are interesting ideas i \nam not i mean it is very hard to say upfront whether this is wrong or right but yeah these \nare you get the idea basically something is happening which is you are aggressively  \nkilling the learning rate \nrefer slide time seventeenfiftynine \n \nnow i just want to make sur e that you are not  so aggressive so what happens because \nof the aggressive killing  is the frequent parameters  they start receiving fewer updates  \nnow this is what  rmsprop does i want you to stare at this for a minute  assume that \nbeta is going to be something which is greater than zeroninety or zeroninetyfive or something and try to \nmake sense of what is happening try to imagine what is vt is going to look like  in terms \nof grad w zero grad w one and so on to start from v one and see what happens what was v one \nearlier and what it is going to be now ok but it still grows my magnitude when i am still \nadding stuff so how does it help me in not blowing of the denominators \nso yeah i think you most of you get  so again this is the trick is basically you are using \nthis expon entially exponential moving average  so even at the first step  earlier i was \ndoing grad w t square now actually doing zerofive into grad w t square  oh sorry grad w one \nsquare right so that is what my v one is going to be now what is my v two going to be it \nis going to be zeroninetyfive into zerofive grad w one square plus grad w  two square right so this \nquantity is even shrinking further  and at each step this is going to keep  a zerofive ok and \nyou see now at each step this is going to get multiplied by this quantity and shr ink \nfurther \nso now i am not aggressively growing  the denominator i am not considering the full \ngradient but only a fraction of it and  in fact a very small multiple of it  so i am still \naccumulating the history but i am not being very aggressive whil e doing that right so \nyou understand this everyone gets this \nrefer slide time nineteenfiftyfour \n \nso now let us see if we run what would happen  any guesses ok so initially now this \nis i think a brown curve  it is already there but you can see it so i will keep running it \nand at some point  it will diverge  from the green curve  yeah do you see that  now i \nhave reached its destination right so at the point where the b learning rate  the learning \nrate for b was getting killed  i n this case that does no t happen because you have \nprevented the denominator  from growing very large actually  multiplied by its small \nvalues so that it does not grow very fast  \nso adagrad got stuck when it was close to convergence because the learning rate was \nkilled and it was  no longer able to move in a direction of b  but for rmsprop it \novercomes this problem by not growing  the denominator very aggressively  ok n ow \ncan you think of any further modifications  there is everything that you learned  so far \nand my everything yeah  \nyeah i am not very sure why that i agree that i am also bit surprised that it completely \noverlaps with it  i checked it and that  is how it turns out to be and guessing it  is a n \nartifact of the artificial data that i have created so it is trying to say is actually making \nsense that it should  not overlap  so much right initially it should slightly be biased \ntowards b and then  probably that is what you are trying to say right  but i told it just an  \nartifact of this data that i have but what matters is from as going to say illusion but from \nthe illustration is that it actually does not kill the learning rate \nrefer slide time twentyonefortyeight \n \nwhat is  the one idea  that now think  of everything that you learned in  starting from \ngradient descent then you tried to improve it  using something then you tried  to further \nimprove it and  so on and now  we have taken a slide  d two from there  you are now \nfocusing on the learning rates  but there were other  things which you are doing earlier  \ncan you bring those back add momentum how many of you say add momentum as if i \ncan just added you are right actually \nso let us see what we can do  so it does everything that rmsprop does that means it \ntries to make the learning rate inversely proportional to a sane  cumulated history by \nsane mean it does not allow the  history to blow up and it also will use the cumulative \nhistory of the gradients  so let us see the update tool for adam so what is this term \ndoing a ctually it is taking a moving average of  there is the same as  the momentum \nbase role right just taking a moving average of your gradients  ok the same analogy  \nthat i am going to phoenix market city  i am just taking all my history into account  ok \nand vt  is again a cumulative history  t his is the same as what was  happening in  \nrmsprop right where you get lost  \nnow what would be the next step  be can you give me the final update rule  at least \nthink about it mt into vt no ok just try to think about it  and it is very hard to say it out \nthere are too many grads and suffixes and so on so just think about what you did in the \nmomentum case ok now there is one more step which i am going to ignore i will just \nsay what that step is and then i will come back to that later on \nso this is something known as bias correction ok just ignore it for the time being i will \ncome back to this discussion just for the time being just assume that i am taking mt and \ndividing it by some quantity  right so for all practical purposes i am just using mt just \ndividing it by a  quantity ok just for now that should suffice and then my final update  \nrule is going to be this  \nso let me go over this  what did you expect here in a normal gradient descent  they \nshould have been grad w t  th at means the derivative with respect to c urrent w  ok \ninstead of that i am using a cumulated  history instead of using just this quantity  i am \nusing a cumulated history does it make sense this is same as momentum base gradient \ndescent how many of you get that ok and now this quantity there is nothing new this \nis the same as what rmsprop suggested that you divide the learning rate by a cumulated  \nhistory of gradients  right so just a combination of these two  one is take care of the \nlearning rate and the other is use a cumulative history does it make sense now ok fine \nnow this part is something that i need to tell you about  so i will tell it to you after  i \nrun the algorithm and then  i will come back to that  but is the update rule clear that it is \na combination of momentum plus killing the learning rate ok fine \nrefer slide time twentyfivenineteen \n \nit is a similar set of equations for bt \nrefer slide time twentyfivetwentyone \n \nnow let us see what happens to  this algorithm is actually call at adam it  stands for \nadaptive moments right yeah what is can you tell me why that name \nwhy moments \nstudent sir mean is \ngood where is the mean  here this is a mean  this is a moving exponentially weighted \naverage right this is a n exponentially weighted mean  what about this  what is this \nquantity if  you tak e the average of this is the second moment  right exponentially \nweighted second moment right so using the first moment and the second moment  we \ncome up with an adaptive learning rate \nso now i will run this algorithm are you able to see this  see a coloured curve ok so \nit is here you see that now ok do you see what happen do you see this curve everyone \nsees that  ok so what is happening  it is taking u turns right so again whatever  \nhappens because of momentum  it is happening in this case  also and then finally it will \nconverge again let me be clear that in this  case now it should be very clear  we need to \nchange who is ta for the slide  so this colour  needs to be change d or it should be \nbright right from the first  so what is happening is it is getting overlaid  and then  it \nbecomes bright when we need to have a brighter colour right from the beginning ok \nso this again in this toy example  right you do not really see the speed as such because \nall of them are converging you know  almost the same number of steps  but this again i \nrepeat for the toy example  but at least you see that the behaviour is very different and \nbehaviour is consistent with whatever you have put into the update rule  right in one \ncase the learning rate gets killed  in the second case  it does not decay and in third case \nwhen you using this moments  sorry this momentum term you again have this behaviour \nsimilar to the momentum gradient descent  where you actually overshoot and then  you \ncome back ok so is that clear all these algorithms ok now here is the million dollar \nquestion \nrefer slide time twentysevenfiftythree \n \nwhich of these two you use in practice so what are the options that you have for your \nback propagation assignment even if you have not read the assignment you should just \ntell me based on whatever you have learned you have gradient descent \nstudent momentum \nmomentum \nnag rmsprop \nstudent adagrad \nadagrad adam ok so which of these would you choose and if there is one or which is \ncalled eve but it did not really gain much momentum  but adam so in practice adam \nseems to be more or less the default  choice i should tell you that recently there was a \npaper or called couple of papers which actually show that there is a slight error  i mean \nthere is you could showcase where adam will not actually converge as  expected with \nbut still then after that as is the case in whole of deep learning  resources that one person \nsays this work  and immediately  the next is someone else  this does not work or vice \nversa right \nso someone show that this does not work  adam does not work in some cases  but then \nsomeone else did detailed study showing that in most practical applications  ok you have \ntaken a toy  data set where you can show something under some conditions  adam will \nnot converge but if i look at real world data sets  like mnist image data or something \nthose conditions do not hold there so adam really works well  so in practice adam is \nmore or less the  standard choice  n owadays at least all the image  classification work \nwhich deals with  convolutional neural networks and  convolutional neural networks and  \nso on that uses adam as the optimization algorithm \nwe have used it largely for a lot of sequence to sequence d learning problems and it \nworks well although it is supposed to be robust to the initial learning rate  right because \nyou are tampering with the learning rate as you go along  right you are not sticking to \neta but you are conveniently blowing it up or shrinking it based on your requirement  \nso it should  not be sensitive to the initial learning rate  but we have observe d that at \nleast for the sequence generation problems  if you use one of these learning rates as a \nstarting point  they work best  of course  of course  these are heuristic  right we also \ndepends on how much data you have and so on \nif you are going to train but only thousand samples and first of all of course you should \nquestion why are you using deep learning but you have gone pass that question already  \nhas everyone else has then you are still be using a deep neural network  and in that case \nmay be these learning rates are going to be very small  but in general for a large number \nof data sets out there  which lot of academic research happens which are of reasonable \nsize these learning rates happen to be well in practice \nnow having said that many papers report that sgd with momentum either the nesterov \nmomentum or the vanilla momentum with a simple annealing learning rate  we \nremember we did this learning rate decay  either a constant decay or that heuristic decay  \nthat after you look at the validation loss and then  decide whether to decay or not  that \nalso seems to work at par with adam right so my advice would be that if you really \nknow what you are doing with sgd and momentum right that means if you really know \nhow to look at the loss how to track it how to adjust the learning rates and so on \nwith a little bit of manual tampering  it should work as well as adam there are people \nwhich show that it works well as adam but if you are j ust a practitioner who does not \nreally want to bother too much about setting the learning rate  setting the momentum  \nsetting the schedules on both of them  remember for momentum also we had a  schedule \nand was just  given by one of these papers  and it might  differ for your application  you \nmight want to tweak that a bit  so if you are not really bothered about doing all these \nthings then adam would just be over all the best choice  right with very  minimum \ntempering of the initial learning rate \nas i said s ome recent work suggested there is a problem with adam and we will not \nconverge in some cases but then it still i mean i would say that juries not out on  that yet \nbecause there is of course  theoretical angle to it and also  the practical angle  a gain \npractice has been used widely for the last three to four years at least and it works  well in a \nlarge number of applications right so that is why adam would typically be the overall \nbest choice \nnow there is this one thing which i need to do which is i need to tell you why do we use \nthis bias correction so now what do you actually want  to you are taking a mean  ok \nyou do not want to rely on the current estimate of the gradient  but you want to take a n \nexponentially moving average of the gradients \nnow what would you actually  would be doing all this  what is the intuition behind this \nsince you are talking about moments and  so on can you think in terms of probability \ndistributions so let me just try to say this we write  that your gradients your values of \ngrad wt right and i will just i think alternately use gt instead of grad wt  just needs to \ngradient in that form it actually comes from some distribution depending on the point at \nwhich you are right the gradient would change but it comes from a certain distribution \nand now what you actually want at any time step  when you are making this update  this \nparticular update ok is it clear yeah when you are making this update what would you \nactually want it should not move too much away from sorry \nso now your gradients how you are computing say if you are doing the  stochastic \nversion you are computing it for every point that you have  right with respect to that \npoint you would have some loss function and some derivative with respect to your \nparameters if you move on to different point  you will have some different parameters  \nso there is some randomness in this  ok so i am saying that these gradients would be \ntreated as random variables which can take on values according to a certain distribution \nand now what do i mean so what would i actually want when i am making an \nupdate so i have to one basic choices  i could have just use grad wt which is the \nderivative with respect to the current time step  add the current time step ok instead of \nthat i know why i am not happy with that because it has this problem that it could pull  \nme to the extreme so at this point is actually saying change it change your w value in a \nparticular way which is more suited to me some other point would say something else  \nso what we want is that whatever  update we make should be very close to the dash of \nthe distribution mean of the distribution right and instead of computing the mean we are \ncomputing a moving average and exponentially moving average \nso now what do we actually want to say i said that gt is the random variable for \ndenoting the gradient what do i actually want i want the expected value of mt should \nbe equal to what the true expected value of gt this is what i want because i want to i do \nnot want my upd ates to move in the extreme  it should be closer to the average to the \nmean of the distribution  do you agree that this is my wish list  this is what something \nthat i should desire for  ok now let see what is m t actually if i want to write it as a \nformula \nrefer slide time thirtyfivetwentyeight \n \nso i have mt is equal to one minus beta i will call this as gt right so remember the gt is \ngrad of w t ok so now let us try to write formula for this so m zero i will set it to zero so \nm one is going to be one minus beta in to gone ok mtwo is going to be beta into one minus beta into \ng one plus one minus beta into g two and m three is going to be beta into one minus beta square g one \nplus beta into one minus \nstudent beta square \nsorry beta square \nstudent minus beta  \nminus beta wait is the first term correct \nstudent yes \nno beta ok so wait what am i oh beta is getting multiplied  to g two plus one minus beta \ninto g three so what is the general formula going to be it is mt is equal to so one minus beta \ncan come out ok summation i is equal to one to t one minus beta \nstudent beta square \nbeta raise to t minus i and gi right ok so this is what my mt is ok now let me take the \nexpectation of this this fine now ok this is b one minus beta now this is going to be  is \nthat fine so what is this this is an ap gp what is the sum going to be \nso it is going to be one over one minus oh it is actually sorry one minus beta raise to t over one \nminus beta is that fine so what will happen is this will get cancelled and what you are \nleft with this one minus beta raise to t into e of gt ok so what is the relation that you have \ne of mt ok e of mt is equal to one minus beta raise to t into e of gt what did you actually \nwant \nstudent e of gt \nright so now how will you ensure that divide by divide mt by one minus beta raise to t and \nthat exactly the bias correction that we have done  ok sorry about this messy derivation \nbut i guess most of you get it  if not we will just type it properly and  upload it in the \nslides how many of you got this most of you got fine so that is the similar derivation \nfor vt also fine so that is why we need the bias correction"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 5.9 Gradient Descent with Adaptive Learning Rate.wav", "duration": 2407.22, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  five \nexplanation for why we need bias correction in adam \n refer slide time zerotwelve \nso in this video we will try to look at an explanation for why we need bias correction in \nadam or in other words i want to explain why do i do this particular step why did i take \nm t and v t as it is but why did i do this particular step which i calle d as the bias \ncorrection step \nrefer slide time zerotwentyone \n \nso note that  in the case of adam if you look at this equation for m  t we are actually \ntaking a running average of the gradients and storing it as m  t right  so this is the \ngradient and we are taking a running average or exponential runn ing average of these \ngradients exponentially decaying running average \nso the reason we are doing that is that we do not want to rely on a single estimate so we \ndo not want to rely only on gradient of w  t we want to look at the overall behaviour of \nthe gradients over multiple time steps and then take a decision  so that means in one \nparticular gradient at time t is actually pushing us in some direction we  do not want to be \nvery hasty and start moving there we want to accumulate the history and appropria tely \nweigh everything in the history  that is the idea behind  taking this running average of \nradiance \nand the other way of looking at is that we are interested in the expected value of the \ngradients and not the point estimate at time w  t right at time t rather so gradient of wt \nwhich is this quantity which is the point estimate at time t  we are not interested in that \nwere interested in the expected value and our behaviour should be according to the \nexpected value that is what we desire \nso however instead of computing the expected value of this quantity which should have \nbeen ideal we are computing mt as the exponentially moving average  so in the ideal \ncase we would want that these two quantities are the same that the expected value of mt \nthe way i am computing it and the expected value of the gradient  of w t should be the \nsame if that is the same then  i am fine because then  that means  i am just taking the \nexpected value or the  of the gradient instead of relying on the point estimate ok  so let \nus see if that is indeed the case \nrefer slide time twofourteen \n \nso for convenience we are going to just denote this gradient w  t as g t because it  is \ncumbersome to write this grad symbol and we will just  not make it  so readable the \nderivation that we are going to do so i am just going to replace that as g t so what i have \nwritten is g t here instead of grad w t right so from now on i will just use g t for grad w t \nis that fine ok so we have this expression for m t \nso now let us just try to expand it and s ee what happens right so m zero it is going to be zero \nbecause that is my starting points i have no history nothings so i will just going to keep \nit as zero m one is my first time step at which it is going to be beta into m zero so i am just \nsubstituted t minus one and t here and in the original expression  i have just substituted \nappropriate quantities for m of t minus one and g of t so m of t minus one is zero m zero and g of t \nis g one and of course b zero m zero itself was zero so what will be left it is one minus beta g one \nnow let us look at what happens is m two m two is going to be beta m one plus one minus beta g \ntwo but i already have an expression for m one so i am just going to substitute that here and \nthis is what i get now let us look at m three m three is again going to be beta times m two plus one \nminus beta times g three and i have an expression for m two so i am going to substitute that \nhere and see if that leads to something interesting \nso i have just substituted the value of m two here right and i already had the m three part here \nthe this term here as it is ok and now let us see so this already starts looking something \ninteresting you see some pattern here  in particular we could take these one minus beta \nterms outside they can be taken common and then you will be left with beta square g one \nplus beta s quare g one plus beta g two plus g three so let us try to write this more compactly \nright so i have taken one  minus beta  common and then  i have written the remaining \nterms as this particular summation and you can verify \nso when i is equal to one this is going to  be beta three minus one which is beta square into g one \nwhen i is equal to two this is going to be beta three minus two which is going to be beta into g two \nand when i is going to be three this is going to be beta raise to three minus three which is beta raise \nto zero which is just one into g three right so we get back the same expression that we had here \nof course there is a one minus beta outside so this is a more compact way of writing it and \nthis was for the threeth entry right this was for m three the third entry \nnow what if we want to write it for the t\u2019th entry in general what if we want to write the \nexpression for m t \nrefer slide time fivefive \n \nso in general m t we can write it as one minus beta as i equal to one to t b beta t beta raised \nto t minus i into g i right so this three is here i have just replaced them by t  s right you can \njust verify that this is from you can just generalize from the third entry to the t\u2019th entry \nrefer slide time fivetwentyseven \n \nso now let us see we have the following expression we have simplified the expression \nfor m  t and written it more compactly  but what we were eventually interested in the \nexpected value of m t right we wanted to show that certain things holds for the  expected \nvalue of m t \nrefer slide time fiveforty \n \nso you just take expectation on both sides  so this is what we will  get ok now one minus \nbeta is of course a constant  so i can move it outside the expectation  so then i get an \nexpectation of a sum \nnow the expectation of a sum is the same as the sum of expectations  so i can write it as \na sum of expectations ok now again beta is a constant so i can take it outside the expect \nexpectation so what i will be left with is beta raise to t minus  i outside and expectation \nof g i right so this is actually expectation of g one when i equal to one then expectation of g \ntwo expectation of g three and so on \nnow we will make an assumption that all these g i\u2019s that means the gradient at time \nstep one the gradient at time step two the gradient as time step three and so on they all come \nfrom the same distribution ok  we are going to make that assumption  so let us try to \nunderstand the implication of that right  so let us say this was a distribution from which \ng one came right suppose  i am dealing with a scalar quantity and maybe this was the \ndistribution from which g one came now g two could have come from a different \ndistribution g three could have come from a different distribution and if that was the case \nthen expectation of g one would be different from the expectation of g two and so on \nso what we have assumed to it will make things simple for us is that g one g two g three any g i \ncomes from the same distribution  and hence you can say that the expectation of all these \ngi\u2019s is going to be just the expectation of g  that is this one single distribution from these \nwhich these entries come t his of course a very strong assumption but we are going to  \nlive with this assumption \nrefer slide time sevensixteen \n \nso then this expectation of g  i just becomes expectation of g  so i have gotten rid of the \nindex i that means  i can move it outside the su mmation right so this is what  i will get \nnow these two have come out of the summation and inside  i have this quantity now let \nme just expand this quantity this is nothing  but beta raise to t minus one plus beta raise to t \nminus two plus so on at last you wil l reach t minus t which is just going to be beta raise to \nzero \nso this is nothing but a sum of a g  p with common ratio beta and  i can replace that sum \nby this formula  you know this is the formula for the sum of a g  p with common ratio \nbeta so i have just replaced that and now what happens is this one minus beta and one minus \nbeta cancel out so i get this particular expression that the expected value of m t is equal \nto the expected value of g into one minus beta t \nso i will just take one minus beta t on the othe r side and  i can move it inside the \nexpectation because it is a constant it does not matter  so i will get as oh actually yeah i \ncan just move it inside so i will get it as expectation of m  t over one minus beta is equal to \nexpectation of g t right and this quantity the one which  i have circled is nothing  but m \nhat t right this was exactly the bias correction that  i was applying if i go back to the \nprevious slide or the slide before that  so this was exactly the bias correction that  i was \napplying \nso what i have inside is this  so what i have shown is that if  i apply the bias correction \nthen the expected value of the bias corrected m  t is equal to the expected value of the \ngradient and that is actually what i wanted i wanted that whatever m  t i am computing \nif i look at its expected value it should be the same as the expected value of my gradients \nand that is what i have arrived it \nhence this bias correction makes sense and hence we apply this bias correction for \nadam so this we have shown for m t we had a similar expression for v t right so for m \nt we had this bias correction as m hat t and similarly for v  t also we had this bias \ncorrection as v hat t  so you can derive the same kind of derivation for v  t also and show \nthat that bias correction makes sense right so this is an explanation for why you do bias \ncorrection in the case of adam \nthank you"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.1 Eigenvalues and Eigenvectors.wav", "duration": 1028.74, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  six \neigen values eigen vectors eigen value decomposition principal component \nanalysis singular value decomposition \nso this lecture actually is a bit of a digression and it is supposed to cover  some of  the \nbasics that we  need for various sections of the course  so it is very important that you \nunderstand some concepts for linear algebra specifically eigenvalues eigenvectors and in \nparticular today we will do principal component analysis and the reason that i do it is \nthere is an very neat relati on of pca and to autoencoders an autoencoder is something \nthat well cover in the course it is a part of any deep neural network course \nand singular  value decomposition is something  that we  using  when  we  learn  word \nvectors the word vector is again something very important i can just i can do the non \nsvd version of it where i just talk about what word to wick is but that will not give you \nthe same probably not the same interpretation as if you start from svd and then reach \nword vectors right so that is why i am covering these basics \nso how many of you know eigenvalues and eigenvectors very embarrassing question \nhow many of you absolutely hate eigenva lues and eigenvectors so let us see if we can \nchange that today i mean on the positive side  \nrefer slide time onetwentyfour \n \nso what happens when a matrix hits a vector so most of you a lot of people that i talk \nto right actually think that eigenvectors are the villains of linear algebra it is very hard to \nunderstand them and so on  but today i am going to make a case for they are not the \nvillains they are actually the superheroes of linear algebra so that is  what the lecture is \nabout so what happens when a matrix hits a vector \nstudent transforms it \ntransforms it right so actually what happens is that it strays from it is path so this is \nthe original refer time onefiftyeight this is the  original vector x ok and now once i multiply \nit by a that means  if i do the transformation a x  then i get a new vector  and two \nthings happen right one is the direction changes which is obvious and in many cases the \nscale also changes that means the vector might get elongated it is magnitude would \nincrease or it would decrease  \nso if you really think about it actually right so matrices are the real villains of linear \nalgebra right and we just look at this vector was minding it is own business going along \nit is own direction a  metric comes and hits it and completely changes it is world right i \nmean it just throws it off path increases a dimension or slows it down or whatever it  so \nthat is they are the bad guys now for every villain what do you have a super hero right \nso what is a super hero corresponding to orbit what does a super hero do know that \nis a very linear algebra i am talking about comic books that this is very linear algebraic \nanswer he stands up to the villain right \nrefer slide time twofiftyfour \n \nand that is e xactly what eigen vectors do it right they refused to change th eir part they \ntell the matrix you can hit me as many times as you want probably you can increase my  \nyou could probably  slow me down a bit or push me ahead or something but i am not \ngoing to stray off from your path right so that is what eigenvalue eigenvectors do \nso here is a matrix which is a villain and here is an eigenvector which is our hero and \nnow when this matrix hits this eigenvector it refuses to stray from it is part right it says \ni will move forward i will move back whatever but i will not change my direction ok i \nwill just stay honest to what i am and these vectors are called the eigenvectors i am \nmore formally you can write it as ax is equal to lambda x right so that mean s the \ndirection remains the same only the scale changes it will either get slowed down or it \nwill get boosted up right so the magnitude would change but the direction remains the \nsame \nrefer slide time threefifty \n \nnow what is so special about eigenvect ors like why are why is it that  they are always \nin the lime light i know the any course that you do invariably touch eigenvectors or \neigenvalues at some point  in that course right where be it machine learning  image \nprocessing whatever you do you alway s speech everything that you do you will always \nhave eigenvectors and eigenvalues why is it so well it is turns out that several properties \nof matrices can actually be explained away by looking at their eigenvalues  so if i look \nat a matrix i would pro bably not be able to comment much on it but if you tell me \nsomething about the eigenvalues \ni can see a lot of things about of it  and there is an entire field on this way this entire \nspectral graph theory which looks at properties of laplacian matrices an d come in \nsomething on the properties of the graph and so on right and that is just an example \nwhich we do not care about but what we care about in this course there are a few things \nthat we care about with respect to eigenvalues and eigenvector and tha t is what i am \ngoing to focus on right so that is what this lecture is going to be out and i will take two \nspecific cases which are very important for us to understand certain concepts later on \nso i will start with the first one \nrefer slide time fourfifty \n \nand i will start with a very  simple example to motivate this problem and eventually \nwill lead to a result  which will help us understand a very important concept in  deep \nneural network training which is exploding and vanishing vanishing gradient we will \nnot touch that concept today but we will use these ideas when we are looking at that \nlater on \nso let us take this example of two restaurants so there is a chinese restaurant and a \nmexican restaurant and on day one k one students eat in the chinese r estaurant and k two \nstudents eat in the mexican restaurant so this is what my situation is on day zero k one for \nchinese and k two for mexican now what happens as is obvious people get bored or they \nhave different want to try out different things so on day two or other each subsequent \nday what happens is that a fraction p of the students  who ate chinese today will opt for \nmax mexican on day on the next day and a fraction q of the students who ate ma \nmexican today are going to opt for chinese \nso you get this situation right so i started with k one k two so what i am saying is on day \none that is the next day only a fraction p of the k one students will remain for chinese and a \nfraction one minus q would be transferred from mexican to chinese ok  and similarly only \na fraction q of the students would again stick to the mexican food and a fraction one minus \np into k one would shift from chinese to mexican is this setup clear ok can you write this \nas a matrix operation it would be a matrix multiplied by a vector right can you tell me the \nvector \nstudent refer time sixtwentynine \nk one k two k one k two and the matrix is in all this ok this is what it is and i am saying that this \nhappens on each subsequent day it is every day now this keeps happening so on day one \ni started with s ay one hundred and eighty and now day two it change to something again day three it will change \nsomething by the same fraction \nnow let me call this as matrix m and this is of course v zero right by definition as we \ndecided now what would happen on day two what would v two be m applied  to v one right and \nwhich would be m square applied to v zero i am just substituting the value of v one which is \nm into v zero in general on the nth day what would happen m raised to n into v zero ok so \nyou see that the number of customers in the two restaurants is gi ven by this series you had \nv zero then m into v zero then m square v zero and so on up to m raised to n vn ok you see how \nthe number of customer is changing \nnow and this is how i represent it as a state transition diagram right so i had certain \nnumbers on day one and it changed with the trans with the probability p they will stay back \nwith a probability one minus p they will move to the next or the different restaurant and so \non right \nrefer slide time seventhirtytwo \n \nand now this though a very toyish example can you relate it to many things in real life or \nmany things that you will take in decision making right that you are so even if you are \nplaying a game for example and even if you are playing atari games or something you \nare in a certain state based on some action that will take will move to a different state and \nso on right so these things happen in various real world applications right there is a \ncertain state for example even in stock market prediction you are at a certain  value of \nfish stock it might chan ge to a different value right and these values you could just say \nthem as high low or neutral that i am not going into the actual numbers \ntoday the stock value is high it does it possibility that it will transition to something low \nand so on right so these kind of state transition diagrams occur in various real world \nexamples now this is a problem for the two restaurant owners right why is this a \nproblem for the two restaurant owners they do not know how much food to make but \nevery day the number of customers is changing right but is the number of customers \nactually changing will the system eventually reach a steady state will it is it obvious \nthat it will reach a steady state or maybe it will not even reaches steady but the way i \ndescribe it i do  not see why it should reach a steady state right you have some people \nhere they go there come back go there and so on \nthe only thing which i have assumed is that the transition matrix which was the matrix \nm is constant across all the time steps right so every day it is at the same priorities by \nwhich things are changed right so what is your guess if i were to ask you to take a \nguess ok let us see how many of you think and it is  there is no correct answer here at \nthis point so just tell me how many of you think it will reach a steady state how many \nof you think it will keep changing and why is the sum never equal to one  ok so fine so it \nturns out that they will right and let us see how  \nrefer slide time nineforty \n \nso we will define some things and  some of these are just definitions some of them have \naccompanying proofs which i am not going to do here you can the proofs have been \nlinked from the slides so you can take a look at them if you are interested \nso suppose there is a matrix a  n cross n matrix which has eigenvalues are lambda one \nlambda two up to lambda n now what this definition is saying is that assume that there is \none eigenvalue which is greater there is no assumption actually the eigenvalue which is \ngreater than all the other eigenv alues is called the dominant eigenvalue and when i am \nlooking at a dominant eigenvalue i am only concerned with the magnitude not the sign \nso it could be that an eigenvalue is minus ten and all the other eigenvalues are one two three four five \nso the dominant eigenvalue would be minus ten right and i will just take it as step is that \nclear the definition of a dominant eigenvalue \nnow how many of you know what is the stochastic matrix  so matrix m is called a \nstochastic matrix if all the entries are positive and the  sum of the elements in each \ncolumn is equal to one so now this definition is again slightly misstated so there is a row \nstochastic matrix the column stochastic matrix and also doubly stochastic matrix right \nso what i am talking about here is a column stochastic matrix like our matrix have you \nseen such a stochastic matrix any time in your life in the last five minutes the m matrix  \nright so the m matrix is a stochastic matrix because the sum of the columns was one \nright you had p one minus p q one minus q ok or was it some of the rows was one rows was one is \nit the columns \nso this is a stochastic matrix just a definition now i combine these two definitions \nwhich is dominant eigenvalue and stochastic matrix and give you a theorem right so \nthe largest dominant  or the dominant eigenvalue of a stochastic matrix is equal to one ok \nso to prove this what do i have to prove so i need to prove two things one that one is an \neigenvalue of this matrix of any stochastic matrix and second all the other eigenvalues \nare less than one so that is exactly what this proof does here  you can take a look at it and \njust to give you a heads up so last year i use to do this that please see the proof go back \nand look at the proof people never look at the proofs \nso i used to ask them in the quiz where i should be sure that people not going to answer \nright so please when i say go back and look at the proof do that ok  so and lastly if a \nis an n cross n square matrix and you have this series a v zero a square v zero up to an vn \nthen this series will converge to the  dominant eigenvector of a what does a statement \nmean let us not get into the proof right what does it actually mean ok so let us start \nwith very basic stuff right what is the series actually what is each element in this se ries \nit is a vector it is a vector everyone gets that every element in the series is a vector \nnow what do i mean that a series of vectors  converges to the dominant eigen vector \nwhat is convergence mean if i keep finding the next element next element ne xt element \nof this series and i keep doing this as long as i can i will reach a value n right where n is \nthe nth element in the series which will just be a multiple of the dominant eigen vector is \nthat clear you not seem to be clear everyone gets that \nso what do you mean by if you take a series of numbers and if i say that the series \nconverges to zero what does that mean if you keep finding the next element in the series \nyou will hit a point n where you find the nth element of the series and it will be zero refer \ntime thirteentwenty that  ok so we will just i will leave it at that for now now  so stochastic \nmatrix dominant eigenvalues the connection between two and the convergence theorem for \na series of vectors which is a v zero a square v zero and so on \nrefer slide time thirtythirtysix \n \nnow let ed be the dominant eigen vector of m where m is a dash matrix in  our case it is \na stochastic matrix so what with the corresponding dominant eigenvalue be \nstudent one \none ok so given the previous definitions and theorems what can y ou say about the \nsequence it converges to a dash of ed \nstudent refer time thirteenfiftynine \na multiple of ed right so there exists an n such that the a length nth element of the \nseries which is given by this is going to be equal to some multiple of the domina nt \neigenvector no no k is some multiple no this is not related to eigenvalues yet just wait \nfor the next statement then you will see the difference that this is not the do eigenvalue \nyet \nnow my question is what happens from here onwards what would be  the next element \nin the series how many of you say some k dash into ed what is the other pause i do not \nhave the other option what is the other option \nstudent k into ed \nk into ed how many of you say k into ed a large number of ok so you see that now just \nnotice the eigenvalue will come up right so at step n plus one you would have m into vn \nwhich is m into k into ed and this quantity is actually one so the theorem says it will \nconverge to some multiple of k and now if it is a stochastic matrix what  will happen \nafter that time step it will just remain the same vector \nso what would happen to the number of customers in the two restaurants it will remain \nthe same right  you get that  ok fine now this was all for what kind of matrices \nstochastic matrices square stochastic matrices \nrefer slide time fifteenfifteen \n \nbut we generally care about any square matrix in fact we should care about any matrix \nnot discriminate but any square matrix will do for now  so for a square matrix let p be \nthe time step at which this series approaches a multiple of the dominant eigenvector \nthe theorem was for any square matrix remember it was not for stochastic square \nmatrices we just use this value that for a stochastic square matrix the  dominant \neigenvalue is one which  it need which leads to that neat result that the num then  the \nnumber of customers just becomes constant but for any square matrix i could write it as \nthis that there exist some step p at which  the element of the p\u2019 th element of the series \nwould just be a multiple of the dominant eigenvector \nnow what would happen a t step p plus one is this fine  what about step p plus two and in \ngeneral at p plus k or p plus n everyone gets this so now  can you tell me what does this \nknowing this dominant eigen value tell us about this series when will it stabilize \nactually \nstudent refer time sixteentwentyfive \nwhen lambda is equal to one that is the case we already saw if the dominant eigen value is \ngreater than one what would happen \nstudent refer time sixteenthirtythree \nseries will explo de the series will explode and if it is less than one what would happen  \nthe series will vanish ok so this is an important result that we will use when we are \ndiscussing exploding and vanishing gradients \nso we will see that in the case of something one as a recurrent neural networks you end \nup with something of this sort and  then i will make some comments on that right so \nthat is why we will be using this will come probably six  seven or maybe more lectures down \nthe line ok but we will be using it at this point so the main result from here is that if the \ndominant eigenvalue this should be lambda d is greater than one then it will explode less \nthan one it will vanish and equal to one it will stabilize so that is one result one important \nproperty of eigenvalues and eigenvectors that well be needing at a later point in the \ncourse"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.2 Linear Algebra \uff1a Basic Definitions.wav", "duration": 637.54, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sixtwo \nlecture  six \nlinear algebra  basic definitions \nnow from here on we will go on to somethin g even more ba sic we will star t defining \nsome basic definitions from linear algebra a nd these are aga in important for something \nthat i need in the next lecture so let us start with this  \nrefer slide time zerotwentyseven \ni mean in the process we all just see why the eigenvectors are important for us in this \ncourse  \nrefer slide time zerothirtythree \n \nso how many of you know what a basis is so a set of vectors belonging to r n is called \na basis if they are linearly independent right and every vector in r  n can be expressed \nas a linear combination of these vectors  so a set of n vectors vone to vn is linearly \nindependent if no vector in the set can be expressed as a linear combination of  the \nremaining n minus one vector so a more weird we are stating it that so that everyone get \nconfuse is that if you take this linear combination \nthe only solution to this is all the ci\u2019s is equal to zero and that make perfect sense right that \nis that same as that linear combination linear independence and all that thus is make \nsense to everyone ok so  what does linear independence mean that  any vector from this \nset cannot be expressed as the linear combination of the other set  other vectors in the set \nand a more formal way of saying that is this everyone gets this what is  linear \nindependence  \nrefer slide time onethirtyfive \n \nnow let us consider some very stupid examples again the space r two and we consider \nthese two vectors one zero and zero one are they linearly independent yes  ok they cannot be \nexpressed as a multiple of each other right now any vector ab bel onging to r square \ncan be expressed as a linear combination of these two vectors ok and  x and y are linearly \nindependent the only solution is c one x plus oh sorry c one x plus c two y is c one and y equal to \nzero what about if i move to r three one zero zero zero one zero and zero zero one so x y and z axis right are the unit \nvector  \nrefer slide time twotwelve \n \nso in that x and y turns to be unit  vectors in the direction of the coordinate axis and we \nare used to representing every point in r two as a linear combination of these two vector is \nthat exactly what i what we do so when we say that i have a point two co mma three i am \nactually telling you that the point is two one zero plus three  zero one right i am expressing at are the \nlinear combination of the coordinate axis  \nrefer slide time twothirtyeight \n \nbut now this nothing sacrosanct about x and y right  i could have cho sen just about any \nother axis so in particular we could have chosen this as our basis are these two vectors \nlinearly independent can any vector and r two be expressed as a linear combination of \nthese two vectors sure so i give you a vector a b how do you going to express it as a \nlinear combination of these two vectors so you will do it this way right how will you \nfind that values are the x one and x two so other linear system of linear equations right \nrefer slide time threenine \n \nso this is what you will do i know all are good in doing this and what do we actually do \nwhen we do this what is the algorithm that we use how do we solve this what is the \nalgorithm that you use solving this \nstudent gaussian elimination \ngaussian elimination right in two variables of course we do not call it an algorithm that is \nwhat we did in eight standard or something but when we come to engineering we call it \ngaussian elimination right so the same algorithm \nrefer slide time threetwentynine \n \nso in general given a set of linearly independent vectors we can express any vector that \nbelonging to rn as a linear combination of these vectors right i can say z is equal to \nalpha one u one plus alpha two u two and so on given alpha one  to alpha n are linearly independent  \nok so that means any vector in rn can be expressed using these vectors which form the \nbasis of rn does that make sense that is why call the basis vector because anything else \nthese are the fundamental vectors using  these anything else can be expressed i n that \nspace it is that clear \nso this is how it will be how do i write this in  matrix notation a there are lot of these \nand these thing i do not really understand what you mean  by that yeah good so this is \nwhat you mean so that we writing same in matrix notation and now this is again a dash \na system of linear equation there was a lot of space to fill and one dash good so system \nof linear equation and again you can solve them using \nstudent gaussian elimination \ngaussian elimination what is the complexity of gaussian elimination let us see options \nright n n square n cube fl n cube right the gaussian elimination the complexity is o n \ncube right and i am not doing all this just to the sake of time pass i h ave a point of \nmake which i will make on the next slide right so now this was for any basis that \nmeans if you have any n linear independent vectors \nnow i will consider a special basis where instead of n linearly independent vectors in \naddition thes e vectors are also orthogonal ok orthogonal vectors are linearly \nindependent ok so a set of orthogonal vectors are linearly independent but the converse \nis not all this right \nrefer slide time fivetwelve \n \nso now let us see what if we have an  orthonormal basis that means a basis consisting \nof orthonormal vectors so orthonormal is  combination of two words ortho means the two \nvectors are orthogonal and normal means all the vectors are unit vectors that means i \nam normalized them by their magnitude \nso what is the condition that holds ui transpose uj is equal to zero if i is not equal to j  and \nui transpose ui is equal to one ok now what happens in this special case so we have this \nagain we can express any vectors z as a linear combination of these  now let me try to \ndo this i am just pre multiplying this equation by u one transpose what happens on the \nright hand side everything disappears all of the this terms will disappear because they \nare of the form ui transpose uj where i\u2019s not equal to j and the first term is \nstudent one \none so what remains alpha one  so you can directly find alpha one using a dot product of two \nvectors what is the co mplexity of this operation n th  is just n products ok now how \nmany such alphas do we need to find \nstudent refer time sixnine \nn of those  so what is the complexity  n square so that is now you see why an \northonormal basis is a very convenient basis you can get all these coefficients just by \ndoing a dot product between two vectors  and later on i will show you that you might not \nneed all of these you might just need some subset k of these right  so that means you \njust do k of these dot products and get these values so do you now understand the \nmeaning of what is why why do i say it is orthonormal basis is the most c onvenient \nbasis that you can hope for right \nso the another way of looking it right at it is again just to make few more comfortable \nwith vectors and projections and so on right so this was your original point z one  z \nwhich is a comma b right and how do you actually draw that vector  that this is a and \nthis is b ok so how do you find the coordinates actually you projects on to your basis \nvectors which were these x and y vectors that is how you found the  components along \nthose the coefficient along those \nnow instead of this x and y i have any other  set of vectors which is u one and u two and i \nwill do the same thing i will project this on to uone ok i will project this on to u two and that \nprojection will give me alpha one and alpha two right so now what is alpha one and that sense \nthis is z this is alpha one and this is theta right so alpha one is equal to z into cos theta  ok \nand what this cos theta so again you arrive at the same thing fine \nso essentially taking a projection of a  vector on to your basis is  this fine to everyone  \nthere is just to difference arriving at the same formula that alpha is are given by a dot \nproduct between the basis vector and your original vector \nrefer slide time eighteight \n \nso an orthogonal basis is the more convenient basis t hat you can hope for that is the \npoint which i wanted to have you are convinced about that \nrefer slide time eightsixteen \n \nnow but what does any of this have to do with eigenvectors i started off with \neigenvectors i proved one property there and then i c ame to this linear algebra basic \ndefinitions and what a basis is set of linear independent vectors and i eventually showed \nyou that an orthonormal basis is the most convenient basis that you can hope so what \ndoes any of this have to do with eigenvector  \nstudent refer time eightfifty \nalways for us square symmetric matrix right why do you care about square symmetric \nmatrix not sure yet so we get to that so first of all it is turns out the eigenvectors can \nform a basis and this is for any matrix so  the eigenvectors of a matrix having distinct \neigenvalues are linearly independent \nso does every matrix if i have an n cross n matrix will it have n eigenvectors no it \ncan have less than or equal to eigenvector depending on the refer time ninefifteen so what \nis this saying is that if these eigenvectors are having distinct eigenvalues ok then these \neigenvectors would be linearly independent fine  ok and turns out that for a square \nsymmetric matrix that is the even more special the eigenvector of a squa re symmetric \nmatrix are \nstudent orthogonal \northogonal right and  we already know that orthogonal is good right so remember \nwhen we have orthogonal we do not really care about orthonormal because that is it is a \nsimple operation if you have a set of  vectors u one u two u three which are orthogonal you can \njust divide them by the magnitudes and just get a set of orthonormal vectors right so \northogonal and orthonormal i will use it interchangeably ok and whatever i done thus \nthey form a very convenient b asis so the eigenvectors of a square symmetric matrix  \nform a very convenient basis \nrefer slide time ninefiftyone \n \nso that is how i connect the parts which was about the eigenvectors to the second part \nwhich was about basis  and why would we want to do th is and we already we had a \ncoordinate axis that is the very good basis one zero zero zero zero one zero zero one and n dimension similarly \nso why should i want to use the different basis i have said that eigenvectors is a very \nconvenient basis but why do i care about it i al ready have a very very convenient basis \nwhich is just these one or two vectors are along these directions right so why do i care \nabout a different basis i understand that i that is there somewhere but  something more \nthan that that is one advantage which i  will talk about what else more interesting ok \nin what sense i love the power which comes with my job right that you give a right \nanswer and still i can embarrass you know so that is correct actually"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.3 Eigenvalue Decompositon.wav", "duration": 544.96, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sixthree \neigenvalue decomposition \nlecture \u2013 six \nin this module  we will study eigenvalue decomposition  so the answer  to that was \nactually which i was hoping all of you  will give because all of you have  done two \nprerequisite whic h is linear algebra a nd mac hine learning both of the m teac h you \nprincipal component analysis so i was hoping that you will give that answer \nnow can you give that answer he already  of course  gave  that  answer  is that make \nsense so we relate it to that so but before  going to principle component analysis we \nlook at eigen value decomposition \nrefer slide time zerofortyfour \nthis is very straightforward so let u one to un be the eigenvectors of a matrix a and let \nlambda one to lambda n be the corresponding eigenvalues \nnow i am going to construct a matrix u such that the columns of u are these vectors uone \nto un is that fine what u looks like and now i am going to do this product i am taking a \nthe product of the matrix a with the product of with the matrix u where u is this right \nit is the all the  eigen vectors tagged one after the other is this fine the next step i am \njust pushing the matrix inside  if you know the four different ways of multiplying a matrix \nyou will know that this is correct  or else for now just thing that you ca n just push the \nmatrix inside \nnow what is this i can replace them by the lambda one u one lambda two because a u one is \nequal to lamb da one u one by definition ok now can you write this again as a product of two \nmatrices one is of course the matrix u and the other is \nstudent diagonal \ndiagonal so the diagonal matrix will come first or the matrix u will come first how \nmany if you say u  will come first how many if you say the diagonal matrix will come \nfirst the sum is never one ok \nso it is going to be like this ok and you can write this as u lambda so u is again the \nvector the matrix containing the eigenvectors of a and lambda is a diagonal matrix \nwhere every diagonally element is a corresponding eigen value \nrefer slide time twofourteen \n \nnow this is what we have so far a into u is equal to u into lambda now suppose u \ninverse exists i will assume that u inverse exists and later on  i will tell you under what \nconditions it exists then i could write it as this any of these two forms in one case i am \npost multiplying by u inverse in the other case i am pre multiplying with u inverse ok \nso this is known as the eigenvalue decomposition  of a matrix and the other way of \nwriting it is known as diagonalization of the matrix right you take a matrix apply some \noperations to it so that the result is a diagonal matrix is this clear to all of you is very \nstraight forward ok and again eigen v ectors play an important role in this now  the \nimportant question is under what conditions would u inverse exist u inverse would \nexist if the columns of the matrix u are \nstudent linearly independent \nlinearly independent ok do we know the columns of t he matrix are linearly \nindependent \nstudent yes \nyes because it is a \nstudent refer time threeten \nset of eigenvectors and we already saw the proof that the eigen vectors are linearly \nindependent ok this just follows whatever i say ok now do we need p roof for this i  \nslide nineteen we did this i did not realize it fine \nrefer slide time threethirtyfour \n \n now if a is symmetric the situation is always more convenient why is it \nstudent refer time threeforty \nwhat would u be \nstudent orthogonal matrix \nwhat is an orthogonal matrix actually \nstudent refer time threefortyseven \nso the eigenvectors are orthogonal so we have this situation right  suppose i want to \ndo u transpose u ok this is how that operation would look like ok now what is the \nij\u2019th entry of the resultant matrix  \nstudent dot product \nit is the dot product between the \nstudent ui and uj \n ui and uj everyone gets this right the ijth entry of this product is going to be the dot \nproduct between ui and uj this dot product would be dash if i is not equal to zero or j \nstudent j \nj and there is no point in this  so each cell of the matrix q ij is given by the dot product \nand it is going to be zero if i not equal to j and it is going to be one if i is equal to j ok so u \ntranspose u is equal to the identity matrix that means u transpose is the dash of u \nstudent refer time fourfortyfive \ntranspose of u and of course inverse also ok so u transpose is the inverse of u and it \nis very convenient to calculate what is the complexity of inverse so now you appreciate \nthat that is a that has high complexity and in this case if the vector if the matrix is \northogonal that means it is a collection of orthogonal vectors and the inver se just comes \nfor free right \nrefer slide time fiveeight \n \nso now given this situation and  do not read the hint as if this is going to help but  yeah \nwhat can you now say about the sequence the same sequence that you saw earlier so i \nhave given you that the evd of a is equal to u sigma u transpose  where u is the \ncollection of the eigenvect ors and sigma is the eigen values the diagonal matrix \ncontaining the eigenvalues \nnow what given this and ignoring the knowledge of the first section of this lecture can \nyou tell me something about this series what would be the nth element of the series \nstudent u sigma power n \nu sigma \nstudent power n \npower n \nstudent u transpose \nu transpose and you arrive at the same conclusion right where i was talking about this \noperation right so if we can say something about this matrix then we can say something \nabout this series what can you say about this matrix  if the largest eigenvalue is greater \nthan one  as you keep raising it is power that value is going to explode and hence the \nentire product is going to explode less than one that product is going to vanish and \neverything else would be less than that right remember is the dominant eigen value \nso everything would be less than that so that product will vanish ok so the same \nconclusions you can arrive at right so that is why i want to do these sec tions again so \nyou would have done these in linear algebra but you would have not arrived at these \nconclusions from a very different interpretation but i want to focus on the interpretations \nthat i care about i do not how many of you have seen this s eries in the course on linear \nalgebra you have ok but i do not see why anyone else would teach this is not required \nis only required for some things that i want to do in the course right that is why i \nwanted to do this section \nso everyone is comfortable with eigenvalue decomposition it is a very simple stuff right \ni mean there is no proof or anything involved there we just use some properties of \neigenvectors and eigenvalues and do it \nrefer slide time sixfiftynine \n \nnow there is one more important property of eigenvectors which well use today so let \nus see what this means right you have a matrix a which is an n cross n matrix ok and \nyour import interested in computing this value x transpose a x where x belongs to rn  x \nbelongs to rn \nso what am i tr ying to do here of all these vectors possible in rn i want that vector \nwhich maximizes this quantity what is this quantity scalar vector matrix tensor \nstudent scalar \nscalar ok such that x is equal toone this is the problem that i have been given to  solve \nwhy it is not clear as of now but suppose this is a problem i am trying to solve or the \ninverse of this which is minimize the same thing of all the vectors in rn find the vector \nwhich minimizes this quantity subject to these constraints then th e solution for this is \ngiven by the smallest or largest the solution is the smallest eigen value of a \nand x is the eigenvector corresponding to that so if you are trying to minimize and the \nsolution is a smallest eigenvalue we need to clarify that if y ou are trying to maximize \nand the solution is the largest eigenvalue is that clear and the value of x would be the \ncorresponding eigen value so largest eigen vector is the same as  something that we \nhave defined today dominant eigen vector right \nso let me just repeat so that there is no confusion let us focus on this problem the \nsolution to this problem that is the x which will give me the maximum which will \nmaximize this is the dominant eigen vector of the matrix a right is that clear fine ok \nand if you want to minimize it  is going to be the smallest eigen vector that means the \ninverse of the dominant \nrefer slide time eightfortytwo \n \nso there is a proof for that i will not go over the proof you can take a look at it at your \nown leisure \nrefer slide time eightfortysix \n \nso what has been the story so far the story has been that the eigenvectors \ncorresponding to different eigen values are linearly independent \nif you are dealing with the square symmetric matrix which is something that we will \ndeal with  soon then things are even more convenient because the eigen vectors are \nactually orthogonal ok and they form a very convenient basis and now we are going to \nput this to use when we talk about principal component"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.4 Principal Component Analysis and its Interpretations.wav", "duration": 1501.16, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  sixfour \nlecture \u2013 six \nprincipal component analysis and it is interpretations \nso in this module we wil l talk about principle c omponent analysis an d it is different \ninterpretations in this model we will look at one interpretation and then in the rest of the \nmodule some other interpretations \nrefer slide time zerotwentyfive \nso the story i add is going to be this we will talk about pca and it is interpretations ok \nrefer slide time zerothirty \n \nso now let us try to motivate pca first  consider the following data ok in what \ndimension is this data \nstudent two dimension \ntwo dimensions it is r two ok and each point here is represented as it i s x coordinate and \nusing it is x coordinate and it is y coordinate ok now it means that were using x and y as \nthe basis right that is clear that is the standard way that you would do any data point \nyou will just represent using that basis \nnow what if w e choose a different basis let me give you one basis and then let me ask \nyou some questions on this \nrefer slide time onefive \n \nsuppose we chose this basis so in the previous  modules we made a case for the x and y \ncoordinate axis there is nothing sacros anct about it you could use any basis  the only \ncondition on the basis that the vector should be linearly independent and in fact if they \nare orthogonal it is even better right \nso now i have given you a different basis  now what do you make any observatio n here \nso they have all the points here have a very small component along the u two axis right \nso now this so far this point right if i consider at this point then this is the component \nalong the u one axis so that is it is u one coordinate as akin to the  x coordinate and this is it \nis u two coordinate akin to the y coordinate is a are the arrows clear here \nso that means there u two coordinate is very small and it is also very small for all the data \npoints right so it is almost as if there is some noise  there it is all within some  epsilon \nnow so it seems that the data which were actually represented in r two can actually be \nrepresented in r one by getting rid of this noisy dimension right so if you had chosen a \ndifferent basis you realize that with just one dimension you could have captured \neverything that was there in the data and the other dimension was just adding noise it \nwas redundant there is hardly any information there \nrefer slide time twotwentyfour \n \nso now can you state this more formally because t his is this intuition but can  you stated \nmore formally in terms of things that you have learned and say probability for it for \nexample what is wrong with the direction u two the spread of the data points along the u \ntwo direction is very small what is the spread mean the variance right so we do not care \nabout u two because the variance in the data along this direction is very very small ok and \nin particular right if i were to build a classifier then would u two have any predictive \npower because along this  dimension the points are indistinguishable  ok so think of it \nthat you are trying to find out  whether you have so you have say one hundred candidates and \nyou want to decide whether they would be good basketball players or not \nand quite naturally all the peopl e that have shown up are say six foot two and six foot three inch \nand so on and there in a very small height  difference between them and all of them are \nsixtwo is the average and very close the spread is not much so this feature is not going to \nhelp you decide wheth er this person is going to be a good basketball player or not you \nwill have to rely on other features where the variance is more for example how many \nteams has he participated in the past how many matches as he won as a team as a \nmember of some team and so on it \nso those who expect some spread to be there all these one hundred candidates might have \ndifferent things right but if the height is the same for all of them it is not going to be a \ngood predictor and that is exactly what is happening along the u two di rection the points \nare almost indistinguishable there that is why it does not matter \nrefer slide time threefiftyfive \n \nso in general given any data now this was a simple case where the data was r two i am \ntalking about the general case where the data is rn ri ght and you will find this situation \nin higher dimensions also so you would not want to use that entire n dimensional data \nwhere you know that there are some columns along with the variance is very small so \nyou want to represent the data with fewer d imensions such that the data has high \nvariance along those dimensions \nnow let me just clear a confusion here right so i am not saying that take your n \ndimensional data ok find the variance across each of these dimensions and then throw \naway the columns which have the lowest dimension in this particular example if you had \ndone this what would happen could you have done that think of the original \ndimensions x and y along  these two dimensions there is enough various in the data \nright the x coordi nates vary from here  to here and the y coordinates also vary from \nthis point right up to that point right so there is enough spread in the x and y \ncoordinates \nso in your original data i am not saying that pick look at each column and see if there is \nno variance along that column then throw it away that would not work because you \nmight end up with the situation that there is enough variance across each of these \ndimensions it is just that when you look at the data from a different angle that means \nyou projected onto a different basis this becomes clear right \nso you see the difference i that is not the same these two things are different operations \nso what i am looking at is projecting the data to a different basis that is exactly what i \ndid with u one and u two and then some things became clear about the data now this \nprojection along a different basis i would be interested in doing that only if i can get rid \nof the number of dimensions right if now i had already had one basis where i had n \ndimensions now if the new basis is also going to be that all these new n dimensions that \ni have come up with are important then you are not gaining much i do still have this \nhigh dimensional data but you would like to project it in a way that you get rid of the \nlower variance dimensions \nso you might project it onto n dimensions but you want to rank these dimensions \naccording to variance and then throw away some of these dimensions is that clear is the \nobjective clear ok  fine is that all that we care ab out n dimensions\u2019 project to a new \nbasis and throw away the key dimensions which have less variance is that all what else \nwould you want people have done the mlpr course no i would not so i am not going \nto classification or anything i just want a be tter representation of the data at this point i \nam not really thinking about what i want to do with the data maybe you are talking in \nterms of classification and we have already seen even if the data is not linearly \nseparable we have solutions for deali ng with right so that is not a critical point ok  so \nthere is something else that very interested in and let us look at that \nrefer slide time sixfortynine \n \nnow consider this data i have three dimensional data ok do you find something odd about \nthis data \nstudent refer time sixfiftysix \ny and z are \nstudent refer time sevenzero \nare highly dash \nstudent correlated \ncorrelated right do you want these dimensions can you think of any practice such \ndimensions occurring height in centimeter and height in inches someone would have \njust given you data right or if you if you take the credit card a credit card fraud \ndetection case right someone would give you the salary and it would also give you the \nincome tax now these two are highly correlated right \nso then you do not really care if you have one you could probably almost with certainty \npredict the other right modulus some rules right because you get some tax exemptions \nand all that but still so you can have this in practice but even in our oil mining case \nyour salinity pressure density those things could be related right so z is not adding any \nnew information beyond what y is happening so the two columns are highly correlated  \nso actually yeah this is the formula for correlation all of you know this anyon e who \ndoes not know this formula good so not nothing is a stupid question right so you can \nalways ask \nso y hat is the mean of this column  ok sorry y bar z bar is the mean of this column and \nthis is how you compute correlation this is just the formu la ok so from every entry you \nsubtract the mean ok so this is known as centering the data so if you do this what \nwould the mean of the new data be \nstudent zero \nzero right so that is why it is called centering the data ok so i will have zero mean zero mean  \nand you so what does this what is the intuition behind this formula does anyone \nknow can anyone tell me so this is a summation ok so this quantity is going to be \nhigh if the summation is high it is a summation of some n terms now these terms cou ld \nbe positive or negative if all the terms are positive what would we happen to the sum \nstudent refer time eightfifty \nit would be high if there are some terms which are negative it would be low now when \nwould all the terms be positive whenever y is abo ve the mean z is also above the mean \nright therefore this quantity is positive this quantity is positive whenever both are \nbelow the mean again the product would be  positive when one is above the mean the \nother is below the mean then there is somethin g wrong happening right and in that case \nyou will have a negative term right \nso for more details of course you can refer your other textbooks and so on but this is \njust the intuition  an important step here is to zero mean the data right we are computing  \nthe subtracting the mean of the data  ok another way of saying this is that the column z \nis actually linearly dependent on y ok it is almost linearly dependent i of course have \nsome noise twoone zeroseventysix and so on but it is largely linearly dependent i can get i can write z \nas some c times x fine ok \nrefer slide time ninefortyeight \n \nso now can you tell me the refined goals that we have we are interesting the \nrepresenting data using fewer dimensions such that remember that when i say fewer \ndimensions i mean a  new set of dimensions right it is not throwing away dimensions \nfrom the current data we are looking for a new set  of dimensions what are the \nconditions that we want from these new set of dimensions \nstudent refer time tentwelve \none there should be high variance along these dimensions the new dimensions and \nstudent refer time tenfifteen \nthe dependence are linearly independent or uncorrelated ok fine \nand even better of course if they are orthogonal why \nstudent refer time tentwentysix \nbecause we are looking for a new dash \nstudent basis \nbasis and the most convenient basis is \nstudent orthogonal basis \northogonal basis ok fine \nrefer slide time tenthirtythree \n \nso now let us assume someone has given us this new basis ok and let us call this p one p \ntwo pm so instead of this x y z and so on someone has given us this new basis eventually \nwe will of course figure out how to find the basis but let us assume that someone has \ngiven this new basis right and they are both linearly independent and actually it is \nredundant actually so yeah this example of a redundant feature such an  orthogonal \nvectors is sufficient they are linearly independent \nlet p be an n cross n matrix such that p one p two p n are the columns of p  right same thing \nas we had put the eigen vec tors in a column and probably i have unknowingly given out \nthe solution but  ok and let x one to xm be the m data points given to us ok so we are \ngiven this data as usual we have this x matrix  each one of them belongs to rn and we \nhave m such data points  right that is the standard thing that we are operating and you \nalways write this as a matrix  ok and we have already done the data is zero mean and unit \nvariance \nactually unit variance is not required but  the data is zero mean fine that we will  sorry i am \ngoing to deal with covariance as a unit variance is not required so the data is zero mean is \nwhat i am going to assume but what if the data is nonzero mean i can always make it zero \nmean right \nso just to remember this is an important trick that you will alwa ys have to use whenever \nyou are doing any large scale machine learning so whe never you are participating in \nkaggle competitions almost the first thing that you do is standardize the data that means \nmake it zero mean and unit variance so why is that important \nstudent scaling \nright scaling issues would not be there right so if i have something in centimeters and \nsome other unit in kilometers right now remember that always you are doing \nsomewhere this linear operation w transpose x you might add a no nlinearity on top of \nthat but now if your xi dimensions some of them are in the range of zero to ten thousand some of \nthem are in the range zero to ten right \nthen there is some abnormality here right some dimensions are winnings in terms of \ntheir magnitude and some dimensions are losing out right that is why you always make \nit unit variance and you also make it zero mean you center the data ok so we will assume \nthis and if we all understand if the data is already not zero mean and unit variance we can \nalways make it zero mean and unit variance just scale it and make it centered \nrefer slide time twelvefifty \n \nnow we want to represent each xi right so xi is one of these data points that we had \nthat means one of the rows of our matrix ok and you want to write it as a lin ear \ncombination of this new basis \nso if you have any basis any vector you can write it as a linear combination of that basis \n is it fine so far it is ok ok now for an orthogonal basis we know that we can compute \nthese alphas just by taking a dot prod uct of the vector with the dimension ok and just \nrepeating some of the things right \nrefer slide time thirteentwentyfive \n \nso now let us see  what this means for one of the dimensions this is my data point xi \nwhich i want to transform ok for one of the dimensions i just had to take the dot product \nwith that dimension and this will give me how many values  one value that means the \ncoordinate along p one i want to do it for all the n of them i can write it as this  vector \nmatrix multiplication right what is the dimension of this \nn cross one how many if you get that ok so this oh not many why \nstudent refer time thirteenfiftyfive \none cross n fine that is fine yeah how many of you get this  ok fine yeah so this will give \nme all the n alphas is that clear for this data point \nso it will give me alpha i one to alpha i is it \nrefer slide time fourteeneleven \n \nnow i want to do this for the entire data right so i have done it for x one i also want it to \nbe done for x two  and all the way up to x m for each of these i would have such an \noperation where i have a vector multiplied by this matrix if i just stack all these vectors \ni get back my matrix x and the whole operation i can write as x into p  ok is that clear to \neveryone ok what is the dimension of x into p \nstudent m cross n \nm cross \nstudent n \nn right so for all the m data points i have alpha oneto alpha n is that clear anyone who does \nnot understand this \nso x hat is the matrix of the transformed points is that clear i have now the new \ncoordinates instead of the original coordina tes according to the coordinate axis i have \nthe new coordinates in this matrix \nrefer slide time fifteennine \n \nnow i will just go through some very simple  theorems or rather results and i will not \nprove them you can prove them on your own or other proof is  there in the slides we can \nlook at it later on right so if x is a matrix such that it columns have zero mean and if x \nhat is equal to xp then the columns of x hat will also  have zero mean is this obvious to \nmost of you not really is it how many of you thi nk it is obvious ok then let me just go \nover the proof \nso for any matrix a one transpose a right so that means you have this vector  this is a \nvector or a matrix  yeah this is a  vector right so i have a vector of n one so one  this is \nnothing but a vector of n ones so what is this product actually going to give me \nit will give me a vector containing n elements what is each element \nstudent sum of that column \nsum of that column right is this fine ok this is very obvious to see from if i have this \nsuppose i have two three one and three six seven ok and then of course the corresponding \nrefer slide time sixteentwentyone \n \nso if i do this multiplication i will get a two dimensional output which would be just  seven and \nsixteen right so that is just the sum of that column \nstudent refer time sixteenthirtythree \nrefer slide time sixteenthirtysix \n \nso now we have this x hat that is the transform matrix now let us see if i do this \noperation i x hat what happen i can write it as this i can club it as this what is this it \nwill be all zeros because the origina l matrix was mean zero that means the of the elements of \nall the columns each column independently was zero that what this is going to be a zero \nvector so zero vector multiplied by any matrix is going to be zero now is it obvious i \nhope this is obvious x transpose x is a symmetric matrix i still have the proof for that \nrefer slide time seventeensix \n \nnow if x is a matrix whose columns are zero mean then a matrix sigma which i am going \nto call as a covariance matrix which is given by this is actually the covarian ce matrix \nhow many of you agree with this how many of you have seen the covariance matrix \nbefore so all of you agree that this is the covariance matrix if you do not please raise \nyour hands if you do not you will not understand the rest of the stuff now you have to be \ngiven the right incentives \nso let us see be the covariance matrix of x now what is the covariance matrix actually \nfirst of all tell me that if i say that i have an n cross n matrix x \nrefer slide time seventeenfortyfive \n \nlet me not make it any  cross n let me make it m cross n ok what does the covariance \nmatrix actually capture what is the dimension of the covariance matrix first of all \nstudent n cross n \nn cross n ok and what does each entry of the covariance matrix capture the covariance  \nbetween the i\u2019th column and the j\u2019th column   \nstudent refer time eighteennine \nso the entry ij of the covariance matrix captures the covariance between the i\u2019th column \nand the j \u2019th column is that fine now what is the formula for covariance suppose i give  \nyou two columns right let us see i have give you x one one x one two x one three  and x two one x two two x two three can \nyou give me a formula and of course i will go up to k or rather m \nso what is the formula summation \nstudent refer time eighteenfiftytwo \ni equal to one to \nstudent m \nm \nstudent refer time eighteenfiftytwo \nmu one \nstudent refer time eighteenfiftythree \nmu two anything missing \nstudent by m \nby m anything else in the denominator no no is it fine ok so an what is mu one mu one \nis just an average of this ok so this is the covariance formu la now if the mu \u2019s are zero \nthen what does this boil down to \nstudent refer time nineteenfourteen \n x one i into x two i  what is this quantity actually \nstudent refer time nineteentwentytwo \nthis is the dot product between the i\u2019th column and the j \u2019th column fine ok now that is \npretty much the explanation right so now the c ij \u2019th entry is supposed to be given by \nthis formula if the means are zero you are just left with this formula and this is nothing \nbut the dot product between the i\u2019th row and the j\u2019th i mean the i\u2019th column and the j\u2019th \ncolumn is that fine \nand now if you write it as a matrix then you can just say that it is the ij \u2019th entry of the x \ntranspose x matrix everyone gets this no one has any confusion  the people who raised \ntheir hands fine good \nrefer slide time twentynine \n \nso we now this is where the we are so far that we have assumed that someone has given \nus these dimensions\u2019 p one to pn which we have put in the matrix p right and we have \nalso made a case that x into p which is what i have written here actu ally is just a \nprojection of the original data onto this new basis right everyone gets that ok and i am \ncalling that new projection or the new result that i get as x hat so that is what my \ntransform data is \nwhat is missing here \nstudent refer time twentyfortytwo \nwe do not know what p is that i am assuming someone has given me that p now i need \nto figure out what is the p here  now using the previous definition we get that this is the \ncovariance matrix of the transform data  so let us just write that th is is fine this is fine \nwhat is this \nstudent refer time twentyonethree \ncovariance matrix of the original data ok  so i will just write it as sigma fine ok now \neach cell ij of the covariance matrix towards the covariance between columns i and j of \nx hat where x hat is the transformed data what is the property that you want to hold i \ngive you two conditions or i will give you only one condition for now when i is not equal to \nj \nstudent refer time twentyonetwentyeight \nzero ok so what should the covariance matrix look like \nstudent refer time twentyonethirtyseven \nremember that this is what is this this is the covariance matrix of the transformed data \nright that is what i started with right this is the covariance matrix of this transformed \ndata what do i want this covariance matrix to look like \nstudent diagonal matrix \na diagonal matrix ok because i want every  non diagonal element to be zero right  and this \npoint i am not telling you what i want the diagonal elements to be i am just telling you i \ndo not want them to be zero \nwell if it is zero what would that mean \nstudent refer time twentytwofive \nthat is the variance right if you take the along a diagonal what you get is the variance it \nis if it is not clear right now well return back to that right now we just know that the off \ndiagonal elements are the c ovariance between the i\u2019th and j\u2019th column and we want that \nto be zero so we want this condition to hold  this is something very new that you have \nnever seen in this course before they have actually not seen in this course before  have \nyou seen this or not \nstudent refer time twentytwothree \nthank god fine \nso what is this \nstudent diagonalization \nthe diagonalization of which matrix this matrix right and what was this matrix it was \nx transpose x this is clear so what is the solution all rows always lead to \nstudent eigenvectors \n eigenvectors right \nrefer slide time twentytwofiftyone \n \nso we want p transpose sigma p to be a diagonal matrix and we know which are the set \nof vectors which i put in p such that they will diagonalize sigma \nstudent eigenvectors \neigenvectors of \nstudent refer time twentythreesix \nx transpose x right ok wait why did i put this it is the matrix of the eigenvectors right \nso it is a matrix of the eigenvectors of x transpose x \nso now have we finished it do we know principal  component analysis now so we \nstarted with the intuition that we wanted to transform the data ok i cannot stress enough \nthat we want to transform the data not chopped off dimensions from the existing data ok \nthat means we need to project the data to a  new basis and we had a couple of conditions \nthe variance should be high and the covariance should be zero we have satisfied one \ncondition which is the covariance is zero and we arrive at a solution which says that the \neigenvectors forms the basis that you sho uld project on so that the covariance would be \nzero \nso we have a solution we know exactly which basis to use to represent the data ok  so \nthat the covariance condition is satisfied what about the variance did we do anything \nabout the variance \nstudent refer time twentyfourten \nso we will come back to that ok fine \nrefer slide time twentyfoureight \n \nwhy is this a good basis what does the what is a good basis the best basis \nstudent orthogonal \northogonal right because the eigenvalues of x transpose x are line arly independent that \nok and they are also orthogonal because x transpose x is a dash matrix \nstudent refer time twentyfourtwentyfour \nok \ngood real symmetric \nso this method is called the principle component analysis for transforming a data to a \nnew basis and tha t where the dimensions are non redundant because they have low  \ncovariance and not noisy because they have high variance the second part i have not \nproved right and i will get to that at some point fine  no that is what we saw right no \nwhat is i did not get that now in practice how many eigenvectors would you have \nstudent n eigenvector \nn eigenvectors do you want to keep all of them which ones will you throw away \nstudent refer time twentyfourfiftyeight \nthe low variance ok \nand now in the next interpretation ac tually we will try to see what is the  what happens \nwhen you throw away the least important dimensions right what do you mean by the \nleast important dimensions"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.5 PCA \uff1a Interpretation 2.wav", "duration": 987.16, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  sixfive \nlecture \u2013 six \npca interpretation two \nso that is what we look at in the second interpretation of pca right \nrefer slide time zeroseventeen \nso again we have the same setup that given n are linearly independent for n orthogonal \nvectors we can represent x i exactly as a linear combination of these vectors what do i \nmean by exactly perfect ok  if you actually describe the who le things in words ok so \nthat is exactly what i mean right so you are going to write x i as alpha one i into p one plus \nalpha two i into p two and so on and when you do the summation on the lhs on the rhs \nyou just get back the  lhs when you do the summation o n the right hand side yo u get \nback the left hand side \nso that means it can exactly be represented when you use all the n eigenvectors now if \ni start chopping of stuff what will happen \nstudent refer time onefour \nit will just be an approximation ok  now we this is what i meant and this is this the \nequation holds that means this is exact and we know how to find the alpha is because p \njs are conveniently orthonormal so we know how to find that easily ok now what if we \nconsider only the top k dim ensions what is going to happen there is going to be some \nerror in the reconstruction i am not capturing all the information in my original data but \nthere is some error which i am not being able to capture and i made a conscious decision \nthat that error is not important i am willing to let it go  \nhence i want to represent the data using fewer dimensions ok so this is exactly what \nyou do in pca when you take the top k dimensions is this fine ok so now we want to \nselect pi\u2019s such that we minimize th e reconstructed error ok and this is again  erratic \nactually we should try to write it as x i minus x since these are vectors and the square of \nvectors would just meet this right \nrefer slide time twosixteen \n \nso but you get the point right were just tryi ng to do the element wise  squared error loss \nwere trying to minimize that  we want to do this  so now let us try to see that if you are \naiming to do this what is the condition that we arrive at ok so no i thought i would ask \nfor some changes on this  \nfor a minute all of you can you just bear with the fact that these are actually vectors and \nnot scalars so this square actually does not mean anything it actually means  x i minus x \ni hat transpose x i minus x i hat so when i use square with vector s this is what i mean \nis that everyone can work with that notation fine \nrefer slide time threenine \n \nso now what is x i actually the real point right the correct point which can be obtained by \nthe full reconstruction if you consider all the n dimensions wha t is x i hat just an \napproximation where you are considering only the k dimensions remember that each of \nthese quantities is a vector  fine ok now what is happening here let me just  try to say \nthis so let me just do this way so this is your original x and you are actually writing it \nas a linear combination of your  p\u2019s somewhere you will have alpha k pk and then all the \nway up to p n  \nso this is p k alpha n ok now what is this full thing this is x and what is this x hat ok \nyou see the picture what is the equation trying to tell you ok now what is the difference \nbetween these two then these guys right if i want to take difference between x and x hat \neveryone gets that it is the remaining term say that means alpha k plus one into p k plus one \nup to alpha n into p n is that clear \nrefer slide time fourtwentytwo \n \nso can i write it as  yeah can i write it as this ok so you get this right so i am only \ntaking these guys because the rest will get subtracted so one is the full n dimensions the \nother is only  k dimensions so if i take the difference between them what remains is k \nplus one to n dimensions and that is exactly what i have written here ok \nand now i am coming back to the proper  notation where this is a vector right so i am \nwriting the square as the dot product between the same vector is this ok these are the m \ndata point right this sum this is overall the m data points you need to minimize that is \nthat clear ok so this is fine  now want just some rearrangement so i have just \nexpanded out that summation this is what it would look like right i have just expanded \nout these two summations  \nnow let us try to do this in your head and see what are the kind of terms that you get \nthere are two different types of terms that you will get so first of all let us understand that \nwhen you expand this you will end up with a lot of dot products you will get a dot \nproduct between this and this and this and so on right  so can you split those terms into \ntwo different types \nstudent refer time fivethirtyseven \nsquare terms so one where i is equal to j and one where i is not equal to j is that clear  \nfine so let me just write it as that so i will have k plus one to n right that means n \nminus k terms where i would be equal to j right so that means pk plus one w as getting \nmultiplied by k pk plus one pk plus two was getting multiplied by pk plus two and so on and \nthen i will have these remaining terms where i is not equal to z right so these are the \ndot product between the other vectors is it fine you see why i have  split it this way \nwhat will happen now the second term will go to zero ok  and what about the first term \nalpha ij square ok now what is alpha ij actually how did you find alpha ij \nstudent refer time sixtwentyeight \n it is a dot product between we did this ri ght finding any of these components is just \ntaking the dot product between x i and that dimension so x i transpose pj is that fine ok \nis this fine and again this is slight abuse so this is actually what no this is  ok right a \nthis is ok sorry i am ju st going to write it as this is this fine i just written it twice and i \ncan change the order since it is a dot product  \nnow what i am going to do is so this is actually summation over an index i and a \nsummation over an index j and i can change the two  summations i can interchange them \nok so that is what i am  going to do now is this fine i will push the summation all the \nway inside what is this actually this entire thing actually m times covariance of \nstudent refer time seventwentyfive \nso is this i is t his what you are telling me that this is m x transpose x is this fine how \nmany if you do not get this i see a lot of blank faces how many if you do not get this  \nquite a few so this is so i is equal to one to m right so you are going over the data points  \nok so this what is the dimension of this actually \nstudent n cross one  \nn cross one and this is one cross n what does this product give you \nstudent refer time sevenfortynine \nn cross n what are the entries in this matrix so this was say x one one up to x one n and this is \nagain x one one up to x one n ok so that is going to be x one one square or rather let me just write it \nin the generic form right so it is going to be x one  i into x one j right is that fine and how \nmany such matrices are you adding \nstudent refer time seventeenseven \nm of these so what would you get then what would the first let us  so ok so let us do \nthis so the first entry of this matrix is going to be x one one square what about the first \nentry of the next matrix in this series \nstudent refer time eightforty \n x two one x two one square right so this is slightly tricky to demonstrate let me just a give me a \nminute i will just collect my thoughts and do it properly ok let us take a small example \nok so x one one x one two x one three suppose we have a three dimensional matrix three dimensional data so \ni am taking a sum of m s uch matrices ok i equal to one to m that means  this is going to \nvary this indexes the first index is going to vary from one to m now  let us see the first \nmatrix and let us look at the first element of that m atrix the first element of this matrix is \ngoing to be x one one square ok \nnow let us look at the next matrix what is the next matrix going to be it would be x two one \nx two two x two three right and multiplied by x two one x two two x two three what is the first element of this \nmatrix going to be \nstudent refer time ninefortyfive \nx two one square what about the third one x three one square this is fine so far now you are adding \nall these matrices so what is the first element of the resultant matrix going to be x one one \nsquare plus x two one square pl us x three one square what is this actually this is the dot product \nof x one with itself right and what does that give you the variance if the data is zero mean \nright ok now can you make a similar argument of the ij \u2019th entry is going to give you the \ncovariance between the i\u2019th and the j \u2019th entry is that clear right you could do a similar \nanalysis you can actually work it out after going back how many of you have found \ncomfortable with this there is still many who are not ok \nso let us look at an ij \u2019th entry right so can someone help me with say that one comma two \nentry ok or the first matrix what is it going to be x one one into x one two right for the second \nmatrix \nstudent refer time tenfiftytwo \nx no this is some  yeah correct and for the third matrix  three two ok now what is  this sorry \nwhat is the summation of these when you take the full sum you will get these three as as \nwhat is this in this summation tell you \nstudent refer time eleventen \ncovariance between \nstudent first and second \nthe first column and the second column i s that clear now is it  with everyone now ok \nfine so what you have here is actually the covariance matrix  you seems to be lost is it  \nwith you sure  ok fine  \nrefer slide time eleventhirtytwo \n \nso what we have here is something of this form ok \nrefer slide time eleventhirtyseven \n \nso now what we want to do is we want to minimize this quantity subject to the following \ncondition is that ok what is the solution for this if i did not have the summation ok  \nsuppose i just wanted one dimension so i want to minimize say p sig p transpose sigma \np such that p transpose p is equal to one what is the solution for this \nstudent refer time twelveeight \nsmallest eigenvalue of sigma right  and you can show by induction that if you want k \nsuch things that here i am looking for n minus k  such things right then these would be \nthe n minus k smallest eigenvalues of sigma but now i am talking about the smallest \neigenvalues but in the first solution i said we need to pick the largest eigenvalues so \nwhat is the difference \nstudent refer time twelvethirtyfive \nthese are the ones we are throwing away these are the ones along which the error is \ngoing to be minimum if we throw these away the error is going to be minimum so we \nwill throw away the last n minus k dimensions which means well keep the f irst k \ndimensions is that clear so you arrived at the same solution is that  right so that means \nin pca you are actually trying to pick the dimensions in a way such that your \nreconstruction error is minimized and this was exactly what our reconstructio n error \nwas so do not worry about this math bit just see that we started with this quantity this \nis what we wanted to minimize ok  \nand we did some trickery and we came to this formula that minimizing that error is \nequivalent to minimizing this quantity  and for this  we know the solution that the \nsolution is the smallest eigenvalue and we want n minus k such things  that means there \nwould be the n minus k smallest eigenvectors is that clear that means we are going to \nkeep only the k largest eigenvectors ok that means you are going to project your data on \nto k largest eigenvectors \nrefer slide time thirteenthirtyseven \n \nnow so the key idea here is this right minimize the error in reconstructing x i after \nprojecting the data onto the new basis \nrefer slide time thirteenfortythree \n \nso let us take an example and we will work with our toy example again \nrefer slide time thirteenfortyfive \n \nso this was the data that we had and suppose i give you a new basis which is one comma one \nand minus one comma one ok this is a new basis this is an or thonormal basis  orthogonal \nbasis you can see that u one transpose u two is equal to zero ok \nnow i need convert it to an orthonormal basis so i have just divided by the magnitude \nis it ok fine now consider the point threethree comma three this was our original point ac cording \nto which coordinate axis  x comma x that means this was threethree and this was three ok now i \ncan find the alpha is right because this is an orthonormal basis i can directly find the \nalpha is now the perfect reconstruction would be this so actually if i  do this i get back \nthe original point \nnow what would happen if i throw away the second dimension because the second \ndimension had corresponds to a smaller eigenvalue ok i will get this so you see that the \npoint is still close to the original point i have not actually lost much right what has \nhappened is i have actually projected the boy lie point on this line right the line x equal \nto y that is why i get x equal to y and in doing that i am not losing much information \nfrom the original data is this  clear right so you understand what happens when you \nreconstruct the data \nrefer slide time fifteeneighteen \n \nthere is no end to this  ok so just to recap the eigenvectors of a matrix with distinct \neigenvalues are linearly independent and we use this fact con veniently at least in the \ncase of square matrix where the also happen to be orthogonal so we know that they can  \nform a very convenient basis and pca exploits this to find the top k eigenvectors which \nto be retained  \nand while doing this they have seen t hat two things are at least ensured one the \ncovariance between the dimensions is zero because that is exactly how we formulated it and \nfound the solution we saw that it turns out that we need to diagonalize a certain matrix \nand the solution is the eigenvectors \nwe also saw a different interpretation where we saw that it is the same as throwing away \nthe dimensions along which the error would be minimum right and both these \ninterpretations led to the same solution which was project the data onto the eigenvectors \nof the covariance matrix of the original data  ok and this n minus k dimensions current \ncontribute very little to the reconstruction now what is the one thing which i have not \nproved yet what was our wish list \nstudent variance and covariance \nvariance and covariance right high variance low covariance i proved low covariance  i \nhave also proved something with respect to reconstruction error because that is \nsomething i require for auto encoders so just remember this bit about reconstruction \nerror"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.6 (Part-2) PCA \uff1a Interpretation 3 (Contd.).wav", "duration": 90.74, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sixsix \nlecture  six \npca interpretation three \nnow you go to the third interpretation where we will try to say something ab out the \nvariance \nrefer slide time zeroeighteen \nso we started off with the following the wish list that we wanted low covariance and we \nwanted high variance  so far we  have paid attention to the covariance  because \neverything was revolving around this covarian ce matrix in both the solutions  but what \nabout variance have we achieved the goal with respect to high variance \nrefer slide time zeroforty \n \nso let us see so what is the i\u2019th dimension of the transform ed data it is this you take \nyour data and project it onto the i\u2019th dimension right so x hat is equal to x into pi now \nwhat is the variance along this dimension  how do you compute the variance   so this is \nmy projected data and let me just call it x hat i \nso this is the i\u2019th column after projection i s that fine everyone is ok with this now for \nthis i\u2019th column i want to compute the variance  how will i do that remember that the \ndata is zero mean  what is the formula actually it is going to be x hat i minus mu i into \nx hat i minus mu i right but mu i is zero so it just turns out to be the  dot product dot \nproduct of x i hat with itself ok and of course divided by m is this fine \nrefer slide time onethirtyfour \n \ni can write this as x p i and then when i take the transpose i will get this ok now what is \nthis quantity  t his is exactly the moment where  i feel like saying  fl what is this \nquantity \nstudent refer time twozero \nno look at the circle what is x transpose x times p i \nstudent refer time twoeight \nwhat is p i with respect to x transpose x \nstudent eigenvector eigenvector \neigenvector so what is this product going to be \nstudent lambda refer time twofourteen \nlambda i p i is that fine what is p i transpose p i \nstudent one \none ok so what is actually the variance along the i\u2019th dimension \nstudent lambda refer time twotwentysix \nwhat is lambda i \nstudent eigenvalue \nso what will happen if i retain the highest eigenvalues \nstudent refer time twothirtythree \n i will get the highest variance dimensions right fine so all roads lead to \nstudent refer time twothirtynine \neigenvectors eigenvalues right  so andrew ng in one of his lecture says that there are \nten different interpretations of pca i only know three of these i do not know the remaining \nseven maybe he was bluffing so that people like us can keep busy oh this is getting recorded \nso yeah so you get this  so we have satisfied everything in our wish list variance  \ncovariance and also did this detour where  we saw that it actually amounts to minimizing \nthe error in reconstruction where we are throwing away the dime nsions along which \nreconstruction did not add much to our knowledge about the data  so these are the three \ndifferent interpretations that  i have right so hence we did the right thing by throwing \naway those dimensions  which correspond to the lowest eig envalues because lowest \neigenvalues is nothing the lowest variance also \nrefer slide time threetwentyfour \n \nso this is the quick summary  the covariance between the new dimensions  you can \nleave actually those you can just read it later on"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.6 PCA \uff1a Interpretation 3.wav", "duration": 201.92, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  six  \npca interpretation three contd \nrefer slide time zeroeleven \na quick summary  we have seen three different interpretations of pca and eigenvectors \nplayed a very crucial role in that and the other thing which played a crucial role was the \ncovariance matrix of the original data and with these three different interpretations what \nwe realize is that the solution that we get or the transform data that we get projecting the \noriginal data on to the on  to a basis consisting of eigen vectors ensures that there is high \nvariance across the new dimensions \nand we can ignore  of the bottom top  n sorry bottom n minus k dimensions along with \nthese variance is not high this also ensures that the error in reconstructing the data by \nignoring this dimensions is minimized right it is a lowest possible error  and it also \nensures that the covariance between your retained dimensions is zero because we are able \nto diagonalize the covariance matrix of the transformed data so that is what we had \nso now if you think of it right just to connect it two things that we need later on for auto \nencoder right we are trying to learn a new representation for the data right and we are \ntrying to also compress the data  and we want this compression to be such that it  is as \nlossless as possible right we are going from n dimensions to k dimensions  and still we \nwant to  retain the essence of the data and  do not wan t to lose out  much of the \ninformation in the data ok so that is essentially what pca is doing now let us see this \nin practice"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.7 PCA \uff1a Practical Example.wav", "duration": 691.4, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  sixseven \npca practical example \nlecture \u2013 six \nso we will in this module we will look at practica l example where pc a is used and i \njust like to give you a flavor of why all this is important right why do we need to throw \naway some dimensions and then how does it practically help \nrefer slide time zerotwentysix \nso consider that we are given a large number of human images right  so this is like \nsome faces data set  a database that says  one of the intelligent agency someone is \nmaintaining one of the governmen t agency or may be  aadhar data bases or something \nlike that ok  now each image here is  one hundred cross one hundred that means it is ten k dimensions \nright it is a very high dimensional data ok and your job is to actually store this on to do \nsome database for a large amount of the population right because you are collecting these \nimages from various people \nso now we would like to represent and store this data using much fewer dimensions \nright and you would be really ambitious that if you want to store that more than  fifty to \ntwo hundred dimensions right so you see the compression that  i am looking at you have ten k \nwhich is a big storage problem for me and i want to just bringing out to fifty to two hundred but i \nhave know that this is crucial data right  i do not want to store information which is not \nable to dis tinguish these faces  i was still be able to  reconstruct the faces from this \ninformation right do well i mean minimum error r econstruction from this and  that is \nexactly what pcas are allowing us to do right so now we construct a matrix of m cross \nten k what is m \nstudent refer time onefortysix \nthe numbers of samples you have the numbers of data point s that we have and each of \nthis is of dimensions ten k ok so this is what matrix what do we call this matrix oh it \nis already given right  it is the x matrix the data matrix that we always  have now each \nrow of the matrix corresponds to one image and e ach image is represented using  ten k \ndimensions just to reiterate \nnow let us see so now  what would you do  this is the original data  i want a \ndimensionally reduced data right  y ou want store this ah  is the mike working you \nwant this data to be represe nted by a  fewer dimensions so what is your solution  do \npca so what will you do x transpose x right and i did not get my slide refer time \ntwofortythree \nrefer slide time twofortyfour \n \nso we retain the top  one hundred dimensions corresponding to the top  one hundred eigenvectors of  x \ntranspose x right so basically we do a pca find the one hundred find all the eigenvectors of x \ntranspose x and then just retain the top one hundred of those now what is the dimension of each \nof these eigenvectors should be straight forward take your time it is early morning \nstudent refer time threeten \nten k right so now can you think of a physical interpretation  of this so what are you \ntrying to do  you are trying to store faces  and now you have come up with these \ndimensions no sorry we have come up with these basis vector which is eigen vectors and \neach of them is also ten k which is as same as dimensions of your faces \ncan you think o f a physical interpretation of  what is happening here  none of went you \nthrough the slide except perhaps you or i do not know just think about it  so what you \nare trying to do is you are trying to represent any possible face in your database  right \nusing a linear combination  of some vectors ok  now these vector should have  some \ninterpretations right it should be connected to faces and somewhere otherwise how will \nyou construct a face from taking a linear combination or some  random vectors do you \nget the point \nso can you think of each of these  ten k dimensional vectors which is the same  as the \ndimension of your original data as a face and try to plot it can you try to do that at least \nit make sense ten k dimensional ok that is the same what you are image size was i could \njust arrange these ten k dimension as one hundred cross one hundred and try to plot it  ok so let us see if \nyou do that what happens  ok we convert eac h eigen vector into  one hundred cross one hundred matrix \nand treat it as an image and let us see what we get \nthis is what we get  so this is the top sixteen eigen vectors that i have plotted now can you \ntell me a physical interpretation  of this this is the basis for constructing  any face in \nyour data base right  that what you are trying to say all the faces that you have in your \ndatabase or in the world you can combine  them by looking at the these  elementary face \nstructures right which are your basis \nand then you could scale them up by using these alphas  you will be multiply them with \nthe certain alpha right and when you combine them you will get the base any face that \nyou had in your original database does a physical interpretation make sense how many \nof you get this  \nrefer slide time fivetwentyeight \n \nso that is what is happening here so we have constructed this basis now i will come to \nthat later  so these images are actually c alled eigen faces and the form of  basis for \nrepresenting any face  in our database  ok in other words  we can now represent a given \nimage as a linear combination of these eigen faces  so this is my original image ok  i \nwant to reconstruct it  now use sixteen or twentyfive of these eigen faces what do you think would \nhappen we will get some face which has s ome error  there is some error  in \nreconstructing this face \nso let us see what we get so i am using only one basis vector and i found out this alpha \none i how would i found it out \nstudent refer time sixeleven \ndot product of the face vector with the basis vector ok now i instead of one i take two you \nsee i have took this  two basis vectors  scaled them with the corresponding alpha \ncoordinates and added it them up right  and i am trying to get some face value it does  \nnot look it goes to the original face  that means the dash is very high  the reconstruction \nerror is very high \nthat means i have still  dropped some of the  important dimensions  i have still drops \nsome of the important eigen vectors right  so the value of  k which is the top  k eigen \nvectors is something that i need to take care of it and should be in a way  i can always \nconstruct the reconstruct i can always compute the reconstruction error here right  how \nwill you compute the reconstruction error \nstudent refer time sevenzero \ntake the square error between the original image and this second image that you see here \nright so you will take the square error between this and this and you will end up with \nthe number which is not acceptable right  now what i will do is i will go further i have \ntaken four ok still not quite there but i can see a shape emerging right \ni go to eight things becomes better since you are already know what the original face at least \nyou can make sense of it  and by the time  i reach sixteen i am almost there right at least i \ncan recognize the face that is probably losing out some subtle things in the face but i can \nrecognize it \nnow how many of you appreciate what is happening here  yeah of course now what is \nhappening here so think in terms of  a practical application right  what have you done  \nwhat have you able to be ac hieve how may basis vectors where you able to store or did \nyou store \nstudent sixteen \nsixteen that means  sixteen into ten k values ok and suppose you had a million images in your \ndatabase how many would you require to store \nstudent refer time eighteleven \nwait let us we do it step wise forget about pca if you had a million image in your data \nbase and each of them have ten k dimensions how much storage do we need \nstudent refer time eighttwentyfour \nmillion into  ten k  floating point values ok  now with if  i say sixteen sixteen  may be  too \nambitious may be later on  i will say fifty or one hundred is ok but let us say sixty then how much \ndata we need to store \nstudent sixteen into ten k \nsixteen into ten k and you can reconstruct any face \nstudent refer time eightfortynine \nyeah alpha\u2019s needs to be stored right  so for every image instead of storing  ten k \ndimensions you will just store the  sixteen alphas right and you can see  that even if  i go \nthere to one hundred and its still manageable instead of ten k i am going to just store one hundred alphas \nright and as i go to one hundred what would happen \nstudent refer time ninenine \nthe reconstruction error would become even lesser ok  so is that is the intuition  clear \nok so this eigen vector storage is a onetime stor age we are going to store this  k eigen \nvectors each of them are  ten k dimensional and k is one hundred or two hundred we do not really care \nbecause the original data was very large right and for each image we just need to store \nthese alpha values k of them right so for each of them instead of ten k we will store \none hundred to two hundred alpha values and of course it is significantly reduces right \nso this is why we need to do all this right and what is the other advan tage of doing this \nanything of something else so what is pca actually allowing you to do if you again \nthink of it not  i mean subtract the math out  just think it in terms of physically \ninterpretation and what is that it is allowing you to do if you had  to say it in english \nwhat is it allowing you to do  compression is a loaded word can you just spell it out  for \nme what is this compression mean actually \nstudent refer time tenfourteen \nright so it is storing all the relevant  information in the image and disca rding all the \nerror element information right  now this also ensures that if you have multiple images \nof the person  then what would happen  you should take the image under lighting \nconditions or may be at person had applied some makeup or something like that right \nwhat would happen \nstudent refer time tenthirtyeight \nin the original space  the ten k dimensional space what would happen to these images  \nthey will be very far from each other right because the lighting conditions have changed \nyou see a dark person so have a light person something like that right and now because \npca has helped you to throw away this dimensions right may be the exact terminology \nwhich i am using may be the lighting condition do not do it  because you can imagine \nthat there would something right that suppose  as some element which is call ing t he \nimage to look slightly different  but that is not the important information right so that \nwould get discarded off and only the relevant information would stay \nso then multiple images of the same person which were dash in the original space would \ncome dash in the new space \nstudent refer time eleventwentytwo \nfar in the original sp ace would come closer in the new sp ace right  so this is what \ncompression helps you to do so this is what you want to learn you want to learn the \nimportant characteristics  of your original data and  that is what pca allows you to do  \nfine"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 6.8 Singular Value Decomposition.wav", "duration": 1519.4, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sixeight \nlecture \u2013 six \nsingular value decomposition \nrefer slide time zeroseventeen \nso with this we will move on to the last topic  in the yeah so that is something that you \nwill have to so the way i would do it right is that  you keep aside some one hundred images from \nyour data as validation data now  once you have learned these eigenvectors try to \ncompute the reconstruction error for t hese one hundred images and just vary it do one hundred one thousand \ntenzero written as many dimensions as you can and see at what point is a reconstruction \nerror ok for  you right and this is assuming that you have some notion of what is a \nreasonable reconstruction error so we all know that the minimum is zero \nbut if you have zerofive then maybe for  face database it might be  but if it is a database \nwhere you are trying to look at  mechanical parts so suppose you are looking at motors \nand rotors from a machine assembly now there you want to be able to distinguish minor \ndetects defects on this and a  detect could a defect could actually just be one single or two \npixels getting different  from the original image right  so there the reconstruction loss \nwould be much needs to be much  more robust you get the point so it depends on your \napplication so you will have to take some validation data either have a domain expert to \ntell you what is reasonable or go by the number that you get  right and this is the \nvalidation error that i get \nso everyone understands the question and perhaps the answers ok so we now go to the \nlast module \nstudent refer time oneforty \nyeah if you can \nstudent refer time onefortyone \nyes you can now project any face into this database a so that is the eigen basis that \nyou have got you have got the basis vectors now any data you can project onto this \nbasis \nstudent refer time onefiftyfour \nnow so if you are trying to learn these eigenvectors by say  using one hundred images  all of \nwhich belonging to a particular  demographic say all caucasian images right and now \nat the runtime you have an asian image then you will have obviously have some error \nright but you have large even of data say if you have if you are constructing this from \nmillion images then it should generalize that is i mean just as for any ma chine learning \nalgorithm \nthe training it from small data and you bring out some outlier at test time  it is not going \nto work right but if you have reasonable data it should generalize  any other questions \nto calculate the eigenvectors x is m cross ten m cross ten k yes now we move on to the \nlast topic for the basic portion and the next class we will do  auto encoders will be back \nto deep neural networks so singular value decomposition right \nrefer slide time twofifty \n \nso this is actually the stuff that i need an important theorem from here at multiple  two \nplaces in the course so now before doing the right  let us get some more perspective \non what eigenvectors do and why are they actually important \nrefer slide time threefive \n \nso let v one to v n be the eigenvectors of a and let l ambda one be the corresponding e igen \nvalues so we know this a v one equal to lambda  v one and so on ok  now suppose all the \nvectors in r  should be r raised to n ok so if a v ector x belonging to rn can be \nrepresented using this basis ok now  what if i am interested in the operation a into x \nwhat is the advantage of representing it using this basis so this is what you are saying \nthe other day \nstudent refer time threefiftyfour \nwhat is ax it is a matrix vector multiplication  right and it is going to be a heavy  \ncomputation now if all my vectors in rn are represented using the eigenvector as the \nbasis what happens to this matrix operation \nstudent refer time foursixteen \nit reduces to \nstudent refer time fourtwentyone \nlet us see so i was interested in ax but i know x is this so you get this step and what \nhappens finally do you have the matrix anywhere here so what happens to the matrix \noperation \nstudent refer time fourfortyone \nit reduces to a sum of scalar operations  right if your vectors were representing using \nthe eigenvector as a basis \nso this is one reason why this is important  right so you can now get away of the  get \nrid of the matrix operations and just do scalar operations right so now there is a catch \nhere which i am going to ignore just to try it if i bring in the catch you guys will get \nconfused so i will ignore if anyone has a doubt maybe talk to me after the class but for \nnow let us go with the fact that the matrix operation reduces to a scalar operation \nrefer slide time fivenineteen \n \nnow so far what we have done is discussed square matrices i have said that they are the \nvillains of linear algebra but who are the super villains of linear algebra  rect angular \nmatrices everyone says that but why  imagine what they do to a vector yeah so can \nrectangular matrices have an eigenvector \nstudent refer time fivefortyfive \nyes obviously yes that i mean any matrix can have an eigenvector \nstudent no \nno why ca n you write so mething of this form you can\u2019 t right because when the \nmatrix operates on an n dimensional vector what does it give you \nstudent refer time sixone \nan m dimensional vector right  hence they are super  villain right because they take \nthe vector from one space and transform it to a completely different space that \ncompletely lots lost it is identity  right so that is  why rectangular matrices are even \nharder \nso now we just saw that for square matrices this eigenvectors form a very conve nient \nbasis where these operations reduce to a scalar operation but now rectangular matrices \ndo not even have eigenvectors so then cannot we have the same advantage there can \nwe have the same advantage there you can\u2019 t right because you do not have an  \neigenvector but i would teach you about singular value  decomposition so i better have \nsomething so get the connection ok there is a problem with square matrices  with the \nrectangular matrices so now let us see so we will try the aim is to see if  we have \nsomething equivalent to this scalar transformation that we had for square matrices \nhow many of you have seen this in linear algebra before so you know whatever i am \ngoing to talk about  fine so the result of ax is a vector belonging to r m an d the \noriginal x belongs to r m  so we do miss it miss out on this advantage that you could \nhave reduced the matrix operation to a scalar operation and now we will try to see if we \ncan still get back that advantage \nrefer slide time seventhirtytwo \n \nso notice  this is matrix you can think of it as a function which provides a \ntransformation from rn to rm so what is the set of inputs  to the matrix it is vectors \nbelonging to rn  that is the set of input \nnow suppose we had a pair of vectors v one u one v two u two vk u k each belonging to these two \ndifferent universes one is rn the other is rm and there was a specular relation between \nthem that a into vi is equal to sigma into  ui suppose i am just being ambitious let us see \nwhether we can actually have this pair but suppose we had this pair then can you \nconnect this back to the discussion on scalar operations  so let us just see that in  detail \nand we will of course assume that these are orthogonal and form a basis so the vi\u2019s \nform a basis in rn and the ui\u2019s  form a basis in rm  is that clear that is all \nstraightforward we have these vectors \nnow every a vector belonging to rn which was the input space can be represented using \na linear combination of v straightforward and any vector belonging to the output s pace \ncan be represented of \nstudent refer time eightfiftyfive \nof u right so that means any x in the input space i can write it as this linear \ncombination and now if i do the matrix operation what happens \nstudent refer time ninenine  \nyou get this a into vi what is a into vi sigma ui i have still not shown you how to find \nthese sigma is ui by the way right ok once again the matrix multiplication reduces to a \nscalar multiplication \nrefer slide time ninethirty \n \nso now let us try to look at a geometric interpretation of this \nrefer slide time ninethirtytwo \n \nso what you have is this original space which is rn you are using a as a matrix \noperation right as a function and you are transforming vectors from n to rm  right so \nthis is the space transfer that i w as saying it vectors are being picked up from rn and \nbeing put into r m ok and  rn is a space of all vectors which can act as inputs to this \nfunction and rm is a space of all vectors which are the outputs of this function ax \nnow we are interested in fi nding a basis u v such that v is the basis for the inputs when \ni say basis all of you should immediately start thinking of dash vectors \nstudent orthonormal vectors   \northonormal vectors orthogonal or orthonormal right  once we have orthogonal we do \nnot care about the rest u is the basis for the outputs such that if the inputs are and \noutputs are represented using this basis then all our matrix operations reduce to scalar \noperations so we are just trying to find the  rectangular analogy for the squa re a \nphenomenon that we observed ok that is what we are trying to do now can you tell me \ni have told you that if such a v and u exists then you could do this can you give me \nsuch a u and v \nso what do we mean by so here i said actually i said this  right that the dimension of \nthe row space is actually k and the dimension of the column space is also k what do you \nmean by the dimension is i mean  right here i am telling this is rn and this is rm and \nnow i am telling you the dimension is k what do i mean by that \nstudent refer time elevenfive \nthe only k linearly independent vectors \nrefer slide time eleveneleven \n \nand this is again something from linear algebra which i expect you to know is that all \npossible vectors in rn only a subspace belonging to rk can actually act as input to a x to \nproduce a nonzero output so i am talking about a null space column space and things \nlike that right so this should be clear if it is not it is  it is not very impo rtant at for us \nright now \nand hence we have only k dimensions \nrefer slide time eleventhirtyeight \n \nso let us look at a different way of writing this so you have this a v one is equal to sigma \none u one av two is equal to sigma two u two so i can again do the same trick that i put all the v \u2019s \ninto one matrix where vi\u2019 s are the columns of this matrix and i will put all the us into \nanother matrix where ui\u2019s are the column of this matrix is that fine everyone ok so far \nand then i can write it as this matrix operation same thing that we did when we are \ndoing eigenvalue decomposition right so we had written it as a into u is equal to u \ninto sigma right because there we had the condition that ax is equal to lambda x now \nwe have a u is equal to sigma v or rather the other way around so av is equal to did i \nmissed up did i no right \nstudent refer time twelvetwentynine \nsorry \nstudent refer time twelvethirtytwo \nfine yeah so av is equal to sigma into u \nso is this fine no but when you do the diagonal operation you will get it as u into sigma \ny the same way as a x equal to la mbda x but when you write it is a into u is equal to  \nlambda comes later on \nrefer slide time twelvefiftysix \n \nand if we have k orthogonal vectors  so remember i said that this basis consists only of \nk dimensions right because that is r the set of vectors whi ch can act as input to a so \nwhat i but i want a basis for the full rn so what do i do for the remaining n minus k \nhave you heard this gram schmidt o thogonalization right so if i give you  if there if \nyou are trying to construct a basis for n ok f or rn rather and if i give you k orthogonal \nvectors they can do k you can construct the remaining n minus k using gram schmidt \northogonalization right so you can get the full basis ok fine so let me just see and this \nis orthogonal ok so you can writ e so you see these two forms can you relate it to \nsomething that we have seen before in the course  \nthis is singular value decomposition what else did we see before \nstudent refer time thirteenfiftythree \neigenvalue so this exactly the same forms right  and i have used the same set of tricks \nto arrive at it right so i first put the vectors into a column as columns into a matrix then \nwrote this in the matrix format and  then pre multiplied post multiplied by certain things \nand i got these two formats and remember that v and u both are dash matrices \nstudent refer time fourteenthirteen \northogonal matrices  right so that inverse is just their transpose  ok ok s o far \neverything is fine now i still do not know what u and v are all this analysis is assuming \nthat i know what u and v are so now can you tell me how to get these u\u2019s and v\u2019s \nrefer slide time fourteenthirty \n \nsuppose v u and sigma exist  then we can write this  right so a is u sigma v so a \ntranspose would be the transpose of that now can you work with me wha t is the next \nstep \nstudent refer time fourteenfortyseven \nok next \nstudent refer time fourteenfiftyone \nthis is u sigma v transpose so then this would be i think the next step is no the next \nstep is also wrong that fine ok fine i just had some error with the transpo se ok what \nwill happen now what will disappear from here \nstudent refer time fifteentwentysix \nu transpose u that is i right u transpose the inverse of u \nrefer slide time fifteenthirty \n \nso you get this what does this look like this looks like the eigenvalue decomposition \nof \nstudent a transpose a \na transpose a that means v consists of the \nstudent eigenvectors \neigenvectors of the \nstudent a transpose a \na transpose a so now can you tell me what u would \nstudent refer time fifteenfiftyfour \n ok fine \nso thi s looks like the eigenvalue of eigenvalue decomposition of a transpose a  \nsimilarly we can show that a a transpose is equal to u transpose sigma square u ok so \nthen u is the set of eigen vectors of a  a transpose right and now here what was  with \nwill the eigenvalue decomposition always exist for a matrix \nstudent no \nno under what conditions would it exist first of all it has to be a square matrix \nstudent refer time sixteenthirtytwo \n ok right but now for a rectangular matrix would be singular value decom position \nalways exist \nstudent yes  \nyes right because it depends on the eigenvalue decomposition of square symmetric \nmatrices ok is that fine ok so for any matrix shall always have the eigen value oh \nsorry the singular value decomposition \nrefer slide time sixteenfiftyeight \n \nso this is perhaps  yeah ok now just one last bit and let us see  if all of you can \nunderstand this so now i can write a in this form this is nothing but what i already said \nright this is u this is sigma this is v transpose ok n ow from here from this step do you \nsee how i got to this step this is something that we were struggling with yesterday also \nwhen we were trying to find out summation x i x i transpose something similar here you \nknow the four ways of multiplying matrices right so this is which way one of the ways \nyeah so does everyone get this right so a simple thing would be first to just take these \nsigmas inside right because this is a diagonal matrix right this is all zero\u2019s so these are \nactually you can write it as sigma one u one sigma two u two and sigma k u k right \nnow this ends up being the product of two matrices  right and you can write it as a sum \nof columns into rows  right so what i am writing it as a sum of sigma one u one multiplied \nby v one so sigma one u one int o v one transpose is a scalar matrix vector matrix  right so each \nof these terms here is a \nstudent matrix \nmatrix and you are adding k such matrices ok now try to relate it to reconstruction \nerror you are taking a matrix trying to write it as sum of m any matrices if i trim some \nterms from this some terms from this sum what would happen if i have all the terms \nthen what would happen i will get a  back exactly if i drop some terms what would \nhappen \nstudent refer time eighteenfiftyseven  \ni get an approximation of a how good would that approximation be \nstudent refer time nineteenthree \nfirst is depending on the number of dimensions but is there a natural ordering in these \ndimensions if i want to throw away some dimensions which one would i throw away \n student refer time nineteeneleven \nsmallest \nstudent singular values \nsingular values sigmas are the singular values so you see that this is getting multiplied \nhere every matrix is getting multiplied by the singular value corresponding singular \nvalue so if i drop  out the terms which have the smallest singular values then those \nmatrices the elements would have been very small so i will not lose much in the \napproximation so again the same idea that i am trying to approximate the original \nmatrix by a smaller rank  \nby of so now the original matrix had m cross n entries ok how much if i use only k \neigenvectors or the sorry k singular vectors or k dimensions to approximate it how much \nstorage would i need how many values do i need so the original matrix was m cross \nn how many entries are there here \nstudent refer time twentysix \neach of this is how much \nstudent refer time twentyten \nm for ui plus n for vi ok and plus one for the sigma and how many of these are there \nstudent k \nk so if k is very less tha n your m and n  right then again you will have some \ncompression you get this ok so all of these ideas are related and i want you to be able to \nconnect them  right that all of this is towards doing some approximations \nreconstructing some reconstructing a matrix from it is components and doing this \nreconstruction in a manner that you end up making minimum error in the reconstruction \nis this idea clear even if some part of the math is not clear is this idea clear how \nmany forget this ok so some of you do not you do not \nstudent refer time twentyfiftynine \nyeah so what is the original dimension of a m cross n right now i am trying to \nreconstruct it using a sum of sum k terms ok so hence this k comes now each of these \nterms how many elements do i have i have ui which is of dimension m i have v i which \nis of dimensi on n and then i have the sigma i  which is of dimension one  right and i \nhave k of these so this is the total amount of storage that i need i am saying that as k is \nmuch less than m and n which would typically be the case  \nthen you are getting a much lower space reconstruction of the original data  right and \nyou are doing this reconstruction smartly because you are not taking any k dimensions \nyou are taking the k most important dimen sions and this most important is defined by \nthe singular values this is designed by the sigma is that fine \nrefer slide time twentytwotwo \n \nok and actually there is a formal theorem which says that sigma one u one v one transpose is \nthe best ranked one approximation of the matrix is this a rank one matrix is sigma one u n \ni hope you guys have done the assignment  right sigma one u one v one transpose is the rank \none matrix and if i take this idea further this summation is the best ranked two \napproximation and if i keep going this summation is the best rank k approximation so \nwhat it says is that  if you are trying to reconstruct the original matrix right from these \ncomponents and if you go by the eigen or the singular values and you pick the ones \ncorresponding the t op k singular values then the best that is the best possible \nreconstruction that you could have done \nnow how do you formally define reconstruction how would you make it as an \noptimization problem what are you trying to minimize \nstudent refer time twentytwofiftynine \nthe actual matrix has some values which is the matrix a ok b is the reconstructed \nmatrix using only k dimensions how many of you understand what is this product \nsaying what is this \nstudent refer time twentythreetwenty \nfirst k columns of u these are the k singular values and these are the first k rows of so \nok i was just talking about this is the first k columns of u these are the k singular values \nare put across the diagonal and this is the first k rows of v transpose and this is exactly \nthe product which i showed you here is that fine \nrefer slide time twentythreefortynine \n \nok so there is a theorem this is called the svd theorem it says that if you want to \nreconstruct a then this is the best rank k approximation that you can get now if i want to \npose it as an optimization problem what will i say what would i have minimized \nactually this is the reconstruction  right so let us call it a hat actually and what does \nthis mean this is the dash norm \nstudent refer time twentyfourfifteen \nfrobenium norm what does the frobenium norm give you squared difference between \nthe elements right  r oughly speaking  ok so it will tell you what is the square \ndifference between the ij \u2019th element of a and the ij \u2019th element of b so whenever we \nhave this situation if you are trying to if this is our objective function that we have trying \nto reconstruct a or try to  transform something and get a predicted a or a reconstructed \na then the best possible reconstruction would be given by this solution \nso this optimization problem h as a solution that you just use the eigenvect ors of xx \ntranspose and sorry a a transpose and a transpose a  right is that clear ok so this is \nthe theorem that we will be using when we are talking about autoencoders and we will \ntry to connect auto encode rs to pca ok so just revise this is the prerequisite for next \nclass whatever we have done in the last three sort of extra lectures you have to revise it \nbefore you come for class tomorrow right ok and yeah this is sigma is just some \nterminology sigma is actually the square root of lambda a that was obvious and u is \ncalled the left singular matrix of a and v is called the right singular matrix of a"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.1 Introduction to Autoncoders.wav", "duration": 3112.22, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  seven \nautoencoders and relation to pca regulation in autoencoders denoising \nautoencoders sparse autoencoders contractive autoencoders \nwelcome to lecture seven of the course on deep learning cs seven thousand and fifteen in this lecture we are \ngoing to talk about auto encoders and we will focus on their relation with pca then talk \nabout regularization in auto encoders wherein we will look at denoising auto encoders \nsparse auto encoders and contractive auto encoders so let us begin with the introduction \nto auto encoders what they are \nrefer slide time zerothirtysix \nso this is what a typical auto encoder looks like  and as you can see this is very mu ch \nlike a feed forward neural network  you have an input which is x i  so you are given \nsome training data  you are given some  i samples x i to x n  so this is your training  \nmatrix x which we have seen in the  previous lectures so this is one of those tra ining \ninputs x i  and then you have a hidden layer and then an output layer  so let us look at \nwhat is the configuration of the  hidden layer and what does the output layer actually try \nto do \nrefer slide time oneeight \n \nso it is a very special type of a feed forward neural network  what it does is it encodes \nwith input x i to a hidden representation h ok and it uses an encoded function to do this  \nso this is what the encoded function does it first does a linear transformation \nso w is a matrix and x i is a vector and you again have the bias b as a vector  right so \nlet us look at these dimensions right so let us try to fix some dimensions so suppose x i \nbelongs to r n that is what we have been considering throughout the  course so far and \nlet us say h belongs to r d \nso it is a d dimensional representation  so in that case what would  w be yeah so w \nwould be r n cross or the d cross n right so it will multiply with the n cross  one vector \nwhich is x i and give  you a d cross one output right and similarly the b would also be d \ncross one and then on top of that you have this non  linearity g which will be operating at \nelement wise just as we had seen earlier  so it could be any of the sigmoid functions the \nlogistic or tanh and so on \nso the end result is you have taken an input x i and encoded into a hidden represent h by \nusing a linear transformation first  and then a nonlinear transformation right so i refer \nto w x plus b as a linear transformation  because it is a matrix multiplication now once \nyou have constructed this hidden representation \nrefer slide time twofortyone \n  \n the job of the decoder or the latter half of the feed forward neural network  which is this \nhalf is to take this encoded representation and then try to reconstruct x again from it \nso again let us first look at the equation  so this is the equation for  the decoder where \nagain you first take the hidden representation  do a linear transformation  and then you \nagain have some  function on nonlinearity on top of it  right so we will see what this \nfunction can be so we will refer to it as f for now we will not say whether this is sigmoid \nor linear or what kind of a function it is we will come back to it later on \nso now let us again look at these dimensions so what is x i x i is again r n and your h \nwas r d so you have to go from a d dimensional input to an n dimensional output  so \nagain your w star is going it to be r d cross sorry r n cross d so it will multiply with a \nd cross one vector and give you an n cross  one output right and that will pass through some \nfunction and it will give you x i hat which is a reconstruction of x i \nso why are we trying to do  this right we took an input  x i we computed it is hidden \nrepresentation by doing some nonlinear and linear transformation and then again we are \ntrying to reconstruct x i hat  so why are we trying to  do this so reason we are doing \nthis is that  we want to learn  what are the most important aspects or most important \ncharacteristics of the input data x i  right so if you compute a hidden representation  h \nwhich is presumably smaller than your original input data \nand from that hidden representation if you are able to reconstruct x i  right then that \nwould mean  that this hidden representation  captures everything  that is requir ed or \neverything that is  yeah everything that is required to reconstruct x i from x i from the \noriginal input right \nrefer slide time fourthirtyseven \n \nso the model will be trained to minimize the difference between x i and x i hat  so you \nwant to make sure  that after passing through this bottleneck which is the hidden \nrepresentation you are able to reconstruct x i  and the reconstructed output  is very close \nto the original input right \nso can you see an analogy with  pca where you are trying to find this hidde n \nrepresentation or this most important elements of the original input x i  so there we had \nused this linear transformation where we are taking the original input x and transformed \nit to a new basis  and we had used that basis for representing  the original input right so \nsomething similar is happening here  we are using this hidden  representation h to \nrepresent our original input \nrefer slide time fivetwentytwo \n \nnow let us consider a few  cases the first cases when  the dimension of h is less than the \ndimension of x i  in this case  as i was trying to say earlier if we are still  able to  \nreconstruct x i hat perfectly from  h then what does it say about  h it tells us that  h is a \nloss free encoding of x i  it captures all the  important characteristics of x i write  just \nrepeating what i had said on the previous slide  and now you can see an analogy with \npca because h has all the important characteristics required from the original input data \nso it has probably got rid of all the noise  or all the low variance  dimensions or the \ncorrelated dimensions and so on and this is just the compact representation which is as \ngood as the original  representation and from there you can reconstruct  the original \nrepresentation and such an auto encoder  where the dimension of the h idden \nrepresentation is less than the dimension of your original input  is known as an under  \ncomplete auto encoder \nrefer slide time sixseventeen \n  \nnow let us look at the  other case where the dimension of  the hidden representation is  \ngreater than the dimensio nal of the original input  ok such an auto encoder  is i will tell \nyou what it is called so we will we are looking at the case where the dimension of the  \nhidden representation is greater than the dimension of the original input \nso now in such a case  the auto encoder could learn a very trivial encoding by simply \ncopying x i into  h and then copying h into x i right  so think of this from a compression \npoint of view right so now suppose you have ten bits initially right and then you want \nto somehow compress it and store it only in four bits \nand now this  four bits should be such that it  captures everything that was there  in the \noriginal ten bits because you would want to reconstruct the original input again right so \nthis is what we do typically when we compress any of our files right we have a larger file \nwe compress into a smaller information  while making sure that everything important is \nthere so that whenever i want to recover it i can just recover it from there \nso this is definitely a hard task  but now what i am doing in this auto encoder is that i \nhad ten bits i am actually giving it  more bits now because the dimension of  h is greater \nthan the dimension of the input and then from these sixteen bits i want to reconstruct the ten \nbits now this is a very tri vial task right because all i could do is  copy these ten bits into \nthe first ten bits here  leave the remaining  six blank and then from those  ten bits just \nreconstruct the input that  is very very trivial if you give me more storage and what  i \noriginally needed then definitely i can easily reconstructed \nrefer slide time sevenfiftyeight \n \nso this looks very trivial and this is what it could do right just copy the  input to the first \nthe n bits \nrefer slide time eighttwelve \n \nso this was n and this was d and we are looking at the case where d is greater than n so \nit will just copy the input to the first n bits and then just take it back to the output just as i \nsaid in the case of you have  ten bits sixteen bits and then again  ten bits it is very trivial to do \nthis \nso such an identity encoding is useless because you are just not running any  important \ncharacteristics of the data  your h is almost the same as x i it also has all the useless \ninformation that x i had in fact it has slightly more because it has these blank units also \nbut this is not really useful right why would you want to actually  learn such a  hidden \nrepresentation right so it is not clear why would you want to  do that so we will take a \nlook at it we will come back to why this is important \nso such an auto encoder is known as an over complete auto encoder because it has  the \nhidden representation has  more number of neurons as compared to the original input  \nnow let us look at a case where this would actually be important  right so this is a very \nrough intuition for why you would want an over complete auto encoder \nso let us consider the case where you have as input one of the features that you are looking \nas bmi so suppose you are trying to find out  whether the person is likely to get a \ncertain disease or not  right so whether he would have a heart attack or whether he \nwould have a diabetes  would have diabetes and  so on and you are looking at various  \nparameters or various medical parameters of that person and one of them could be height \none of them could be weight and one of them could be bmi \nnow for whatever reason you have not computed  the height and weight and you have \nonly looked at the  bmi so now what has happened in your input and all of you know \nthat bmi is actually body mass index which is a function of the height and the weight \nso now what has happened is that in your original input there was already this compact \nthe your feature space is already compact because you would actually look at you should \nhave actually looked at both the features h eight and weight  but for some reason you \nhave only computed bmi and you could think of various some other correlated  features \nwhich are functions of many other features  but you do not look at all those features and \njust this final function of those features  \nso now in that case  if suppose your prediction is that this person  has or has a  high \nlikelihood of being of high likelihood of having diabetes  at some point in his life  then \nyou would want to know whether it was the height  or whether it was the w eight which \nwas responsible for this \nso in your original input your features are actually entangled  and you would like to \ndisentangle them right so you would want to go from this smaller  feature space to a \nlarger feature space where some of these  entangled features get is disentangled so in \nthose cases we reach an over complete auto encoder  however the problem still remains \nthat there is no reason why the machine should actually learn to  disentangle these \nfeatures it could still just simply copy the bmi here and then copy it back here  \nso that is why when you are dealing with over complete auto encoders  you will have to \ndo something special to prevent this kind of identity encoding so as you just take the \ninput and copy it to the hidden layer  and then copy it back to the output  so we will \nlook at what kind of special treatment you need to do  to prevent these kind of  identity \nrepresentations \nrefer slide time eleventwentyfour \n \nhere is the road ahead  so first we will talk about the choice of f x i g x  i right so we \ndid not say anything about what these functions f and g have to be so we will talk about \nthose and then we will talk about the loss function so i have just told you so far that we \nwill train this model in a way that x i is very close to x i hat right and i have argued that \nif we are able to actually achieve that that x i hat is the same as x i  in which case \npresumably presumably the loss would be  zero that means our hidden representation has \ncaptured all the important characteristics of the original data \nsame as in the analogy of  ten bits to four bits to  again ten bits right if i am able  to \nreconstruct this without any error that means loss is  zero then these four bits or the hidden  \nrepresentation of my original x  was actually able to  capture e verything that was \nimportant in x so that it can reconstruct x again as x hat  without losing any information \nright so that is the  loss function that we would want now what is the actual \nmathematical formulation for this loss function that is what we will see next \nrefer slide time twelvethirtyfive \n \nso first let us start with the choice of f  f and g so we will consider two case two cases one \ncase when your inputs are binary  and the second case when your inputs are actually  real \nnumbers right so the first we w ill look at the binary case  so now just some notation \nclarification so remember our original data was this matrix x  which was m  cross n  \nthat means you had x one x two up to x n and each of these was r n  \nso now when i am referring to the entire row or  entire data instance i will use bold x i  \nas i have circled here and i want to refer to  one of the elements of this guy  then i will \nuse this notation x i j same as what i have written here so what i am saying is that each \nof these x i j\u2018s actually is a binary variable \nrefer slide time thirteentwentyfive \n \nnow which of the following functions would be most appropriate for the decoder  so \nremember was the input was binary  that means your output also has to be binary you  \ndo not want to produce numbers arbitrarily large belonging to any or want do not want to \nproduce any real number  you want to produce numbers which lie between  zero to one so in \nsuch a case what would be an appropriate loss function or sorry what would be an \nappropriate function for the decoder so remember i am asking you what would f  be \nso i am giving you  three choices it should be tan h or just a linear decoder or  a logistic \nfunction which of these would be most appropriate logistic why would that be  because \nit will make sure that your outputs ar e between  zero to one tanh would give you outputs \nbetween minus one to one but you do not want that because your inputs were between zero to one \nso when you are reconstructing obviously you want outputs between zero to one you do not \none minus one to one and linear of course can give you any  real number which is not what \nyou want right  \nso if you produce any arbitrary real number like hundred and so on your loss is going to \nbe very high because your inputs were just  zero to one and you are producing these arbitrary \nreal numbers which are very  different from what your input was  ok so in this case the \nlogistic function makes  the most appropriate choice  and g is typically  that means the \nencoded function is typically again chosen as a sigmoid function  so it could either be \nthe logistic function or the tan  h function right so the there is you could choose any of \nthese as the encoder function fine \nrefer slide time fifteenone \n \nnow let us consider the other case where your inputs are real valued  that means when \nyou reconstr uct something  you should again  produce real  values that means your \nfunction f should take whatever is the input  given to it and map it  to some real numbers \nright so that is what we want  from this function f earlier in the binary case  we wanted \nit to map it to binary numbers right so that is the difference that we have now \nso in this case which of the following would be appropriate  the second  one right \nbecause tan h does not make sense because it will just produce minus  one to one but you \nwant to produce any possible real number because some of these  are actually higher than \none greater than one linear would be fine because it will produce  any real number logistic is \nagain not fine because it will produce numbers between zero to one \nrefer slide time fifteenfiftyone \n \nso the logistic and tan  h as i said would clamp the  output to certain ranges  so that is \nnot appropriate hence you should choose the linear function  and again in this case also  g \nis typically chosen as the  sigmoid function fine ok so the next thing th at we look at is \nthe choice of the loss function \nrefer slide time sixteenten \n \nand again we will consider both the cases  where a case  the first case is the inputs are \nreal valued and the second case is when the inputs are binary \nso let us look at  the rea l case first  now here  the objective  of the auto encoder  is to \nreconstruct x i hat to be as close to x i as possible  now we have actually seen this \nbefore so something similar before when we were talking about regression  so now \nyou want to produce rea l valued outputs and they should match  your real valued inputs  \nso what is an appropriate loss function that you can choose  the squared error loss \nfunction right  \nso what does this actually capture it says that for all my input data x one to x m for each \nof these dimensions x one to up to x one n right i want to make sure that my original input i \nwill have a similar x hat reconstructed where i will have x one one hat x one two hat and x one n \nhat \nso i want to make sure that  each of these  pairs of variables  are actually similar and i \ncan capture that by  ensuring that the squared error loss  between the i j\u2019th entry in my \noutput is the same as this  or sorry rather i could capture the squared error loss between  \nthe i j\u2019th entry in the output  and the input ok  that is what this function is trying to \ncapture straightforward similar things we have seen while we were doing regression \nexcept that there we had  y hat and y but here we are just trying to reconstruct the input  \nso there is no y here we just have the x ok and the parameters of the objective function \nare of course all the variables or all the parameters that we have in a network  which is \nw w star c and b \nrefer slide time seventeenfiftysix \n \nand the matrix or the vector  way of writing this  is the following  so we have x i so \nwhat i am looking at here is  i have gotten rid of this summation  and i am just written it \nin vector form  so let me just explain what this means  so this is what x i would look \nlike right so this would be x i one x i two up to x i n ok this is the vector and then you have \nthe x i hat vector which is going to be x i one hat x i two hat up to x i n hat \nso taking the difference between these  two vectors that is what this term is  so what \nyou will get is essentially x i one hat minus x i one up to x i n hat minus x i n right and then \nyou are taking the dot product of this  vector with itself which will essentially give you \nthis summation right so the dot product of this vector with itself is actually going to be  \nthis summation it is going to be the sum of the squares of the elements of this vector and \nthat is exactly what we wanted \nrefer slide time nineteensix \n \nso this is a more compact vectorial way of writing the same thing and now we can just \ntrain the auto encoder by treating it as a regular feed  forward neural network this is just a \nlike any other feed forward neural network you have find the loss function \nand you can just use back propagation  to treatment right but and in this case all we will \nneed is a formula for  the gradient of the loss fun ction with respect to with your \nparameters which are w and w star i have again ignore the biases and the bias is here b \nand c so we will also need dou l theta by dou b and dou l theta by dou c right so these \ntwo gradients also you will  need but these are generally the easier ones to handle if you \nknow how to compute this b and c are very easy \nrefer slide time nineteenfortyeight \n \nso let us look at this now what we need for  back propagation as i said we will need this \ngradient right all these four gradients but let us focus on one of these now we have \nalready done back propagation  and we have looked at  arbitrary neural feed forward \nneural networks here right we did not have  we just said that there are l hidden layers \nand in this case l is equal to  one right or other we had said there was l minus  one hidden \nlayers and the l\u2019th layer was the output  \nso in this case l minus one is equal to one that means there is just one hidden layer so it does \nnot matter we had actually derived it for the general case when l is e qual when the \nnumber of hidden layers is l minus one and here we just have one eight n layer so it is much \nmore simpler than what we had learnt  and even for the number of neurons in the each \nof these layers we are just assumed general that it could be  r n and in this case we \nwould have some r d which is less than n or it could even be greater than n right  \nso but it does not matter because whatever algorithm we had or whatever equations we \nhad derived for back propagation  they did not care about what thi s n or d was we had \njust derive it in general terms right and the same for the output layer \nwe did not assume any number of inputs  any number of neurons in the output layer  we \nagain said that  it has some  k neurons but there the catch is in the earlier c ase when we \nhad derived back propagation  we were dealing with classification and we had these k \nclasses that we want to predict at the output \nand in which case our loss function was actually the  cross entropy or the negative log \nlikelihood function  right where we were trying to maximize  the probability of the \ncorrect class out of the k given classes but here our loss function is slightly different it is \nactually this squared error loss between the input and the output \nso now given this difference  in the loss function does it mean that everything that we \nlearn in the previous lecture on back propagation we just have to throw it all away \nbecause now there is a new loss function that means my gradients are going to be very \ndifferent from what i had derived for the back propagation loss  where i was looking at \nthe cross entropy loss as compared to the squared error loss so does it mean that i will \nhave to throw away all the hard work that we had done in that course  in that lecture or \ncan we reuse something from them we can reuse \nso let us look at what we can reuse  and i will just give you an intuitive explanation for \nthat so you can think of this as  a composite function  right and you are taking your \ninput passing it through a lot  of functions and t hen arriving at the output and then your \nloss function is actually a function of the output itself \nso what we have is something like this right we have a situation like this  that you had \nan input x you computed some function of it say x square right so i will call this as y one \nthen you computed some other function of it  say y one say log of y  one right so they this \nwas log of y one so in effect it is actually log of x  square because y one is equal to x square \nand then some other function and then finally you had the output so you had this other \nfunction which was sign of i am calling this y two so say this was sign of y two and finally \nyou had this function which was e raise to y three \nso you have a very complex  composite function of your original input  right and this is \nyour final output function that you are considering which is e raise to y  three now the way \nyou would do this is if you want to take the gradient  of d l with respect to your input d x \nright in that case what would you do is you just apply the chain rule you will write it as \ndou l by dou y three dou y three by dou y two dou y two by dou y one and then dou y one by dou x right \nand this is something very  similar that we are  done in the back  propagation lecture we \nhad constructed this chain  and then we had attack ed every element of this chain and \nderived how to deal with that right derived an neat expression for that \nnow the question which i am asking you is that in that lecture we had assumed a certain \nl and that l was actually  cross entropy but in this lecture i have actually changed the l  \nwhat i am saying is the l is actually equal to the squared error loss  now does that mean \nthat i have to throw away all this work that i had done no right \nso even in this example if you look at it suppose i change this function from e raise to y \nthree to say square root  of y three so i have just changed my l  but notice all of these other  \nguys are going to remain in the same  because y  three is still sign of y  two so that the \nderivative of y three with respect to y two is not going to ch ange even though i have changed \nthe output function the loss function  everything else is going to be remain  in the same  \nright  \nso that means all these portions i could just reuse from the time when i had computed \nfor this chain i just need to rework o n this final expression and plug it in  right so that \nis why all the work that we had done in the case of back propagation will not go to waste  \nin particular everything that we had done \nrefer slide time twentyfourfortyfive \n \nso let me just go to the next slide so in particular everything that we had done for  this \nportion of the network right which is  actually dou a two all the way up to dou w right so \nif ok so let me write it like this  i want dou l by dou w so i can write it compactly as \ndou l by dou a two and then dou a two by dou w right  \nso this portion is not going to change because i am not change any of the functions here \ni have just assumed sigmoid or logistic or the same kind of  network the only thing i \nhave changed is something at the output layer so i will just need to recomputed this and \nthe rest of it can be reused right so that is the intuition which i wanted to give you \nrefer slide time twentyfivetwentyseven \n \nand that is exactly what is written on this slide  so i am written it as dou  l theta by dou \nw star that is the first gradient i have interested in and i could write it as dou l theta by \ndou h two dou h two and dou a two by w star right  \nnow this portion as i was trying to say  is something that we have already seen in the \nback propagation lecture  and nothing has changed in the network in that part  so you \ncan just reuse it as it is  and this portion is something that we need to recomputed right \nthat is the only thing that we need to recomputed and plug it  into our back propagation \ncode or the algorithm which we had in the previous lecture and similarly if you want to \ndo dou l theta by dou  w it is the same idea here that you could write  it as the following \nchain and this  part of the chain you already know how to compute from the back \npropagation lecture \nall you need to do  is change the loss  function and just try to find the  derivative of the \nloss function with respect to your  output layer which  is h two that is the final thing  that \nyou have changed just as in my toy example i had changed e raise to y three to square root of \ny three right that is the similar change that i am trying to do here fine \nso all we need do is dou l theta  by dou h two but dou h two is the same as x i  hat right \nbecause that is my output and my output i am calling it as x i hat  so i need t o take \nactually the derivative of  this so i am just using the vector form here i could have also \nwritten it as this summation over i equal to one to n x i j minus x hat i j the two whole square \nright i could have also written it as  am i just writing it as t he vector here in the vector \nform here right  but this quantity ultimately is going to be a scalar because  it is a dot \nproduct between two vectors which is the scalar \nso what i am doing here is taking the derivative of a scalar  with respect to this  vector \nso what is that derivative going to be it is going to be a vector \nrefer slide time twentyseventwenty \n \nand i am just so we have similar stuff in the past  so you can actually easily work this \nout so this will actually turn out to be the  following vector which is to times x i hat \nminus x i  right so this is very simple i have just computed  this and all i need to do is \ngo back and  change my back propagation code  and change this derivative of the loss \nfunction with respect to the output clear and the rest of the code i can just reuse it as it is \nso now similarly so we have both of these ready \nrefer slide time twentysevenfifty \n \nnow let us look at the other case  when we have binary inputs  ok this is the most more  \nthis is something different that we will have to d o here so we will now look at the  \nsecond case where the inputs are  binary so first we look at case when the inputs were \nreal numbers and hence your outputs also needed to be real numbers \nnow we look at the case where inputs are binary and hence your  outputs also need to be \nbinary ok now here so each of these guys is actually a sigmoid functions so it is in or \nrather if you look at the output you could  divide into  two parts so this is the pre \nactivation and this is the activation  so your this is actually the pre activation and this is \nthe activation right  \nso this activation is actually chosen as the  sigmoid function or the actually the logistic \nfunction not the sigmoid  function of course  logistic is the sigmoid function  but  the \nlogistic function which was one over one plus e raise  to minus z right so logistic of z is \nequal to one over one plus e raise to minus z  and remember that this sigmoid function  was \nelement wise \nthat means this is a is a vector it has elements a one a two up to a n and then you know apply \nthe sigmoid to it  you get  h which is going to be sigmoid  of a  one sigmoid of a  two and \nsigmoid of a n right so it is just the sigmoid applied to every element  of the activation \nlayer that means every element of this vector which have circled \nso now in this case  your outputs are going to be between  zero to one right because your \ninputs were also between zero to one and your sigmoid or the logistic function is going to give \nyou clamped outputs between  zero to one so since this is between  zero to one we could actually \ninterpret it as probabilities  right so we could say that whatever you are reconstructing \nis actually telling you that with zeroeight \nsuppose the reconstruction value is zeroeight then you could think of it that with probability \nzeroeight it is telling you th at the output should have been  one right and if it tells  you that the \noutput is zerotwo if the sigmoid gives an output  as zerotwo then you could think of it that with \nprobably zerotwo the output was actually  zero or rather the input was  zero because an input is the \nsame as the output \nso that is one way of interpreting it  and this way of interpreting it  why does it make \nsense so we will just  look at that right  so  before at if i do not give you this \ninterpretation and remember that the sigmoid is going to produce values between zero to one \nbut not necessarily zero and one right it will try to be  as close to zero when the input is zero but it \ncould also produce zerofive and so on and when the  input is point nine it could also  \nproduce something like zeroninetyfive \nso at the output also  you are going to get these vectors which are of which would look \nsomething like this  right and suppose you are input  was zero one zero one now can think of a \nsuitable loss function for this yeah so again these are two vectors these are x hat and x \nso once again you could h ave just gone with the  with a squared error loss right  you \ncould have taken the squared error  difference between  these two and you could have \nbeen fine \nso that is definitely one way of going about it  but whenever we are looking at these \nbinary inputs and whenever this probabilistic  interpretation is possible  we tend to do \nsomething better which is look at the cross entropy loss instead of looking at the squared \nerror loss so i am not saying that the square error loss is wrong in this case  but you \ncould also use this cross entropy loss  and in practice for our binary inputs  the cross \nentropy loss often works much better than squared error loss \nrefer slide time thirtyonethirtyone \n \nso let us see what i mean by the cross entropy loss  so remember that you have  n \noutputs right that is why this summation  let us not worry too  much about what  is \nwritten inside for the time being  i will explain that but that is the i just want to explain \nthe summation first  so what you are saying is that for each of these  green guys at the \noutput you are going to make some loss and you just want to some over that loss that is \nwhat we are trying to see \nnow ideally you could have just written it as  just done what you had done  before and \nwritten this entire replace this entire box by this squared error loss  and that would have \nbeen just fine right of course there should have also have been this summation i equal to \none to m here because you are going over all the m training instances and for each of the m \ntraining instances you a re trying to minimize this  loss so this two summations followed \nby this squared error loss would just have been fine \nrefer slide time thirtytwotwentysix \n \n but instead of that i have this something special here ok so let us look at what  this \nspecial quantity is ok and now for that  remember that i am trying to interpret  each of \nthese inputs as a binary random variable i am saying that they can take values zero or one \nso i can think of it that when i am given  that this value  is zero i can write it as this  \ndeterministic probability distribution where i have p and the probability mass is entirely \nconcentrated out on this zero value and my the probability mass on the value  one is zero this is \nsomething similar to what we had done earlier  when we are given these labels suppose it \nwas apple orange mango and banana and the class label was given to us that this is an \napple then we could still write it as the probability  distribution where all the mass was \nconcentrated on apple and everything else was here \nso i am saying something similar here right so you could think of it that  two possible \nvalues can occur here one and zero and if i tell you this is zero right then i am telling you that \nwith probability  one into it is  zero and with probability  zero it is zero so i still write it as a \nprobability distribution  now the same thing i can have at the output  so for this unit \nwhen i am trying to reconstruct it  and if i produce the output as zerotwo then i can or rather \nlet us say zeroeight then i can say that with zeroeight probability i am predicting zero and with zerotwo i am \npredicting a one right  \nso now i can think of this again as  two probability distributions and once i some have \ntwo probability distributions i know that cross entropy is the right or a better loss \nfunction to look at  right and what  is cross  entropy actually  in this case it would be  \ngiven by summation i equal to  one to two right or rather i equal to  zero to one because if the those \nare the values it can take p of i right into log of q i plus yeah so p of i into log of q i that \nis how i can write it \nso let me just since there are only two terms i can just expand this summation right so \ni can write it as p i or rather p zero log of q zero plus of course it is a minus sign here this is a \nminus sign at the out p one log of q one i can just open up because ther e are only two terms \nso i can write it as this is that fine ok \nnow also i know that there is this relation between p zero and p one right that p zero is actually \none minus p one yeah similarly you have this relation between q  zero and q one that q one is equal \nto one minus q zero because the sum is going to be  oneok now let us look at this sum right so \nin the binary case  this sum becomes interesting  because now suppose your input x i j \nright which is the entity that i am looking at suppose that was equal to  zero in which case \nall the probability mass would be concentrated on p  zero and p one would actually be equal to \nzero which means the second term would display \non the other hand  if x i j is equal to  one then the reverse situation  what happen  that \neverything would be concentrated  on p one that means p one is equal to  one and this guy \nwould become zero because p zero is going to be  zero right ok so there is this another way of \nwriting it that you could day that instead of x  instead of writing p  zero and p one you could \njust write it as x i j right into log q zero plus one minus x i j into log of q one \nso now let us look at it again so when x i j is  zero first which is the same which happened  \nhere just an refer time thirtysixtwenty in same thing right because whenever s x i j is zero p zero is \nequal to sorry it should have been q one and log q zero sorry i made a mistake here so it have \nbeen x i j into log q so or rather let me just rewrite it \nso this is going to be actually i can write it as i look at this term first so i can write it \nas x i j  into log q one and then the second term i am going to write it as  one minus x i j  into \nlog of q zero right and then i am going to simplify this further  but let see what is the \nconsequence of this \nso now whenever x i j is equal to  one this term will remain and the second term  will \ndisappear and that is exactly what was happening in our original formula right so this \nis just an equivalent way of writing your x i j is equal to  zero this term will disappear  but \nthis term will remain that means log q zero will remain this is exactly what was happening \nin our original formula right  \nso that is so now i have given you why a i can replace p zero and p one or rather p one and p zero \nby x i j and one minus x i j and now i can make a similar argument for x hat i j also  so i \ncan think of q  zero as whate ver s predicted at the output right  sorry i can treat q  one as \nwhatever is  predicted out  one output so whatever my sigmoid function predicts i can \nthink of it as it is predicting  the probability of getting a  one right so it is just predicting \nthe heads probability or the probability of getting one so i can instead q i q one i can write it \nas x i j hat  and similarly instead of q  zero i can write as  one minus x i hat  i g right so did \nyou get that so these become very messy \nrefer slide time thirtyeightthirteen \n \nso let me just clean this up and i will just go over this again right  \nso what i was trying to tell you is that in the ideal case you could have just replaced this \nby the squared  error loss but since you are dealing with  binary inputs you can do \nsomething better because you can interpret the outputs as probabilities  so when you get \na zerotwo here you can interpret it as it is telling you that the probability of this unit being one is \nzerotwo it is very less and that is the same as saying that the probability of this  unit being zero \nis one right  \nso you can interpret this as a probability  now if you think of it that way then you can \nsay that at the input  you are actually given a probability distribution  so which tells you \nthat in the first case  your probability distribution  looks like  one zero  right because all the \nmass is focused on value  zero because your input is  zero at that case  and now suppose your \noutput was zerotwo right and zerotwo is what you are treating as a probability  of so this is the \nprobability of one this is the probably of  sorry this is the probability of  zero oops and this is \nthe probability of one \nso if your output is predicting zerotwo that means it is predicting zeroeight for zero and zerotwo for one \nnow if you think of it this way then you can capture  the loss function between these two \nguys using the cross entropy formula which is going to be summation i equal to zero to one p \ni log q i is that fine ok and now i just said that the since there are only  two terms i can \njust write it as p zero log q zero plus p one log q one \nthen i focused on this relation between p zero p one and your input so whenever your input \nis zero ok your p zero is going to be  one so then i can just replace p zero by one minus my input  \nright so if the input is  zero then this guy is going to be  one and that is exactly what this  \nexpression is also going to be \nso i can write it as one minus x j log q zero and similarly for this second guy whenever input \nis one this guy is going to be  one whenever my input is whenever my input is  one this p one is \ngoing to be one whenever my input is  zero this p one is going to be zero so i can just replace p  one \nby log by x i j  and now you can see that  this expression evaluates to the same as this \nexpression right you can substitute value of x i j  zero or one you will get the corresponding p  \nzero p one which would be  one or zero depending on what yo ur input was and these  two \nexpressions will evaluate to the same thing \nso just as i replaced the p\u2019s by x i\u2019s x i j \u2019s i can similarly replace by a the q \u2019s by x i j \nhats right  because once again  q zero is nothing  but  one minus whatever  my output was \npredicted because whatever is predicted  i am treating as the probability of getting a  one \nso one minus that is going to be the probability of getting a zero so that is what q zero is and \nsimilarly q one i can replace by x hat i j  and so that is exactly what i have done i n this \nexpression here \nso now this expression every term in these  n terms captures the cross entropy  for that \nparticular random variable  right so this is the original distribution  p for this  random \nvariable this is the predicted distribution q for this random variable and i have just told \nyou that this  the cross entropy between  these two distribution can be written in this \nsimple form as the function of x i j and x hat i j \nso this is the standard thing to do when you are dealing with  bernoulli random \nvariables so you can go back and read up a bit about it ah but for now i guess with this \nexplanation it should suffice to know why this expression is used and remember that i \nam not telling you  that this squared error function was bad  i am just tell ing you that \ninstead of the squared error function  cross entropy loss function works better  when you \nare dealing with binary inputs \nrefer slide time fortytwothirtyfour \n \nso with that let us pursuit and the another we have looking at it is the following you can \nnow look at this expression and tell me when is this expression  going to be minimized \nso we have x i j  and x hat i j  you can see that  this expression will be minimized only \nwhen x i j  or rather x hat i j is equal to x i j  right so now x i j could take v alue zero or one \nok and now x hat i j could take zero one or zero one \nso you can see that for these  two combinations the value is going to be minimized only \nwhen x hat i j is actually equal to x i j  that means if x hat if x i j was  zero then x hat i j \nshould also be zero and similarly in this case also  if x i j was  one then the expression will \nbe minimized only when x hat i j is equal to  one so let us see this so suppose x i j was  zero \nthat means this term is going to go to zero but this term is going to remain and now if you \nare x hat i j was not equal to zero \nthen you will get some log of one minus x hat i j as the loss right but if x hat i j was also zero \nthen you would get log of one which is zero so this whole expression would then evaluate to  \nzero which is the minimum possible value for this expression right  \nso that means if x i j is  zero then this expression will be minimized only when x hat i j is \nalso equal to zero similarly if x hat i j sorry if x i j is one then this one minus one will give you zero \nso this term is going to disappear but this term will remain so this will just be log of x \nhat i j because x i j is equal to one \nnow if x hat i j  is also equal to  one then this is become log of  one which is zero that means \nagain this expression will attain  it is minimum value when x hat i j is equal to x i j is \nequal to one right so this expression now attain it is minimum value in two cases when x \ni j is equal to  x hat i j  is equal to  zero or when x i j is equal to  x hat i j  is equal to  one so \ncompactly i can say that this expression will attain it is minimum when x i j is equal to  x \nhat i j that is why this loss function makes sense \nrefer slide time fortyfourfiftyfour \n \nnow again we have this problem that we want to use back propagation to train this \nnetwork and once again for back propagation we will n eed the following gradients the \ngradients of the loss function with respect to w and w star ok this is what we are going \nto need and i am going to make this same argument again that whatever hard work you \nhad done in the back propagation lecture you can just reuse all of it \nrefer slide time fortyfivenineteen \n \n because the only thing your changing is this final loss  function so you just need to  \ncompute the gradients with respect to this loss function  and everything else is going to \nremain the same right  \nso that is exactly what i am going to do on this slide  so whatever is in the boxes here  \nthese two boxes that is something that you have already computed  and now what i am \ngoing to compute is  the stuff which is outside the boxes  so let us look at that so i am \ninterested in computing this dou l theta by dou  h two this is the derivative of a scalar \nquantity with respect to a vector  say it is going to be a vector and i am going to follow \nour usual recipe which is h two is actually equal to h two one h two two up to h two n \nso i am going to consider  any of these guys which is  h two j i am going to compute  the \nderivative of the loss function  with respect to this  one entry and since i have that i am \ngoing to construct the entire gradient right so now i will have this dou l theta by dou h \ntwo j right and once i have that expression i am just going to generalize  it to all the other  \nentries in this vector \nso let us look at that expression first  ok so now if you look at  this actually it does not \nhave an h two j right but we know h two j is the same as x hat j or rather x hat i j right for the \nith input it is going to be x hat i j because h two is equal to x hat i ok you can just see that \nthe top left corner of the slides say x two is equal to x hat i  so this is nothing dou  l theta \nby dou x hat i j \nso now i want to take the derivative of this quantity with respect to one particular x i j and \nremember that this quantity has the sum which is indexed over  j so j goes from one to n i \nam looking at one particular j so that means if i expand this sum of all the js possible  the \nderivative with respect to all  but one is going to be  zero because they do not depend on this \nparticular j so if i am looking at j  equal to three then the term which has x hat i  one is going \nto the derivative of that term is going to be zero \nso for all these terms in the expression  only that term where a j is equal to the j which i \nam considering is going to remain ok so that means only one term in the summation \nwould remain and for that one  term so let me just rid of the summation right so that \nmeans only one term in the summation would remain  i am trying to find the derivative \nof this quality x which has a lot x i j\u2019s with respect to x i j \nso now this is of the form  a log x  so the derivative would a over x  right so that is \nexactly what i have written here and similarly for the second  guy this is  one minus a into \nlog of  one minus x  so the derivative is going to be  one minus a over  one minus x  and of \ncourse there is this minus sign here which will then get adjusted app ropriately right so \nthat is how this expression has been completely that  is very straight forward  and now as \nyou need the derivative of h two j with respect to a two j \nso remember that h two is equal to sigmoid  of a two which means it is just an element wise  \nsigmoid right so i just need to compute the derivative of the j \u2019th entry of  h two with \nrespect to the j\u2019th entry of a two all the other derivatives are going to be  zero because they do \nnot depend on that particular entry of a two so now that is just going to be sigmoid of a two \ninto one minus sigmoid of a two right  \nso i have computed these  two quantities i can just plug it then back into back \npropagation code  the rest of the code  is going  to remain  the same  and i have the \ngradients ready with me \nrefer slide time fortyninefourteen \n \n and as i said once i have this one guy i can just extend it  i can just generalize it  so i \njust had these j \u2019s here right for h two j so i can just replace the j by  one two up to n and i will \nget the same expression \nso that is the end of module  one where we introduced auto encoders  what we showed is \nthat they are actually just like any other feed forward neural network  accept that they \nhave this special objective  that they want to reconstruct the input  and the reason they \nwant to reconstruct the in put is they about to first create a bottle neck which is this  h \nhidden representation and then try to reconstruct from there  and just as i gave you that \ncompression analogy that you have this ten bits you want to compress it to four bits and then \nreconstruct the entire input again \nso this will happen only if these  four bits capture everything that is required or the most \nimportant characteristics of your original input  right and then we could have a loss \nfunction which tries to capture  the difference  between my original input and my \nreconstructed input \nnow we argued that this loss function will be dependent on the nature of your input  so \nfor the real inputs it was straight forward we just said that we can use the  squared error \nloss function for the binary i nputs we actually did something special  we said that we \ncan actually use the cross entropy  and then we  had this funny way of writing the cross \nentropy which was  this x i into log of x hat and  one minus x i into log of  one minus x hat  \nand just gave you some intuition that that is the same as writing p log a pi log or rather p \nzero log q zero plus p one log q one write and the i just gave you some explanation for doing that \nyou can go back and check on how do you write the cross entropy for bernoulli random \nvariables a nd you will see  that this expression makes sense  and once we had this  \nexpression computing the gradients was easy so the other thing that we relied on is that  \nin the back propagation  lecture we had taken  care of everything up to this point  and in \nthis lecture we have actually changed the loss function \nso one loss function was the sum of squared  squared loss errors  and the other loss \nfunction was the sum of  sum of cross entropies whereas in the back propagation lecture  \nwe had only dealt with cross entropy by the case that we made is that sense you have this \nchain all you have done is change the last function in the chain  right you have changed \nthis l function all the other functions you have not changed \nyou can just reuse the computations from these or you can just use the code that you had \nwritten for these in the back propagation  assignment and you just need to change  this \nlast guy to adjust for  the change in the  output layer or the change in the loss layer  so \nwith that  we will end the introductio n to auto encoders  there we have  done we have \nactually covered how to train an auto encoder using back propagation"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.2 Link between PCA and Autoencoders.wav", "duration": 1018.86, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 seventwo \nlecture \u2013 seven \nlink between pca and auto encoders \nso we will move to the nex t module where i w ould like to show y ou a l ink between \npca and auto encoders \nrefer slide time zerotwenty \nso this is what i am trying to  show you that under certain conditions pca is or rather \nan auto encoder is equivalent to a pca  and the conditions are if you use a linear \nencoder if you use a linear decoder if you use a squared error loss function and if you \nnormalize the inputs to this so for the time being just ignore the last bullet let us look \nat the other three bullets using squared error loss functions  \nso remember i gave you differe nt choices right you could have used the cross entropy \nor the squared error loss but i am going to prove this equivalence only under the \ncondition when we have the squared error loss  what do i mean the u encoder is a linear \nencoder g is a linear functi on we are not using a sigmoid or any logistic or anything \nlike that and linear decoder again the same thing we are not using the sigmoid or soft \nmax or anything at the output  it is a linear function under these conditions i will show \nthat or i will try to show you that pcas equal auto encoders equal to pca  \nwhat does this mean actually  now what do i mean by it is equivalent what do i have \nto show you actually how many of you understand what i am trying to prove how \nmany of you can mathematically define it ok so we will try to make this clear over the \nnext fifteen minutes \nrefer slide time onefortythree \n \nfirst let us look at the last condition right which i ignored  ok i always anticipate all this \nright so i have full faith in you guys ok what is thi s mean now what i am doing  \ncentering the data and i am also doing one by square root of m why  mean as the \nstandard deviation \nso the operation in the bracket ensures that your data now has become  zero centered right \nit is a zero mean  and now let x dash be  this matrix this one right such that all it is \nelements are zero mean is this still a flicker again alright \nso let i am calling x dash as this matrix ok so this matrix where i also have one by \nsquare root of m i can write it as  everyone gets this is si mple now do you see where \nthis is headed what would x transpose x be covariance matrix so i needed that one by \nm right at the out \nso now this is the co variance matrix so if i do this normalization to the original data \nand then if i take let x dash be that quantity and then if i take x transpose x then i will get \nthe co variance matrix everyone gets this that i did this to get the co variance matrix  so \nthat i mean i did this so that when i take x transpose x i get the co variance matrix after \nthis normalization only it will be the covariance matrix \nrefer slide time threefour \n \nso first we will show that if we use the linear encoder decoder and a squared error loss \nfunction then the optimal solution to the following objective function what doe s this \nobjective function \nstudent squared error \nsquared error loss is obtained when we use a linear encoder do you understand the \nimplication of this what does being stated here ok so i have fixed the decoder i \nhave said that the decoder is going to be a decoder i have fixed the  encoder or i have \nfixed the loss function this is going to be a squared error loss function  this is given to \nme now under these conditions i am trying to minimize this loss function ok \nthen i am telling you that the o nly solution to this is that the function dash should be a \nlinear function which function the function g should be a linear function you cannot \nchoose sigmoid or logistic or anything else right the optimal solution will occur when g \nis a linear function everyone gets what is being stated here \nrefer slide time fourtwo \n \nso this summation that i have written right or  in fact this the entire objective that i \nhave written is actually equivalent to this objective  is this fine with everyone  even \nthough i have not defined what h is just fine with everyone so we had   x which was x \none to x m ok i had picked one of these xi\u2019s what is the dimension of this \nstudent one cross \none cross m and then i had multiplied it by a weight matrix w  not w star remember that \nwhat do the dimension of w \nstudent n n \nn cross k and what will i get as the output \nstudent refer time fourfiftyeight \ni got an h which was one cross k what did i do this \nstudent multiply it by \nmultiply it by \nstudent w star \nw star which was k cross m and what did i get as the output \nstudent x hat \nx hat which was one cross n right so what i am telling you is that i could do this together \nfor all these x i s i could do this operation at one go and i can call this as x matrix  and \nwhat will i get here h one to h two to h m and i can call it as the h matrix and i multiply it \nby w star and what do i get x cap ok is that fine ok but without defining these things \nalso it was fine so it does not matter \nrefer slide time fivefiftyone \n \nso now how many of you get that this quantity is the same as this quantity  now how \ndo i explain this  was the frobenius norm of a matrix  some of the squares of the \nelements \nnow what is the matrix x  it is the x one one up to x one n and x m one up to x m n  and all \nelements in between right what is the matrix h w star  we just did that the sam e thing \nexpect that it is x hat \nstudent refer time sixfour \ni take the difference between  these two what do i get   every element of that matrix is \nequal to this quantity that i have underli ned right so i get a new matrix such that every \nelement of that matrix is equal to this quantity is that fine  now i am taking the square \nof every element of that matrix and adding them up what is that equal to \nstudent refer time sixfifteen \na frobenius norm how many of you get that now  almost everyone ok so this is \nequivalent to the frobenius norm ok now where have you seen the frobenius norm \nbefore what did we show in the svd theorem \nlet us try to connect things right if you do not learn how to  connect things it is going to \nbe very difficult what is this x hat it is a dash of x \nstudent reconstruction \nreconstruction it is a dash of x approximation  what is the solution to this optimization \nproblem what is the solution to this optimization p roblem i shall started off with the \nanswer that we saw this in the svd  theorem and then i asked you a question what thirty \nhours thirtytwo hours not even thirtytwo hours are passed since we did this come on what is the \nsolution to this no no that is fine \nbut what is the solution x hat is equal to what the best approximation to x is given by   \nwhat is it fine yeah so some k yeah but it is going to come from the svd theorem \nright is that fine  it depends on what rank approximation you want but it the best \napproximation to this is going to be given by the svd of x is it ok everyone gets that \nyes forgot about it but now do you remember it all those extra lectures eight\u2019o clock in the \nmorning \nrefer slide time eighttwentysix \n \nso that means  h w star should be equivalent to  this that we know from the svd \ntheorem that the optimal solution is going to be given by svd so if i just compare \nterms ok then i could write that one solution is this that h h is equal to u into sigma \nand w star is equal to v transpose  i could have chosen the other solution also where h \nis equal to v or sorry u and w star is equal to sigma v ok but i will work with this \nparticular solution you see this i am just matching variables right it is said that a b is \nequal to c d e so i am saying that a is equal to c d and b is equal to e \nnow we will work with this so  and we will try to show something  so let us see what \nwe are trying to show \nrefer slide time ninefourteen \n \nnow first thing that we will show is that h is actually a linear encoding  so what does \nthis mean you first always understand what has been tried to prove right i am saying \nthat i am going to show that h is a linear encoding of x then what is it that i am trying \nto show  \ni am trying to show that h is equal to  a linear encoding of x when h is of the form w x \nand not something of the form w sigmoid of w x or something like that or any other \nnonlinearity for that matter  is the statement clear that is what i am trying to show \nwhen i say h is a linear encoding i mean that h  is obtained by a linear transformation \nof x \nrefer slide time ninefiftytwo \n \nnow h as we defined on the previous slide is equal to this  now if i already had an x \nhere then i was done but i do not have any x there yet so i want to a get to a form \nwhere i can show that h is equal to w in to x so i will just do some simple trickery and \narrive try to do arrive at that form \nrefer slide time tenthirteen \n \nso the first thing i am going to do is pre multiplying  pre multiply by this  quantity and \nthis is fair be cause this is just equal to i what next i will write these three x \u2019s as u sigma v \ntranspose and i will leave one x as it is that \nnow just can you just try to see what the next step would be this v transpose v will \ndisappear because it is equal to i now what happened here i actually just expanded this \ninverse so i will think of this as a b c so a b c inverse is equal to c inverse b \ninverse a inverse  \nso i have just applied that it just that my inverse is a very straight forward matrices here \nthey are just the transform of the original matrices everyone gets this step well you can \nstare at for a for a few more seconds if you want how many of you do not get this how \nmany of you get this ok now what is next this u transpose u disappears \nstudent refer time tenfiftynine \nthis also disappears \nstudent refer time eleventhree \nno \nstudent refer time elevensix \nit is this u is only the first k columns of u right this is not the entire u this is just the \nfirst k columns of u fine now what next a into b inverse is \nstudent b inverse \nb inverse a inverse what will happen now that quantity will disappear so what do \nyou have left now ok so this is something ok so now let us look at this  is let us say \nthis is n cross n and this is n cross k what is the output going to be \nstudent n cross k \nn cross k and what is the output going to look like is the first k columns of \nstudent identity \nthe identity matrix everyone gets that if you do not you can just work it out  with the \nsmall matrix after going home a nd you will get it right  if so if i done the full \nmultiplication i would have got the identity matrix  but i am just talking the first k \ncolumns so i will get the first k columns of the identity matrix do not fed too much if \nyou are not getting this you can just work it out on paper and you will get it \nso i get the first k columns of the identity matrix and this inverse disappears this sigma \ntranspose into sigma transpose refer time twelvenineteen now what next what is this product \ngoing to be the first k elements of \nstudent sigma inverse \nsigma inverse and that is going to get multiplied by  sigma k cross k so that will give \nme the first k elements of \nstudent identity \nidentity m atrix there is some very simple matrix operations where  you are just ta king \nsome columns right so if you do not understand this right now do not worry you can \nwork it out everyone is confident they can do this please raise your hands if you are \nconfident and now what do i finally get this multiplication will give me \nstudent the first k columns \nthe first k columns of v ok  so have we come to the  desired form what i have shown \nnow h is a dash of x or linear transformation of x that means my optimal encoder was \na linear encoder and what was the optimal weight matrix w the first k columns of v yeah \ni someone pointed it last time also i could not i ignored it i will just pretend i \nunderstood \nbut i get it i know that there is a simpler solution i do not know why do it this way but \nthere is a simpler solution i just like making life miserable for you guys but  but the \npoint is you can figure it out that it is a it is a linear transformation of x now \nrefer slide time fourteenzero \n \nwe have that the encoder is equal  to the first k columns of v  ok what is v eigenvectors \nof x transpose x ok \nstudent a \nwhat is the other thing that you know about the eigenvectors of x transpose x they are \nthe solution for the \nstudent eigen \nif you have given an matrix x then the pca is the eigenvectors of the co variance matrix \nwas the co variance matrix  x transpose x what is are it is eigenvectors  capital v right \nso what have we arrived at are we done with the proof yes how many of you think that \ndone with the proof how many of you think that we are done now \nso it is done right so we have proved what we wanted to prove  right so what did we \nwant to prove that you are doing auto encoders you are trying to train an auto encoders \nand you are  loss function is the squared error loss function  we saw a neat way of \nwriting that squared error loss function as a matrix operation  where x minus capital h \ninto w \nand then we saw that these squared error loss function is nothing but the frobenius \nnorm of this and we knew that the minima of this objective function the frobenius norm \nof x minus h w would occur when s w is equal to svd of x right we started from \nthere and showed that h is actually a linear transformation of x  and what was that linear \ntransformation which matrix was used for the linear transformation  v capital v what is \ncapital v it is the eigenvectors of \nstudent x transpose \nx transpose x so what is happened in effect is that if i was trying to train my auto \nencoder with this objective function the weights in my initial layer w would actually \nconverge to v which a re the eigenvectors of x transpose x that means the \ntransformation that i have learnt  this transformation which i have learnt is the same as a \ntransformation that i have had learned using pca because pca would also have given \nme v into x where v was the eigenvectors of the co variance matrix and we just arrived \nat the same solution everyone gets it now we are done with the proof \nrefer slide time sixteentwelve \n \nso what we have proved is under these specific conditions that the encoder of a linear \nauto encoder is linear auto encoder is equal to pca if we use a linear decoder if we use \na squared error loss function and if we normalize the inputs to this  and you understand \nwhy each of these steps was important why was the last step important \nstudent refer  time sixteenthirtytwo \nonly then we would have got the co variance matrix  why was a step before that \nimportant because only if it was the squared error loss we would have got that frobenius \nnorm objective function right and why was the linear decoder important again the same \nthing because x minus h w we wanted it to be linear right is it fine  \nso you see why all these assumptions were important and under these conditions we \nhave proved that auto encoders e equivalent to pca"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.3 Regularization in autoencoders (Motivation).wav", "duration": 706.34, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  seventhree \nlecture  seven \nregularization in autoencoders motivation \nthen we will go to the next module  where we will talk about regularization in auto \nencoders and we will talk about a motivation for doing that \nrefer slide time zerotwentytwo \nso poor generalization so why do we need a regularization people have done the \nmachine learning course or any equivalent course why do we n eed regularization to \navoid \nstudent or enable \nor enable generalization right  now in the case of an over complete  auto encoder what \nis likely overfitting is likely why is it so what does what do you mean when you see \ngeneralization actually can you talk in terms of training time test time and so on \nso generalization is essentially that your are training so remember that at training  time \nyou are trying to solve an optimization problem  where you are looking only at the \ntraining data so it is q uite likely that you will drive the error to zero for the training data \nthat means you have learnt perfectly everything for the training data right but now it is \nalso possible that when i give you a new test instance which you had not seen during \ntraining that means you had not seen instance  while doing the optimization that means \nthis instance did not contribute to your loss function \nthen it is very lightly that when i gave this instance then you would get a non zero loss or a \nloss much higher then what you get for your training data does that make sense that is \nwhat over fitting is and it leads to less generalization your model should have generalize \nto unseen data but it cannot do this  one typical  situation where over or where \ngeneralization happens is if you have a dash number of parameters now what did i ask \nactually \nstudent generalization \nno ok if a case where a over fitting would happen is when you have a dash number of \nparameters \nstudent large number of \nlarge number of parameters r ight now do you see why i am saying this what is there \non the slide an over complete auto encoder what would it have \nstudent a large number \na large number of parameters so what could it do \nstudent overfitting \nover fitting what do we do to avoid over fitting \nstudent regularization \nregularization so that is why we need regularization i have still no told you why do we \nneed an over complete  auto encoder ok  still that is an random variable i still need to \ndecide but can this happen in an under complete auto encoder also it can right because \nunder complete auto encoder just says that your k  is less than n it d oes not say how \nmuch less it is right so it is it is still have and depending on a data that you are trying to \nmodel it could still have a large number of parameters \nrefer slide time twofiftyeight \n \nso for example let us take an example  for the under complete case  suppose you are \ndoing image classification where you have a digit three at the center of the image ok and a \nlot of these are white spaces so what is the dimension and suppose this is a one hundred cross \none hundred image what is the dimension of this image input how many if you cannot multiply \none hundred into one hundred \nstudent ten \nten k right of this a lot of data is not important  so my n is ten k and at least by this thing \nthat i have drawn it looks like  probably only twenty percent of that is what actually captures \nthe digit  but now if i choose k to be equal to one thousand it might still be large for this \napplication so i am using an under compete auto en coder but it could still be a \nsituation that my under complete is still having a large number of parameters  all of get \nthis intuition \nit is a very weird example but still really do you get the intuition  you could have a very \nhigh dimensional input  and you might think you are shrinking it a lot but there is so \nmuch redundancy in your input that even that shrinking still leads to a large number of \nparameters and you could still over fit  therefore even for an under complete auto \nencoder you could still need over a regularization \nso fine so that was the motivation  since the over complete case of course the model \ncan simply learn to copy we have seen that  and that is why we need to introduce \ngeneralization fine  now what is the simplest sorry w e need to  introduce a \nregularization what is the simplest regularization technique that you know that is not \nthe simplest l two regularization \nand you see why i say that is the simplest we can take the derivative for those of you do \nnot get it do not worry we will get to it or if you do not get to it do not worry \nrefer slide time fourfortyone \n \nsook the simplest solution is to add the l two regularization term to the objective function \nso this was my objective function i wanted to  minimize the squared err or loss i have \nadded a term to this what does this term do what does it doing  first of all tell me \nwhat is this quantity theta is a \nstudent all \nall the parameters that you have  right and i am assuming that they have just put it into \nlarge vector i am taking the ltwo norm of that vector  so even you though you have those \nmatrices and just flattening them all out and putting them into a large vector called theta \nright so what is happening here i am not allowing my weights to shrink or grow \ngrow because if my weights are very large what would happen \nstudent grow grow \nthis quantity would grow so then i cannot really minimize this minimize this as \neffectively as i want right  why this makes sense how many of you why this makes \nsense so i am now why am i not preventing the weights to go to zero ok so we will see \nthis in more detail in the next  lecture this is again a basic lecture on bias variance and \nregularization and so on so we will try to arrive at a more  reasonable answer for this  \nfor now just see that i am putting some constraints and the weights \nso effectively and i am doing gradient descent i am not allowing the weights to take \nvery large values i am trying to restrict them to a certain area so i am not allowing to it \nto explore the entire w comma b plane but trying to restrict it to smaller values of w \ncomma v how many of you get this intuitive explanation \nso in other words what i am trying to do is that i am not giving it in a freedom so that it \ncan completely drive the error on the training data to zero and my hope is that if i do not \ndo this if i do not allow it to completely memorize a training data then it should \ngeneralize well on the te st data is that intuitive fine  ok now i have changed the loss \nfunction again i have the square i have told you how to do it for squared error loss for \nthe cross entropy loss and so on but now i have changed the a loss function again  so \nagain i need to teach you back propagation no what will change now again i need to \nderive with respect to the last layer \nwhat is the minimalistic change that is going to happen now  just tell me this theta is \nactually w one w two and so on right just assume all the parameters just flattened out into a \nvector fine and now tell  me what is dou l theta by dou w one going to be  or let us \nsimplify things let us call this l theta a nd let us call this omega theta  let us call this l \ndash theta and then your l theta is the combination of these two terms so this derivative is \ngoing to be a sum of two derivatives out of that one you already know what is the second \nstudent two times lambda \ntwo times lambda w one so it is a very simply change to your gradient descent update \nrule how many of you see that whatever update you will had just add minus two lambda \nw one to that ok should have been two lambda w but of course you do a half here so it is \nfine is it \nrefer slide time sevenfortyfour \n \nrefer slide time sevenfiftythree \n \nanother trick which is typically used at least in  the context of auto encoders is to tie the \nweights of the encoder and the decoder how does that help  what does tying the \nweights mean now i appreciate what you are trying to say so one we have doing this  \nis just say w  star is equal to w transpose you will enforce that you actually have  only \none matrix w and here you are using w transpose mathematically does that make \nsense all your operations go through because this is going to be n cross k and this is \ngoing to be k cross n \nso whatever effectively done i have  reduce the number of parameters in my network \nright i am enforcing i am forcing this upon the network that i am not going to give you \ntwo sets of weights  you just learn the w \u2019s in a way such that when you use  w transpose \nyou should be able to reconstruct this how many of yo u get this not many ok please \nask me doubts if you do not there is nothing very \nstudent why is it w transpose \nwhy is it w transpose because otherwise \nstudent refer time eightfiftytwo claim that \nhow can you claim that that would work because you have  no linearity\u2019s in between \nright no w inverse would not work what is the simplest thing to do  why would you \nwant to compute an inverse  that is an interesting question how would you implement \nthis how would you if there are multiple paths from a weight to the output how do you \ncompute the gradient sum it across all those paths what is happening here how many \npaths to there exist from the weight to the output  one is this direct path  and then the \nother is another this path also  so you just sum it a cross these two paths do you get that \nhow many of you do not get that how many of you do not get that ok \nso if this was w star you did  not have a problem  you could just have computed dou l \nby dou w star and dou l by dou w now think of it as this right that you have this this is \none path w w to the output ok and now the gradient is just going to be sum across \nthese two parts  one path is the single path and the other path is the double path so it is \njust going to be a sum across these two paths oh no no so you just have one matrix w \nwhich are going to update you do not have two matrices you just have one matrix w at one \nplace you are using w the other place you are using w transpose but just look at it \nelement wise right do not try to look at it in the terms of matrices \nso you have n cross k elements here  w one one to w n k  right you have to computing the \npartial derivative with respect to each of these and every time they are considering all \npossible paths to the output and that value is getti ng updated right and at one place  you \nare using a particular arrangement of these w\u2019s at the other place you are using a \ndifferent arrangement of those w\u2019s that so it will just remain the same right is that ok \nstudent no \nno this is for regularizati on right so we are reducing the number of parameters by \nhalf \nstudent refer time tenfiftyone \nyes \nstudent refer time tenfiftytwo \nno that i mean that also has but that is not  be the objective we are trying to do \nregularization how many of you have lost  at this point  please ask me if you have \nquestions really i do not mind answering but if you just give me bl ank spaces i cannot \nread them so this is used at quite a few places where you tie some weights right so that \nso effectively you are saying t hat learn it in such a way that it works at both the places \nand you are reducing the number of parameters so weight tying is something which is \nvery commonly used for regularization in the context of neural networks \nso that is where we will end the mo tivation part and it is too very simple ways of doing \nregularization one is the standard known trick which is to use ltwo regularization and the \nother one was something special that we s aw which was tying the weights you all have a \nlot of doubts about tying the weights"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.4 Denoising Autoencoders.wav", "duration": 1537.38, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \n indian institute of technology madras \nmodule  sevenfour  \nlecture \u2013 seven \ndenoising autoencoders \n in this module we will learn about denoising autoencoders \nrefer slide time zeroseventeen \nso the idea behind the  denoising autoencoder is very simple what you do is  you have \nyour original x i now for the minute for a minute just consider the discussion when your \nx is are binary inputs ok so each of these red guys can be between  can be zero or one  now \nwhat i do is  before feeding it this  input to my autoencoder the box is the autoencoder \nwhat i do is i do a corruption \nso the corruption is as follows with probability q i will set x ij that means one of these \nguys to zero right and w ith probability one minus q i will keep it as it is ok so with some \nprobability q i am actually corrupting the data otherwise i a m retaining the data as it is  \nand then feeding that data to the autoencoder why would this work  binary input case \nas i said just assume that the inputs are binary  \nwe will also see the other case why would this  work what was our problem earlier  \nthat was completely able to reconstruct the training data right but at test time i had \nissues now what i have done to the trai ning data corrupted it  just think for a minute \nwhat will happen now i want someone to ask me a question in return oh that is the \ncorruption that i am choosing  or you could flip it is what you are saying yeah if it is zero \nchange it to one so that is also fi ne that is the question i was expecting what is the loss \nfunction now what is the loss function x hat my i minus x tilde i or x hat i minus x i \nwhich choice makes sense \nstudent first tilde \nfirst let us the case take the case when i do x tilde i  what happens in that case  from this \nnetworks perspective it is still learning to memorize the training data right it just this is \nwhat it thinks as the training data and just trying to learn that transformation right so it \nis not really helping my case do you understand that i just corrupted the training data \nthat is fine but from the networks point  of view it still gets away by memorizing this \ndata and that is not what i want so what should i do  can anyone tell me the i mean \ncan everyone tell me the answers \nstudent minimize \nminimize the error between \nstudent an x i \nan x i how many if you understand why that should help all of you gave the answer \nbut only few of your raised your hands why so hard to deal with this inconsistency \nrefer slide time threefour \n \nbesides because i am still going to minimize my original objective function ok  now can \nthe network get away by copying the input to the output so input remember the input to \nthe network is this and what i am trying to minimize this  if i just copy x tilde i to the \noutput will my objective function be minimized  no right so it does not have incentive \nto copy now so what will  it have to rely on say a reasonable probably twenty percent is \nthe standard right so even if i reconstruct i w ill not get zero error i will at least get some \ntwenty percentage \nso let us let me give you an example and then let me know if you can figure out what \nhappens this example will contradict something else that we have done before but just  \nplay along suppose my input features were height weight and bmi and we all know that \nbmi depends on height and weight i hope all of us know \nnow can you think what is happening i am corrupting one of these inputs  and i still \nwant everything to be reconstructed back so wha t will the network now have to rely \non it will have a now rely on this relations between these inputs also so again if i take \nmy example of digit three  i have corrupted some of these pixels right  but i still want  to be \nable to reconstruct three so it will h ave to be smart enough to learn that if i have seen this \nand i have seen this  then it has to be  something in between which gives me a three  do you \nget the intuition \nso now i am making it is job harder so that  it is robust  to changes at test time that \nmeans a test time if my digit looked something like this it should still be able to predict  \nit as a three or it will still be able to learn the same  representation as three do you get the \nintuition right so that is what i am trying to do i am trying to somehow br ing in the \ncorruptions that i would expect a test case and trying to make the model more robust \nit can no longer get away by memorizing the training data because i am not feeding it the \ncorrect training data it has to do something smarter than that ever yone gets this i will \ncome back to your question everyone gets this please raise your hands yes yes this is all \nunder regularization no this is regularization no so at that case i have already made that  \noverfitting can happen in an over complete  as well as under complete autoencoder \neveryone gets that right i show that example where it could happen in both the cases \nso my figure maybe over compete but it can just happen in any of these is that fine \nrefer slide time fivefortyfour \n \nit no longer makes sense for the network to just start copying the input data \ndifferent kinds of noises means  yeah so let me try to  answer that  right so what \nprobably you are trying to say is that all my input images were three vertically written  i \nadded some noise and managed it but now at test time suddenly you show me a three of this \nkind like that will not work also that is what were your question was a different types \nmean different values of the noise twenty percent twentyfive percent and so on \nrefer slide time sixsixteen \n \nso we will  first see  practical application in which autoencoders are used and then \ncompare it to denoising at autoencoders so this the next few slides for those of you may \ncare is also a small answer to the difference between machine learning and dp \nrefer slide time sixthirtysix \n \nso suppose you are given this task which is handwritten digit recognition  i see \neveryone paying attention now i should say this before every slide  ok so this is the \ntask handwritten digit recognition you are given some data  where you w ant to classify \nthe digits into one of these ten glasses the traditional machine learning approach to this \nis we just construct a feature vector this is a twentyeight cross twentyeight image  so i guess twentyeight cross twentyeight \npixels which is seven hundred and eightyfour  i treat this as a feature vector  and feed it to any of my machine \nlearning algorithms say svm or multi class svm or logistic regression or any of these \nright and do a  classification based on them  this is what you would have done in your \nmachine learning course if i had given you this assignment right \nrefer slide time seventwentyfive \n \nnow the autoencoder approach or in general the deep learning approach would be you \ntake this data which is the original feature representation that you had  there is no \nengineering feature engineering happening here right ideally i want to have features of \nthe form that if pixel twentyfive comma thirty was black and if pixel thirty comma twenty was also black \nthen probably i am drawing a curve somewhere so it could be one of these curvy digits \nand not one or any of these  seven or any of t hese things right so you want to do some \nfeature engineering \nso typically in machine learning what you do is you start with these seven hundred and eightyfour features you \nobserve a few things and you have these handcrafted features added on top of these  \nright so you will a dd some more features to the data  now the deep learning approach \nis that you let you also learn the features on their own  so how did we learn these \nfeatures we took this original input we passed it through the an autoencoder which \ncaptured some of these relevant characteristics \nthe differences we do not really know what these relevant characteristics are that \nmeans you and i cannot read them and make sense of them i cannot say that this pixel is \nactually capturing the interaction  oh sorry this neur on is actually capturing the \ninteraction between my seven hundred pixel and seven hundred and ten pixel i cannot do that  i could have \nhandcrafted those features if i believe that all my data is around the center  i could have \nhandcrafted some features which say that  capture the in teractions between those that is \nwhat you do in machine learning \nhere you are trying to learn the features also on their own right what would happen if i \nadd one more layer to this autoencoder i would learn even more complex interactions \nbetween these f eatures so this neuron is actually learning interactions between  all the \ninput neurons ok i add one more  layer here again this neuron will learn  all the \ninteractions between these abstract representations right so i could learn more and \nmore abstract representations of the input  so i am not doing feature engineering i am \njust throwing data at the network  and i am assuming that it will learn better and better  \nrepresentations \nnow i am doing this in the autoencoder setup where actually i am trying to optimize the \nobjective function of  minimizing this loss  and of course the squared of this loss is just \nfine so first what i will do is  i am not happy with my original seven hundred and eightyfour dimensions so i \ntrain a autoencoder to learn some k dimensions which are good i know these are good \nbecause they are able to reconstruct the data perfectly to a certain extent right of course \nbecause you add  regularization it may not be perfect but  it captures the essence of the \ndata you get that \nso i h ave better dash represent ations now feature representations  right my original \nfeature representation was seven hundred and eightyfour i have come up with some better representations  now \nwhat will i do was my task to learn feature representations what was it classification \nright so what will i do now is i will i have learned this much from the autoencoder \nrefer slide time teneighteen \n \ni will throw away the last layer i do not care about the last layer what i care at the last \nlayer is a classification problem right so i will construct a new neural n etwork where \nthe first two layers of the network are the same as what i learned from the autoencoder and \non top of that i will add an output layer and now i will try to train this network how \nmany of you get what is happening here those of you do not get  it can you ask me \nsome questions let me just try to answer on my own it is like playing chess with yourself \nso this is my original input  seven hundred and eightyfour dimensions what i have learned with autoencoders is a \nsmarter representation of this data ok now one simple so lution that i have is i have this \none hundred dimensional data suppose this is the representation  so for all the training examples  \ninstead of using that seven hundred and eightyfourdimension data and feeding it to a multi class svm  what i can \ndo is i can first compute this one hundred dimens ional representation and feed that to a multi \nclass svm is that fine and you see that should work  better in practice because  i have \nreduced the dimensions i have reduced the dimensions smartly \nand now i can train this network is this fine all i am sayi ng is instead of a multi class \nsvm i could also have a neural network right i could feed that representation to a \nneural network so what would that neural network look like one hundred  what are the \nparameters here w belonging to one hundred cross ten  how many if get it now  ok so this is \nwhat i could have done \nso i have learned a better feature representation and now i am using that representation \nto learn my classifier  if i do this in an end to end manner that means my feature \nrepresentation is also came out of  a neural network  and my classifier is also a neural \nnetwork then i have a complete end to end solution for this you get this \nrefer slide time twelvefourteen \n \nnow we will  see a way of visualizing this  and then we will make some observations \nfrom the visualizations so first let me tell you what the visualizations is \nrefer slide time twelvetwentythree \n \nso i am returning to the autoencoder setup so i had this input  and i had this h \ndimensional or k dimensional hidden layer  now i can think of each of these neurons  as \nsomething which gets activated for a particular type of input is that fine what do i mean  \nby activated it is output would be \nstudent one \nremember this is a logistic neurons that we are talking abo ut or even tanh  neurons the \noutput would be one  so it is the maximum output that you could gain fine  now so for \nexample h one is equal to sigmoid of this  when would this fire when where w one transpose \nx i is  very high right when you are in that  regime where the sigmoid flattens right this \nregime ok when it is very high  \nso i want to be able to maximize w one transpose x i do you get this i want to be able to \nmaximize this i want to find my w one transpose is fixed now because i have  trained the \nautoencoder i have got these weights this is all post mortem r ight i have trained the \nautoencoder i have got these weights now i want to find an input which will cause this \nparticular neuron to fire \nso what is my max what is my  optimization problem maximize just help me out \nmaximize w one transpose x let me just call it x and the optimization is with respect to x \nright because i want to find the x which maximizes this quantity my training is done i do \nnot no longer care about changing ws my training has been done i am interested in \nfinding x\u2019s which will maximally fire this \nrefer slide time fourteeneight \n \nso and i am going to assume that all my inputs  are normalized this just makes some \nanalysis easier and remember that normalization is always ok you always do that  so \nthis is the optimization problem that i am i nterested in solving  what is the solution to \nthis how many if you can solve this no i want to find the x i \nstudent refer time fourteentwentynine \nnow i have trained the autoencoder now i have known all these the one i am \nconsidering one column of the matrix w  one i want to see what is the input that  i should \ngive so that i am sure that this neuron will get activated  and i know that this neuron \nwill get activated if i maximize this quantity right  \nso i want to maximize that quantity and find an x such that i t will get maximized i was \njust hoping that no one brings in eigenvectors w one is a column  it is not a matrix just try \nto work it out what is this this is a dash between w and transpose and x i  dot product \nwhen would the dot product be maximized  when they are both in the  same direction \nright that means you know the direction is going to be x i is equal to and w hat did i \nwant the norm to be now do you get it fine \nrefer slide time fifteentwentysix \n \nso the solution is going to be this is fine w one by  the norm of w one  so just remember \nthat this quantity is going to get maximized when the dot product is maximized the dot \nproduct is maximized when both x i and w one transpose are in the same direction right so \nthat means x i should be in the same direction as w one and i also wanted this constraint \nthat x i should be the norm of x i should be one  so i am just dividing w one by the norm of \nw one \nso i know now what is the input i should feed to the network  so that one of these \nneurons fires now what i am going to do is i am going to plot the xi \u2019s which maximize \neach of these neurons i am going to consider some one hundred neurons in the hidden layer and \ni am trying to find out the input image which is going to maximize or which is going to  \ncause each of these neurons to fire  do you get what i am trying to do even though you \ndo not get why i am doing it but do you get what i am trying to do ok \nso what am i going to do is  this is a vector right so i am just going to try to plot this \nas an image of the appropriate dimension \nrefer slide time sixteenfortyone \n \nand this is what i get with a vanilla autoencoder there was no noise this is what i get and \nthis is for the mnist digit data set  right so my data is two three one and so on digits this is \nwhat happens when i get twentyfive percent nice and this is what happens when i get fifty percent \nwhat do you understand from these figures  remember that each of this  is the figure \nwhich caused one particular neuron to fire is that clear each of these is a trigger which \ncaused one neuron to fire  \none image yeah one box corresponds to one column yeah so it is just that the \ndimension of the column is again twentyeight cross twentyeight so i am just plotting it as a twentyeight by twentyeight \nimage so i will just let me just clarify that is i think that is what i said yet  so what is \nthe dimension of this in fact you just know this right this dimension of this is twentyeight cross \ntwentyeight \nso i can just take that vector and again plotted as a twentyeight cross twentyeight image  so what i mean \nis this is seven hundred and eightyfour right so x i is a seven hundred and eightyfour dimensional vector  i am just taking it as a twentyeight cross \ntwentyeight image and plotting it because my inputs were actually images so i am just plotting \nthose images fine so at least you see what i am doing here and what i am telling you is \nthat each  of these boxes that you see  corresponds to  one of these images so i had \nimages x one x two up to x k  such that each of these caused the k\u2019 th neuron to fire ok now \nwhat are you seeing here  i mean what how do you make sense of what you are seeing  \nand remember in the mnist ok sorry so let us try to forget all this neural network and \neverything and let us just try to see yes the weights would be \nstudent more distinct \nno why do you say the weights are more distinct yeah but on average you would be still \nreducing it right ok so let me just expla in what is happening then we can come back to \nthis so now  we have this set up  we had some input  we had a certain number of  \nneurons here and then we had the output ok this is what our  neural network was trying \nto do \nstudent refer time eighteenfiftythree \nnow let us take this task of recognizing a digit now how do i actually recognize a digit  \nif i want to distinguish between a nine and a three i would try to see if there is a curve in these \npositions and it is not there in this  hence this is a three this is a nine that is something roughly \nlike that right so in other words i am now i have given delegated so that means what i \ndo is i think of three as a combination of  you get the idea as a combination of these images \nwith these strokes right so this is actually this strok e this is actually this stroke  this is \nroughly this stroke and so on you get the idea \nso i think of three as a combination of many of these strokes right  now what i would like \nis if this guy  could detect one of these strokes  right the other guy could de tect one of \nthese other strokes right now you see that some of these strokes are shared across digits  \nfor example all these strokes here  look at the digit nine these strokes gives common to three \nand nine both right but some strokes would be missing for three some strokes would be missing \nfor nine and you would have extra strokes in both of these  so now each of these neurons \ncould actually recognize these strokes then a combination of the information that each of \nthese neurons is capturing could help me decide whether  it is a three or a nine how many of \nyou get that intuition \nstudent refer time twentytwentythree \nso i would like each of these neurons to detect certain strokes ok that means i would \nlike this neuron the first neuron to fire for an input like this where there is a  stroke at the \nbottom i would like some other neuron to fire for a different input whether there is \nstroke here now can you relate this to what you are seeing in the picture in the second \nand third picture this neuron is firing for inputs which would h ave a stroke at the corner \nright and you see different neurons are firing four different strokes so each neuron is \ntrying to capture something relevant  and together now i could combine them to get the \nfinal output how many of you see this how many of you do not get this \nso to ask questions otherwise  i cannot really help it  how many of you want me to go \nover this again which part yeah so let me just repeat what each of these boxes is  right \nso each of these boxes is the image  which causes the k \u2019th neuron to fire  right so \nremember i decide i came up with this that this is the  input which causes the second \nneuron to fire what was the dimension of this input twentyeight cross twentyeight  \nso i am just plotting that twentyeight cross twentyeight input right and i am realizing that  this input seems \nto be something which has a dark spot here right so now just related to the analogy that \ni am trying to give at the bottom that this neuron fires for inputs which have a stroke \nhere that is that is capturing and there are other neurons which are trying to fire for other \nstrokes \nand i would want these neurons to capture different strokes so that together they \ncaptured all the information in the image and helped me decide that a combination of \nthese strokes gives me a nine a combination of th ese other strokes gives me a three is that clear \nnow you also \nstudent yes sir \nso yeah so now the thing  is this right  the again the same thing you could learn to \nreconstruct the output but you may not capture the important characteristics in the input \nright so now as you keep making it is job  harder it has to rely on capturing these \nimportant characteristics in the input right  and actually if you look at the difference \nbetween the second figure and the third figure right let us look at the same guy here \nso you see that this is actually thicker and wider the stroke that you see here is thicker \nand wider so now it is actually relying on more neighborhood information to fire  it is \nnot firing just for this stroke but it is fighting for a larger str oke it is also requires more \nneighborhood information because you are corrupting the pitch \nso it has to rely on information from the other guys  the same example that i gave for \nheight weight and body mass index right the same thing holds here i have corrupted a lot \nof inputs so now it will fire only if it gets a lot of information from the neighboring \ninputs also is that fine  ok and i now coming back to your question  yeah i do realize \nnow what you are saying that the weights are actually becoming la rger yeah it makes it \nmore robust but again \nso regularization just does not always mean that your weights have to be small right that \nis one way of constraining or regularizing but this is another way of regularizing where \nyou are making it more robus t but it does not necessarily need to lead to the same \nsolution where your smaller weights does that make sense it is  ok for most of you any \nplease raise your hands if this \nrefer slide time twentythreefiftynine \n \n and this is same thing that i have written here \nrefer slide time twentyfourtwo \n \nnow we saw one form of this  function ok which was  just flip the input if the output is  \njust corrupt the input right  you could also add a gaussian noise so you could take the \ninput add a gaussian noise to it with zero mean and then again try to reconstruct the original \ninput back is that fine so you could just use different noise functions to do this  so we \nwill now see such a  denoising autoencoder where we have actually added a gaussian \nnoise instead of the zero one noise or the corruption that we were doing \nrefer slide time twentyfourthirtynine \n \nyeah so the purpose of this particular  example that i am giving is  to compare an \nautoencoder which is regularized by adding this gaussian noise  with an autoencoder \nwhich is regularize by using weig ht decaying right the ltwo regularization so ltwo \nregularization is also known as weight decaying  because you kind of decay the weights \nright you force the weights to be small  \nso what they showed is that with  denoising autoencoder using a gaussian noise  you \nactually learn something known as edge detectors  right so you see all of these are \ntrying to detect edge again the same thing is happening  i am plotting the images  which \nwill maximally cause a particular neuron to fire and it looks like all these neurons fire for \ndifferent edge patterns in your original data \nso now they are capturing all the edges  in the data  and the combination of these edges \nshould tell you what your final class is  ok and this seems to work much better than the \nweight decay filter which is not really capturing any regular pattern ok so this is just an \nempirical evidence that  an autoencoder with a gaussian noise seems to do better  than \nautoencoder with the ltwo regularization"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.5 Sparse Autoencoders.wav", "duration": 511.08, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sevenfive \nlecture \u2013 seven \nsparse autoencoders \nso in this module we will talk about sparse autoencoders \nrefer slide time zeroseventeen \njust some concepts before we jump into the  actual way of doing this  so hidden neuron \nwith sigmoid activation will have values between zero to one and you say that the neuron is \nactivated when this output is close to one and it is not activated when its output is close to zero \nok now a spare encoder tries to ensure the neuron is inactive most of the times  what is \nthat mean \nstudent close \nit is close to zero for \nstudent most of the \nmost of the \nstudent refer time zerofortyseven \ninputs right so i am passing a lot of inputs to it it will try to ensure that it is close to zero \nfor most of the inputs  so in other words what does it  trying you ensure  i am looking \nfor the word average  the average activation of a neuron is close to zero does that make \nsense is that fine  \nrefer slide time onesix \n \nso this is on  what you see on the  left hand side  this is how you would compute the \naverage activation of a given neuron  you have all the m examples you see what the \nactivation was for each of these and take the average right  \nnow if the neuron is sparse  then the average activation would be close to zero is that fine \nthis is all just  different ways of saying the same thing  now a sparse encoder  uses a \nsparsity parameter say rho and it is very close to zero say zerofive  \nand it tr ies to enforce the constraint that on average the activation of any neuron in the \nhidden layer should be equal to rho  which is again close to zero now can you think of a  \nthis is all fine in plain english right you understand what we are trying to do  first of all \ntell me why does this makes sense  what is it that you are trying to ensure  over fitting \nhappens because there is lot of dash \nstudent parameters \nparameters slightly abstract it out \nstudent memorization  \nlot of \nstudent memorization \nmemorization ok lot of freedom right i mean the weights have a lot of freedom to move \nwhere ever they want to do  whatever they want to do  such that they can just drive the \ntraining error to zero what have we done to that freedom now \nstudent we are restrict refer time twotwentytwo \nwe are restricting them so any kind of regularization always tries to restrict this \nfreedom that the parameters or the network have in general right  and there are different \nways of restricting this freedom  you see that this is one of those ways  right you are \ntrying to ensure that on average the neuron should not fire so it is clear that this some \nkind of regularization any one has a doubt with that no  \nnow the second question is taking slightly more on this right it is i can just move ahead \nand i have convince you that this is regularization but can you think of bit more and see \nwhat is actually being tried to achieve here  what are we trying to do how many of you \nget that or at least could here that first of all only the second row ok so yeah how many \nof you can think about this like what is it trying to achieve  \nstudent refer time threethirteen \nright so on average neuron is going to be inactive that means where ever it is active it \nis really going to capture some rel evant information right so it is going to be active  \nwhenever it is active it  is going to adhere to certain patterns  so we are ensuring that \neach of these neurons are just a very few patterns and it has discriminative power in that \nsense do you get that  \nso now if that means if i show it a three if i show it a two if i show it a one every time if the \nneuron fires when there is no discriminative power in that  but now if i ensure that the \nneuron fires only a few times  it will try to fire for meaning ful l patterns so it will try to \nfire for a curve or a curve in the between as you have it in the case of three right you have \nthis cusp in the between in the middle so it will fire for some kinds of pattern \nso that is what the hope is  it is not  just like  adding some math and adding some \nregularization but at least there is some intuition behind that  how many of you get that \nintuition ok good and now can tell me a way of putting this  everything english is \nfine intuition is fine but how do convert this to a mathematically equation  \nyou want to ensure that rho hat l is equal to rho there will of course be different ways \nof doing this  the way these guys do it by adding this term to the loss function  so \nremember your loss function  is always going t o be l dash theta plus omega theta right \nwhere omega theta does the regularization  and l dash theta is your  regular loss which \nwould be the squared error loss or the cross entropy loss or whatever loss you are dealing \nwith right  \nso remember this term is always there  but the reason i  do not bring it up  so often is \nbecause we have already dealt with it we know how to compute the gradients we know \nhow to do the back  propagation and all that  and now since your final loss is just a sum \nof these two terms i know how to deal with this and i know that gradients are additive so \ni just need to deal with the second term  that is why i am only focusing on omega theta  \nl theta has been dealt with is that fine  \nnow this is what omega theta is  why does this m ake sense when would this  take its \nminimum value when rho is equal to \nstudent watt \nwatt everyone sees that how many of you sees that  please raise your hands ok fine let \nus plot it and check actually right \nrefer slide time fivetwentythree \n \nrefer slide time fivetwentyseven \n \nso this is how that function looks like  so i have plotted the function which i  have \nwritten here for a of course a single k right  and my rho that i have taken is zerotwo and if i \nplot that function for different values of rho hat l it will reach the value zero only when rho \nhat l is equal to point  so again go back and plot this and check and it  is actually clear \nfrom the equations itself  that it will be minimized only when rho hat is equal to rho l \nright \nso that means this is a genuine i mean this is a reasonable thing to do we would think \nof other ways of  doing that and i  am sure you can  but this is also a reasonable way of \ndoing this \nrefer slide time sixeight \n \nso now our last function is as i said it is going to be a combination of two values l theta \nis a normal squared error loss that we have been dealing with and omega theta is this \nsparsity constraint that you have added  \nnow you already how to calculate the first term what are we interested in now so you \nsee that this pattern will keep repeating right so now you can do whatever you want your \nloss function you have this generic frame of called the  back propagation algorithm and \nyou know that a last part of that back propagation algorithm is going to remain the same  \nright only thing you are changing is the output layer or the loss function  \njust need to compute something there  and the rest of it will remain the same how many \nof you get this general idea  and also appreciate it  right that is why this is a very \npowerful frame right you can just make  minor tweaks at the top and you are rest of the \ncode has to remain the same  \nso you can actually go back and  try out these regularization terms in mnist \nassignments right  if you really want to see what  happens ok so now this is what \nomega theta is and now what i am going to do it can be rewritten as this that is obvious \njust expanding out the law of function  \nand by chain rule this is what i  get now unfortunately the rest of the slide  there is an \nerror the ta\u2019s please note this i can kind of overlooked this ah but i will just convey the \nidea right so you would want to do something of this sort  everyone agrees with that  \nremember what is rho hat it depends on sorry rho hat l it depends on \nstudent refer time seventhirtysix \n h of l right it is the average activation of the l\u2019th neuron and this depends on some of the \nweights so that is why this chain rule makes sense and now how to compute this there \nis an error on this slide  but you have done enough gradients in the c lass for me to have \nconfidence that you can do it on your own  everyone is confident that they can work it \nout on your own  \nrefer slide time sevenfiftyeight \n \nso i will skip this  we will fix these errors  there are some summation and other terms \nmissing here and the second part is actually correct  which has been derived on the next \nslide \nrefer slide time eightfive \n \nbut i would not go over this this is there are the slides again go back and look at it how \nmany of you are confident that you can do this on your own please raise your hands  \nyeah because we have done enough of this in class right  \nso you can you should be able to it no if you are not able to do it  then i am not doing a \ngood job at teaching you  right so you should be able to do it now  fine and we will fix \nthese errors so ta\u2019s just remind me after the class  so everyone gets the general idea  \nyou find a loss  you find a constraint  you define it with omega theta  find out the \nderivative of that with respect to your parameters  and just change y our gradient descent \nupgrade tool accordingly"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 7.6 Contractive Autoencoders.wav", "duration": 478.6, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 sevensix \nlecture \u2013 seven \ncontractive autoencoders \nso with that we will move on to something known as contractive autoencoder so this \nis yet another type of auto encoders again with the sa me aim that you want to do some \nkind of a regularization \nrefer slide time zerotwentyfive \nso it again tries to prevent and over complete auto encoder or even an under complete \nauto encoder for that point from learning the identity function \nso it does not allow you to simply copy the inputs to the outputs  that is what it is trying \nto learn and it does so by adding the following the regularization term to last function \nand the way it does this is b y defining the following regularization term  ok what is this \nterm ok let us see some  things which we already know  what is this  frobenius norm \nof some matrix what is this matrix \nstudent jacobean \njacobean what is the jacobean \nstudent refer time onezero \nwhat are the two variables here that you see \nstudent h \nh and \nstudent x \nh is a scalar matrix vector \nstudent vector \nvector x \nstudent vector \nvector right so it is some function between two vectors ok and it is a matrix so take a \nguess how many entries would not you have if x is r n and h is r k  \nstudent n cross k \nn cross k even if you do  not know what the entries are you are able to guess that it  is \ngoing to be a n cross k matrix right  \nrefer slide time onethirtyfour \n \nnow let us see what this n cross k matrix looks like ok  \nrefer slide time onethirtyseven \n \nso it has the input has n dimensions and the hidden layer has k dimensions  so this is \nwhat the jacobean looks like \nwhat is the first column if the partial derivative of every neuron in the first hidden layer \nwith respect to the first input  right and now you can see what the other col umns would \nbe t his is what the j acobean is  this basically the  derivative of h with respect to the \nvector x answer is just you are taking a derivat ive of a vector with respect to  another \nvector you will get a matrix as the output  ok now what does the jl\u2019th entry here capture \nactually  \nstudent refer time twotwelve \nwhat does a derivative capture \nstudent refer time twofourteen \nhow much does h l change with a small change in \nstudent x k \nx k right that is what a derivative captures is that fine  and then what does the frobenius \nnorm capture it is just the square of sum of the square of all the elements of the matrix  \nright so it is basically how m uch each of these elements vary with respect to the input \nand we are just taking the square of that  so you see what is the term that we have \nadded \nrefer slide time twofortyfour \n \nnow tell me what is intuition behind this  ok so when would this  term so remember \nthis term is added to the loss function and you are trying to minimize the loss function  \nso that means you want this term to go to \nstudent refer time threeone \nyou want the frobenius norm to be  \nstudent zero refer time threethree \nzero right ideally of course that will not happen because there is always a tradeoff between \nl theta and omega theta if you make it zero then l theta would be very high right  \nrefer slide time threeseventeen \n \nso now what would happen if one of these guys say dou h one by dou x one actually goes to \nzero what does that mean h one is not sensitive to variations in x one right fine but was our \noriginal mandate what did we want these neurons to capture  we wanted the neurons to \ncapture these important characteristics right  \nso if x one changes we want h one to change do you get that  how many of you get that  \nwe wanted the neurons to capture the important characteristics of the data  right but \nnow we have added a contradictory condition which says that we do not want the neuron \nto capture a variations in the data  do you see this  so what is happening here  l theta \nsays that i should be able to capture these variations right otherwise i will not be able to \nreconstruct  \nif all my h i\u2019s are not sensitive to variances x one that means i give it any x one it will \nproduces the same h i is that clear is that with ok everyone right that means so see this \nis this so i have these training examples  occurs all these training examples  my bold x \nwhich is vector x is going to change  t hat means xi\u2019s which are the elements of this \nvectors are going to change  \nnow what this condition is saying is that if i change xi i do not want the h l\u2019s to change \ni do not want the values of the hidden representations to change  so that means it is \nchanging the respective of what is the input fed to it try to produce the same output  do \nyou get this argument  ok that means it is not capturing any important characteristics \nof the data is that fine is that valid argument but that is not what we wanted we wanted \nit to capture the important characteristic of the data so what are we trying to do now  \nrefer slide time fiveone \n \nso just i it is hard for me to do evaluate what you have said  but just pay attention and \nsee if that is correct you can judge it on your own right so that is the actually the idea \nright we have  put these two contradictory conditions  with each other right  l theta says \ncapture the important variances of the data omega theta says do not capture variations in \nthe data  watch the tr adeoff capture only very important variations in the data do not \ncapture the variations which are not important  can you relate this to something that you \nhave seen before \nstudent bias variance \nno the other answer there are only two answers bias varian ce and pca when i say the \nother answer \nstudent pca \nwhat am i trying to force it to do  capture only the important variation it is if it is not \nclear right now we will come back to this ok  \nrefer slide time fivefiftytwo \n \nso let us try to understand with this with the help of an illustration  right how many of \nyou get the argument which i made on this slide ok most of all  \nrefer slide time sixthree \n \nnow this is the situation  i have u one and u two as my dimensions fine  which of this is \nimportant u one the variations in the data across u one is something that i should care about \nbecause i can see that brings in some difference what about the variations in u two \nstudent not important \nnot important they seem like noises because these variations are there  they are not all \nlying on the central line  they are slightly away from the line  here are some variations  \nbut should i go out of my way to capture these variations  does it make sense to do that  \nno right so it makes sense to maximize a neuron to be sensitive to variations along u one  \nbut it does not make sense to make the neuron sensitive to variations along this other \ndimension which is u two ok by  doing so we can balance the two conditions  so one \ncondition was trying to capture all  the important variations ok do this but do it only for \nthe dimensions which really matter  the other conditions says that do  not capture \nimportant variations ok do this but do it only for those dimensions which do not matter  \nwhat is this remind you of at least the diagram should have it away right \nstudent refer time sevenseventeen  \nit is same as p rinciple component analysis right so that is exactly what you try to do in  \npca you try to capture the variations across the important dimensions  but not across \nthe non important d imensions h ow many of you get the concept of contractive auto \nencoders ok good so i think that is a where we will end lecture seven  \nrefer slide time seventhirtyeight \n \nand just a quick summary  so we showed that under certain conditions  auto encoders \nare equivalent to pca \nand we use this result very crucially there that svd theorem i will not state it  \nrefer slide time sevenfiftyone \n \nand then we looked at different types of regularizations for auto encoders where we \nlooked at weight decaying  that means the standard ltwo norm we looked at the sparse \nauto encoder the contractive auto encoder  and we also looked at these  denoising auto \nencoders right so that is the summary of this lecture"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.1 Bias and Variance.wav", "duration": 612.46, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  eight \nregularization bias variance tradeoff ltwo regularization early stopping dataset \naugmentation parameter sharing and tying injecting noise at input \nensemble methods dropout \nso in this  lecture we are  going to talk about  a bunch of regularization  techniques for \ndeep neural networks you might find some very familiar terms here for example ltwo \nregularization perhaps something e lse also  but i promise y ou that we will see a very \ndifferent interpretation of this from what you have done in your earlier courses right \nso again as is the trend in this course i will start with some basic  concepts i will take \ntoday\u2019s lecture to finish off  the basic part which is the bias variance  tradeoff and i will \ntry to make it more informative then what you have done in your earlier courses and in \nthe rest of the lecture which will happen on friday we will build upon these basics and \nthen try to look at these as the regularization forms \nrefer slide time zerofiftynine \nso let us start so these are the sources which i have looked at  so one of them is the \nchapter seven from deep learning book  other is this very good lecture by ali ghodsis on \nregularization and of course  this paper on drop out so let us start with  bias and \nvariance a gain some fiveten minutes would be similar to what you have seen in the \nmiddle class but then i will go on to something different \nrefer slide time onetwentytwo \n \nso we will begin with a quick overview of bias variance and the tradeoff between them \nrefer slide time onetwentysix \n \nso let us consider the problem of fitting a curve through a given set of points  ok now \nremember i have always been telling you that there is always this true relation between x \nand y which is f of x right and which we never know \nso we do not know what this is  in the movie example  we do not know what this is in \nthe credit card fraud detection or in the oil mining  example in this particular example i \nknow it right so what i have done is  i know that the true relation between x and y is \nthe sinusoidal curve i know this but instead of giving you every point on this sinusoidal \ncurve what i have done is  i have such sampled some poi nts from it  i have taken some \npoints and given to you \nrefer slide time twoeleven \n \nso from now on we will behave as if w e do not know that this is how it came  it is a big \nsecret and we now want to fit a curve to this  that means i want to learn the fu nction f \nhat of x ok which of course will have some parameters and what will be my goal is that \nnow let us look at this again my goal would be if i feed at this point after the model is \ntrain the output should be as close to th is point as possible  that is our training criteria \neveryone gets this \nrefer slide time twofortyeight \n \nso we consider two models  the first model is a simple model  how many parameters \ndoes it have \nstudent two \ntwo parameters right the other model and this is what happens when i train the simple \nmodel of course i will get a line but do you see something special about this line why \ndid i get this as a line or this as a line  so on average  it is trying to minimize the \ndistance from all the points  if i  have this as the line  then i will have a very high error \nfor these points  right so just something which goes along the average  and hence the \nsum of the squared errors would be minimized right \nso it is important that when you see these figures you should make these connections to \nthe math behind it  so this is the geometry  you have to make connections to the math \nbehind it right and i hope all of you make that connection now i take a complex model \nwhich is a degree twentyfive polynomial ok so this is w one x w two x square w three x cube and so on \nit is a degree twentyfive polynomial that i have used and i again learn the parameters of this \nusing how will you learn the parameters  you have a quiz two days from now on gradient \ndescent \nwhat else do you know  if you know any other algorithm  o f co urse you know  but \ngetting this end right what else will you use  you can use gradient descent for learning  \nthese parameters  the same idea  right you will define a loss  you will compute the \ngradients with respect to all these parameters how many of them are there here twentysix and \njust update those parameters till a fixed number of iterations or any convergence c riteria \nok and this is the curve which i get for the complex model note this in both these cases \nwe are making an assumpti on about how y  is related to x  right in this case  i made a \nsimple assumption in this case  i made a slightly complex assumption  but in both the \ncases we do not know what is true relation is \nthe true relation is actually the sine curve  but we do not know that w e are just making \nan assumption so you remember the five things in machine learning  you have a data  \nyou make an assumption about how the input is related to the output  so these are my \ntwo assumptions then i have some parameters you know the number of parame ters in \nthese cases i use a learning algorithm which happens to be gradient descent and then  i \nminimize an objective function which would be squared error loss in this case fine \nnow the training data actually c onsists of one hundred points ok but you do not see one hundred points \nhere \nrefer slide time fourfiftyeight \n \nso what i have done is  i have sampled some twentyfive points from here and use that as the \ntraining data  so i have learned my parameters w  one and w naught or w  twentyfive up to w  \nnaught using these twentyfive points now i will repeat this experiment k  times what i do is \nevery time i will get a different sample of twentyfive points and i will try to learn the parameters \nof the model  will i get the same curve every time  will i get the same function every \ntime no my parameters would change slightly  right because my training data is \ndifferent \nso i am trying to learn it differently to adjust to that training data  so my function is \ngoing to be different it is the same form it is either the linear function or the polynomial \nfunction but the parameters the coefficients are going to be different \nrefer slide time fivefortythree \n \ni will actually draw these different functions and w e will make some observations from \nthat so this is the black curve that you see is the true sinusoidal curv e from which the \ndata has come  t he blue line is one of these functions which i have trained from one \nrandom sample of the data right \nnow i train different functions from different random  samples of the data and see what \nhappens i  get different lines  this obvious can you relate to this every time  i  am \nbasically learning a different value of w one and w naught is that ok and i have done this \ntwentyfive times and plotted these lines  what do you observe with respect to each of these if \nyou compare any line to any other line \nso if you compare one of these lines to the remaining twentyfour lines what do you  observe \nthey are very close to each other  they are not very different from each other  however \nthere is a p roblem they are very far from dash the actual function  that means we are \nunder fitting  w e have very few parameters  in fact only two that i s why we are under \nfitting let u s look at the other case  fine this is the function  the polynomial the blue \ncurve that you see is the polynomial that i learned from one random sample of the data \nnow i am going to learn this from a different sample of the data you see what happens \nyou see that the green curve is actually very different from the blue curve  you see that \nhere actually this was peaking whereas this is going down similarly this was peaking \nbut this is going down and so on so you see that there are clear differences between the \ntwo curves and if i draw the next curve  you see it is even more different  t he same \nfunction learnt from different data point is turning out to be very different why because \nit is over fitting on those twentyfive points that i have given the simple model did not even have \nthe capacity to do or fit because it is just two parameters \nhow much can i over fit i will just end up drawing  the average line right  but here it is \nreally able to overt fit and you see that these twentyfive curves or i do not know how many \ncurves that i will draw all of these are going to be very different f rom each other  you \nsee that and everyone agrees that this would happen if you actually try to do this ok so \ncomplex models train on different samples of the data are very different from each other  \nwhat is happening there is over fitting ok \nrefer slide time sevenfiftyfour \n \nnow let me define two concepts from statistics one is bias bias is very simple it tells us \nthat this is the true function  if you are trying to learn the approximate function and you \ndo it many times then you will get an expected value of the function so it tells you how \nmuch does this expected  value differ from the true function  ok you get the definition  \nthe definition is straight forward ok now for the simple line or the simple model  the \ngreen line that you see is actually the average of all those twentyfive lines that you had seen ok \nwhat can you say about the bias very high right because this difference is very high \nthis green line is very different from the red curve which is my true function  right \npredicted and true function  now what about complex model  the blue curve that you \nsee is actually the average of all those twentyfive different curves that i had drawn so what is \nthe bias it is v ery low does that make sense  this means that the simple model has a  \nhigh bias and the complex model has a low bias is it clear to everyone \nrefer slide time ninesix \n \nnow let us define another quantity which is variance everyone knows what variance is \nso this is one of the functions that i have learned this is the average of that function \nand the variance tells me the spread  now based on the figu res that you have seen  can \nyou tell me what would happen for the simple model low variance or high variance \nstudent low variance \nlow variance  because all these models were very close to each other  t here was not \nmuch spread in the models what about the complex model \nhigh variance all these models were very far from each other the spread was very high \nok so roughly speaking it tells us how much the different f f act f x i s that you a re \nlearning how different are they from the average f of x \nrefer slide time ninefifty \n \nso informally i can say the following simple model has a high bias  low variance  \ncomplex model has a low bias high variance an d as always going to be a trade off \nbetween the bias and variance \nso why is there always a trade off between the bias and variance  people have done ml \ncourse w hy i s there a trade off how many of you know the mathematical answer to \nthat you have not done this in the ml course no so it turns out that both bias and \nvariance contribute to the mean square error and let us see how"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.10 Ensemble Methods.wav", "duration": 433.24, "text": "deep learningprof mitesh m khapradepartment of computer science and engineeringindian institute of technology madras\nmodule  eighttwolecture  eighttrain error vs test error\nsowewouldstartthenextmodulewherewewilltalkabouttrainingerrorversustesterrorand before that we will see this bias variance tradeoff\nrefer slide time zerotwenty\nsonowwhathavewedonesofarinthesecomplexmodelsandthesimplemodelswehavetrainedthemusingthedashdatatrainingdataandwhatareweinterestedinalwaysa testdatarighti alreadyknowwhatwastheoilamountofoilminedfromthetrainingdatalocationsthati wasgivenandi amnotinterestedinpredictingthosei amlessinterestedinlearningthoseso thatif yougivemea newlocationi shouldbeableto dotherightprediction\nsoi amalwaysinterestedinthetestdatasonowconsideranewpointwhichisnotseenduringthetestdataandthereareseveralsuchpointsthatyoucouldseenowifyouusethemodelf hatxtopredictthevalueofy thenthemeansquareerrorisgivenbyyougetthisit\n\nis justtheexpectedvalueofthissquarederrorthati willgetsowhatistherandomnesshereyexpectedvaluebecausethexthati amgoingtofeedattesttimeisgoingtovaryforeach of these different xs i will get a different error\nsohencethatisarandomvariabledoyougetthatsopleasefocusonthesethingsrightimeanjustdonottakea formulaforgrantedjustseewhatisittryingtoseesowheneveryouseeanexpectationoversomethingalwaysquestionwhatisarandomvariableheresowhatistherandomvariablehereitisthesquarederrorlosswhyisitrandomitisbecauseitchangedtheinputxyouaregoingtotryitoveramultitudeoftestexamplesyouwilltakeone thousandtextexamplestenzerotextexamplesandsoonrightforeachofthisyouwillgetadifferentsquarederror thatis therandomnesssoyouwanttoseewhatistheexpectedvalue of this or very loosely speaking the average value of this now it turns out that this\nnow justtrytorememberthatthisisalsosomeexpectationandyouhadthetermsfxandfxhatherethisalsohadsomeexpectationandtermf xandf xhatandsoonrightifyoudonotremembertheexactformulaitisokbutyoudorememberthereweresomeexpectationsinsideandthetermsf xandf xhatwhethertheyaresothisisjustsimpleyouaredealingwith a minus b the whole square on the left hand side\nifyouifyouopenituprearrangesometermsyouwillgetthisrightsoyoucanshowthatthemeanaverageortheexpectedsquareerroronthetestdataisactuallythebiassquareplusthevariancethatisasmallamountofirreducibleerror youcangobackandworkthisoutandactuallytheproofisgivenhereonthelinkokbutihopeyougettheintuitionyouhavethis a minus b the whole square\nifyouopenitupandrearrangethetermsyoushouldbeabletogetthisnowwhatdoesthistellyouwhathappensif thebiasis highthesquarederrorisgoingtobehighwhathappensisif thevarianceishighit isgoingtobehighsothatiswhyyoudonotwantaveryhighbiasyoudonotwanta veryhighvariancealsoyouwantthissweetspotinbetweenwherethebiasandvariancearejustaboutoptimalyougetthatthatiswhythereisa tradeoff between bias and variance\nyoucannotrelyonsimplemodelswhichhavehighbiasyoucannotrelyoncomplexmodelswhich have high variance you want something in between\nrefer slide time threeeighteen\nnow theparametersoff hatxrememberthattheyaretrainedusingthetrainingdatawhichconsistsoftheseendpointsthatyouhaveattesttimeweareinterestedinevaluatingthemodelona validationsetwhichwasdifferentfromthetrainingdatathisgivesrisetothefollowingtwoquantitiesoneisthetrainingerrorwhichyoudealwithatdashtimetrainingtimethatis theerrorthatyouaretryingtominimizerightbuta testtimeyouhaveadifferent error which is the test error and that is the error that you care about\ntypicallythesetwoerrorsexhibitacertaintrenddoyouknowwhatthetrendisnow onthex axisi havemodelcomplexityandontheyaxisi haveerror asamodelcomplexityincreaseswhatwouldhappentothetrainingerrorit willgotoalmostzerothatisexactlywhathappenedfromthelinearfunctionto thepolynomialfunctionthisis howit willbehaveasthemodelcomplexityincreasesasthemodelcomplexityincreaseswhatwouldhappentothevalidationerrorit willdecreaseuptoa certainpointrightbecauseyouarestill not over fitting on the training data your answers are still generalized\nsoyouhadthisdegreeone polynomialdegreetwentyfive polynomialif i takein somethinginbetweenthenprobablythisiswherei wouldhaveendedupwiththetrainingerrorandthat\n\nwouldnothavebeentoobadforthetesterror youseethisoknowyouseeiwillmarktwopointstworegionsratheroneofthiscorrespondstohighbiastheotheronecorrespondstohighvariancetellmewhichoneiswhichdothisi cannotunderstandsoletmeaskthisis this is ok\ngoodsoyouseethattherearethesetwoextremeandwewantsomewhereto beinbetweenokatleastyougettheintuitionbehindthisfineokandyouarelookingforthissweetsspotwhichistheperfecttradeoff betweenthebiasandthevariancerightsonoweveryonegetswhythereisatradeoffandhowthisrelatestomodelcomplexityandthereforewe arelookingfor theidealmodelcomplexity howdo we achievetheidealmodelcomplexitywellwecannotreally idealisidealbutwetrytodothisusingdashwhatisthe title of this lecture\nstudent regularization\nregularizationi willtrytouseregularizationtoachievethisoksoletusformalizethisabitmoreandrememberthatthiscurveisactuallybecauseofthisequationthatyouseerighthigh bias you will be in this region i am actually inserting it ok fine ok\nrefer slide time fivefifty\n\nsotheintuitionsthatwehavedevelopedsofaristhatiftherearentrainingpointsandmtestpointsthenwehavea trainerrorwhichgoesoverthetrainingpointsandwehavea testerror which goes over the m test points ok\nsoi amjusttakingatotalofnplusmpointsthefirstnistrainingthenextlastmistestnow asthemodelcomplexityincreaseswhathappenstothetrainingerroritbecomesveryoptimisticandgivesyoua verywrongpictureofhowclosethepredictedfunctionistothetruefunctionwhetherit makesyoufeelthatyouhavedonea perfectjobthisyouhaveactuallydiscoveredthetruefunctionbutthatisnotcorrectitisgivingyouafalsepictureofthat therefore we should always look at the dash error\nstudent validation error\nvalidationerrorsonowyouseethatwhyyoualwaysdothistrainvalidationandtestsplittestisunseenyoutrytooptimizeonthetrainingerrorokbutyoushouldalwaystuneforthevalidationerror youroptimizationalgorithmisgoingtotakeinthetrainingerroritisgoingtobeveryoptimisticitisgoingtotrytodrivetozerobutyoushouldlookatthevalidationerrorand try to see that you are not over fitting on the training data everyone gets this intuition\nrefer slide time sevennine\nsonowthisisallintuitionwewillhavetoconcretizethismathematically sothatiswhatwewilldonow sothatd bethesetrainingtestpointsthatwehaveweknowthatthis\n\nrelationshipholdswe donotknowwhatf isbutweknowthatthisrelationshipholdssowhatami tryingtosayherethatweknowthatthereisatruerelationbetweenyandxwhichisgivenbythefunctionfbuti amalsowillingtoadmitsomenoisethatmaynotbeaveryneat function but a small noise might exist\nthatis theepsiloni ok andi amgoingto assumethatepsiloncomesfroma normaldistributionwithzeromeanssoonaveragethenoiseisgoingtobezerobutthereisasmallvarianceeveryonegetsthisthisisatruerelationbuti amwillingtoadmitsomenoiseintherelationokfineandofcoursewedonotknowfweneverknowfrightnow goingbyourparadigmwherewehavethesefivecomponentsweusef hattoapproximateffhatwillhavesomedashwhichwillitrytolearnfromthetrainingdatawhatisthisdashparametersrightwhichwilltrytolearnfromthetrainingdatathetrainingdatatisasubsetofyourtotaldatawhichisthusthoseendpointsrightandweareinterestedinknowingthisquantity thisiswhatweareactuallyinterestedincanwecomputethisquantityhowmanyofyousayyes how many of you say no we cannot why cannot we compute it\nwedonotknowfsowhycannotyouraiseyourhandsifyouallcananswerinchorussowedonotknowwhatf is thenhowdowecomputethisquantityrightbutwhatdoweactuallyknow sonowwe aregoingto seesomethingwhichis trueexpectationandsomethingwhichisempiricalestimateexpectationhowmanyofyouknowthiswhatisthedifferencebetweenthetwomostofyoushouldbutitisnotconfidentaboutitoksowedonotknowwhatfxiisthetruethingbutwhatdoweknow wearegivensometrainingdataright\nwe know these yi\u2019s for was training data and we know these yi hats for those training data\nrefer slide time nineeleven\nsothisissomethingthatwecanestimateyesornothisisgiventoussothisexpectationisgoingtobeanempiricalestimaterightbecausewearegoingtolookatsometen trillion one billion twenty thousandtrainingpointsandestimatethisrightitisanempiricalestimatehowmanyofyougetthatnow i amjustgoingtorewritesomeofthissowhatihavedoneisijustdefinedthatyiisequaltofxiplusepsilonisoi havejustreplacedyibythatokisthatfinenowthisisoftheformaminusbthewholesquaresoiamgoingtotreatitasthatandjustopenupthebracketsoi willhaveasquareminustwoabplusbsquareandnow thisisasumordifferenceofexpectationsoi canpushtheexpectationinsidesothisiswhati getthisisthis fine ok\nnow i amjustgoingtorearrangethetermsorememberthiswasthequantitythatwewereactuallyinterestedinbutthisisthequantitythatwehadahandleoverbecausethesewerethedatapointsgiventoussoi willjustrearrangethetermsandicanwritethiswhichwasmyquantityofinterestasthiscanyouestimateeverythingonlhsonrhsthisthiswhatisthisvariancesigmasquareweassumedit camefromzerosigmasquaredistributionandthis can estimate the answer is no for the same reason we do not know what f of x is ok"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.11 Dropout.wav", "duration": 956.16, "text": "deep learningprof mitesh m khapradepartment of computer science and engineeringindian institute oftechnology madras\nlecture  eighttrain error vs test error recap\nrefer slide time zerotwelve\nsowespokeaboutbiasandvarianceandwesawthatsimplemodelshaveahighbiasbutlowvarianceandcomplexmodelshavea lowbiashighvarianceandsoonandwesawitsome illustrativeexamples that what that is whatthatmeans\n\nrefer slide time zeroeighteen\nand then the important thing tonote was these twoformal definitions ofbias\nrefer slide time zerotwentysix\nandformaldefinitionofvariancewhichyouallknowanywaysandthentheimportantconcept that wespoke about was the train error versustest error right\n\nrefer slide time zerothirtysix\nsothiswasthecurvethatwewereinterestedinandonecornerofthiscurvewasrelatedtohighbias low variance and the other corner wasrelatedto low bias high variance right\nrefer slide time zerothirtynine\nsoi amlookingforsomethinginthemiddlethatiswhatourquestisinthislecturerightand we want to findways of falling somewhere in middle\n\nrefer slide time onesix\nandthisledtothedefinitionoftwoquantitiesofinterestortrainingerrorandtesterrorssotrainingerroriscomputedfromthetrainingpointsthesearethepointsthatyouactuallylookatwhileyouaresolvingthisoptimizationproblemsothetrainingalwaysinvolvessolvinganoptimizationproblemwhichistheobjectivethatyouwanttooptimizeormaximizeandthe test error is somethingthat you want to use itfor at eventually\nsoyouallhavethesetwoquantitiesofinterestthatwedesignandwerealizethatthetrainingerrorismoreoptimisticwhetherthetesterrorsactuallygivesustherealpictureofwhatwedoandwetiedthosebacktothingsthatyouhavedonepreviouslyinthemachinelearningorothercoursesthatwealwayssplitthedataintotrainingvalidandtesttrainingit onthetrainingdatadosomevalidationsonthevalidationdatabutneverlookatthetestdatathatis forthe final evaluation\nsothat\u2019s thethisisthisintuitionwhichi havebeentryingtobuildwiththesetwocurvesisthe explanation for why we do thingsthat way\n\nrefer slide time twothree\nnow weareinterestedindoinga moremathematicallyrigorousanalysisofthisintuitionrightsothatiswhereweleftoff sowhatweareinterestedinsonowi willjuststartfromthispointisthatwearegivensomedatawhichismnmtrainingpointsandntestingpointsandweknowthatthereisatruefunctionbetweentheoutputsandtheinputsandwearealsoexpectingoracceptingsomenoiseinthisrelationjustasinanyotherrelationsowhichmeansthatyisrelatedtoxibutbysometruefunctionbutthereisalsothisnoiseandforsimplicityweassumedasthisnoisecomesfromanormaldistributionwithzeromeanandsomesmallvarianceandasusualweneverknowfrightbutwearetryingtoapproximatethisf hatandwecomeupwithsomeparametricformforf hatandthentrytolearntheparameters of fhat from the training subset of thedatathat is given to us\nsothisiswhatwealwaysdoandwehavealreadyseendifferentvariationsoff hatoneofthembeingthedeepneuralnetworkandwhatweareactuallyinterestedinisthisquantity theexpecteddifferenceorsquaredifferencebetweenthepredictionsmadebyourmodelandthetruevalueoftheoutputwithrespecttothetruefunctionrightthenweaskedi askedyouwhetherwecanactuallyestimatethisquantityandallofyousaidnowhyitisbecauseyoudo notknow what f of xi is right so we will seehow to estimate this empirically\n\nrefer slide time threefortyfour\nsothenwestartedoff withthisinformationthatwehaveweknowwhatyi hatisbecausethatisthepredictionthatwemakeandweknowyiwhatyiiswedonotknowthefunctionbutweseetheoutputofthefunctionintheformofthetrainingdatapointsgiventousoranydata points given to us\nsowewrotethisbymakingthisparticularsubstitutionwherewenoticethatyithatweseeisactuallythetruefunctionplussomenoiseandthenwedidsometrickeryandtrytosimplifythisandthenwejustrealizethatthisisthetermthatweareinterestedinsowemovedittotheothersideoftheequationandcameupwiththisneatlefthandsideorneatrighthandside thatwe need to analyze now so fareverythingis clear\nthisiswhereweendedthelastclassrightyoujustwenttoitveryquickly buti assumeeverythingisclearatthispointokfinesoweareleftwithabunchofexpectationsrightandwehavei amassumingwehavenocluehowtoestimatethisrighti meansoandrememberthatwhenyouaredealingwithexpectationsasalwaysthistrueexpectationandthenthereisthisempiricalestimationrightsowhatwearegoingtomovetowardssotheseall equationswheni writee herecapitale herei amtalkingaboutthetrueexpectation\n\nnow wewillseehowtoapproximatethetrueexpectationwithanempiricalexpectationandthen basedon that we will make some observations\nrefer slide time fivetwelve\nso that is what we will do now\nrefer slide time fivefourteen\nsowewilljusttakea smalld two andi willjusttellyouwhatexpectationsareorwhatempiricallyexpectationishowtocomputethemsosupposewehaveobservedthegoals\n\nscoredink matchesthereissomekfootballmatchesthatwehaveseenandwehaveseenthat the goals scored werethe following\nnow ifi askedqwhatistheexpectedvalueofthegoalnow thenumberofgoalsforwhatwillyoudotaketheaverageofthisthisiswhatyouwilldosowhatisitthatyouaredoinghereyouaretakinga dashestimateoftheexpectationempiricalestimateyouaremakingsomeobservationsthesearetheobservationsgiventoyouthesearethekmatcheswatchasmuchasmanyfootballmatchesasyouwantafterthesemesterendsandthennoticethenumberof goalsthatwerescoredin themandthenyoucancomputethisexpectationrightandthisishowyoudoempirically sothereissomethingthatwedoonaregularbasisbuti justwantyoutorealizethatwhatyouaredoingisactuallyanimplicitestimateof the true expectation\nnow canyourelatethisto thequantitythatweareinterestedinwe areinterestedincomputingacertainexpectationwhichisthiscanyoutakeananalogyandtellmehowyouwoulddothisthehintiswehavedonethisamilliontimesinthecoursealready finesothisishowwewilldoitandhaveactuallydonethisamilliontimesinthecoursesowhenyou computethis we are actually doing an empiricalestimate of the data\nsoletusjusttakeaminutetounderstandthiswearegivensomedataweareinterestedinthistoexpectationwhichwecannotcomputesowewilltakethisdatawewillassumethereisenoughofthiswearegivenmsampleswhichareenoughandfromthatwewillmakeanempiricalestimateandjustasinthecaseofthesegoalscoredrightasyouseemoreandmorematchesyouwillhaveabetterunderstandingofhowmanygoalscanbescoredwhentwoparticularteamsareplayinginthesameanalogygoeshereasyouseemoreandmoredata your estimate would become better butthatis how you will do theestimation\nsonowwewillcomebacktosonowdonotgetsurprisedwheni amgoingtoreplaceallthesee\u2019s bythisallthee\u2019s thatwehadinouroriginalequationi amgoingtoreplacethembythese summations ok fine\nrefer slide time seventwentytwo\nsothiswasouroriginalequationthatwehadderivedandwewereinterestedinthislefthandsidequantitywhichisasumofsometermsontherighthandsidesonowthisexpectationitoldyouthatwecanestimateitfromdatabutwhichdatatrainingdataortestdatabothsowewilltrytoestimateitfrombothandseeifthereisanydifferencewhichariseswhenyou estimate it from one data andthe other data ok\nsothefirstthingthatiamgoingtodoisiamgoingtousetestobservationstoestimatethissocanyoutellmewhataremysummationsgoingtolooklikeitissummationovernplusonetonplusmrightweassumethatthefirstendpointsaretrainingpointsandtheremainingpoints are testpoints\nsothequantityonthelefthandsideistrueerrorrememberthatbecausethathasfxwhichwedonotknowquantityontherightsidethefirstthingisempiricalestimationoftheerrorokthesecondthingisasmallconstanthowever theepsiloni squareandweassumethatcomesfromanormaldistributionwithasmallvariancewhatisthethirdquantityactuallyihavegivenyoutheansweralready buti wantyoutothinkaboutiti amsayingit isthecovariance between two things\n\nwheni sayit isthecovariancebetweentwothingswhatisthefirstthingthati needtoproveisthatthetwothingsaredashrandomvariablesi meanfirstthingweneedtoseeisthat the two things arerandom variables epsilon iclear it is arandom variable\nwhataboutthisotherthingorratherepsilonisarandomvariablewhatabouttheotherthinganddependingonthetraininginstancethatyouhavesampledthisongoingdifferenceisgoingtodiffer rightyouarehavingyourtrainingortestinstancewhateveristhisxithisisgoingto differbecausethesex\u2019s aredifferenttheyareallrandomvariables sothereisdifferencebetweenthesetwoquantitiesalsogoingtobearandomvariableisthatfineokbut still isthis the\nsotheni havetoldyouthisis xandthisisyandwhati amsayingis thatthecovariancebetween x and yis just e of x x into y is that correct\nrefer slide time ninethirtyfour\nthatis howyoudefineco variancewhatis thedefinitionofcovarianceif youhavebotheredtolookattheprerequisitesnoexpectationintheformofesocovarianceiseofxminusmuofxintoyminusmuofphiwhatisourxepsilonandwhatisourywhatismuofx zero\n\nsoi willjustsimplifythisabitoki willopenuptheproductwhatismuofyintoeofxwhat is eof x what is the expected value of thenoise zero\nrefer slide time tenone\nsothenthisturnsouttobeasthatisthatfinethatiswhywearewritingthecovarianceisjust theproduct of the two things\nsolet us justtakea minuteto againunderstandthisthetrueerroris theempiricalestimationoftheerrorplusi meanplusorminusasmallconstantokandthenthisnastyquantitythatwedonotknowwhattodowithitsoletuslookatthisquantityandseewhatwe can say about it\n\nrefer slide time tenthirty\nnow whatisthecovariancebetweenthesetwoi amtryingtocomputethisexpectationfromthetestdatajustrememberthat soeachihereisatestinstancearethesetworandomvariablesdependentorindependentisthequestionthati amtryingtoaskitisindependentsoletuslookatitpiecewisesorememberthatwehadsaidthatyisequaltofofxiplusepsilonirightthisepsiloni hadnorelationtof ofxii meani couldchooseanyxibutthis noise isgoing to be random so there is norelation between these two\nnow istherearelationbetweenfhatofxiandepsiloniwearedoingtestssohowdidwecomeupwithf hatofx ihowdidwheni sayhowdidwecomeupwithf hatis i meanhowdidwelearntheparametersofafhatusingthetrainingdataandwhatarewecomputingexpectationwithrespectto nowtestdatathesetheseepsilonimproveinfluencetheparametersthatwe hadlearnedfurtherfromthetrainingdatano sincethereis nodependence betweenthese two guys\nsothatis whyepsiloni is independentoftheotherrandomvariablethatyouseeinthisexpectationisthatcleardoyougettheintuitionf hatxi furtherno butthisisthemeanthisnoiseiswhatis presentinthetestdataandyouhavenotseenthisaddtrainingtimewhenyouaretrainingtheparametersyoudidnotlookatthisnoiseyouarelookingatthenoise in the trainingdata\n\nsothisisnotparticipatedintheestimationoftheparametersoff hat butthatwasforthetraining dataright but this now i am doing theexpectationfrom a test data\nrefer slide time twelveten\nsothesetworandomvariablesareindependentthatmeansi canwritethisasisthisfinewhatwillhappentothiszerooksowhatdidweeventuallyconcludethatthetrueerrorisequalto empirical test error plus asmall constant right\nsowhatdoesthistellyounow tellmeforgetthemathtellmeinenglishrightwhatdoesthistakewhatdoesthismeancanyourelateittonowwhyyoudothistrainingerrorvalidationerror testerrorsowhatdoesthistellmethistellsmethatifi havetrainedamodelandnowif i takeanestimateoftheerroronsomedatawhichi hadnotusedforthetrainingthenthaterrorwhichi seeisactuallyveryclosetothetrueerror itonlydiffersbythis small constant\nhowmanyofyougetthatthatiswhywheni lookatthevalidationerror itisnotbeingoverlyoptimisticitisgivingmeatruepictureofwhattheactualerrorisrightsotherearetwothingsthatyouneedtounderstandhereonethisisthequantitythatweareinterestedinwhichwecannotestimatewearetryingtoestimateitbyusingthiswearetryingtomakeanapproximationsowearetryingtoseehowgoodthisapproximationiswhatthisderivationistellingusisthatif youareapproximatedit usingthetesterrororthetestdatathenthis\n\napproximationis actuallyverycloseto thetrueerrorandhowcloseit isactuallyit justdiffers by this smallconstant\nsoyougettheimportanceofwhatweareseeinghererightoknowtotrulyappreciatethisi needto tellyouwhatwouldhavehappenedif youhadusedthetrainingdataforthisestimationrightit is largelydependentbutthatis againa normalassumptionthatyoumakesothisisokgoodthatyouaskedatthispointi willbedoinga coupleofthingstodaywherewe will be derivingsomethingswe will try to provesomethingsmathematically but allof these would have underlyingsomeassumptions\nsoifyouremembertheadamderivationwiththiswedidtherealsowehadmadethisfunnyassumptionthatthegradientsareactuallycomingfroma stationarydistributionwhichwillnothappeninpracticesothisremindsmeofthisjokefrombigbangtheory whichsaysthatihaveasolutionbutitonlyworksforsquaredeggsinavacuumrightsoitisbasicallyallthesethingsalwayshavesomeassumptionsunderlyingthembuttheideaistokindofignorethoseassumptionsandseewhathappensinaneatsettingandatleastseewhetherinaneat setting everythingworks fine or not\nsothatiswhatis happeningheresoisavalidpointthatyouareassumingthatthenoisecomesfroma zeromeandistributionnow if thenoisedidnotcomefroma zeromeandistributionthenthiswouldhavenotgonedowntozeroandthemeanwouldhavebeenhigherthanthisisnolongerasmallconstantandsoonsothosethingsaretheresothisisgoingtohappeninsomeoftheotherderivationsthati dotoday itisnotthati amteachingyousomethingwrongitisjustthatyouhavetotakeitwithapinchofsaltinthesensethattheseassumptionsarethereandtheoriginalderivationsthesearenotmyassumptionsandthey work onlyunder those assumptions\nsoyouhavetobecarefulaboutthatbuttheideaisthatstillwiththeseassumptionscanweatleastmakesomethingmeaningfuloutofitrightisthatfinewitheveryonecanweallworkwiththatbasicpremisesowhati havedoneso faris toldyouthatif youareestimating the errors from thevalidation data youare doing a good job\nrefer slide time fifteenthirtyseven\nnow letusseeifi wouldestimatetheerrorfromthetrainingdatatakeaguesswhatwouldhappenwhatwouldmyargumentforthisbenow thiswillnotdisappear rightbecausethesetwoarenotindependentnow i cannotwriteit asaproductoftwoexpectationsthatmeans it will not go down to zero so that is the argument which i amgoing to make\nsohenceactuallythetrueerrorifyouseerightitisequaltotheempiricalestimationplussomequantity thatmeansthetrueerrorisdashascomparedtotheempiricalerror thatmeanstheempiricalerrorthatweseeispessimisticoroptimisticoptimisticthatiswhatistartedwiththatyougaveaveryoptimisticestimationofyourerrorifyouarelookingatthisempiricalestimationfromthetrainingdatabecauseyouhaveignoredthisquantity isitfineso whatis missing in the story\nletusseenowwhatwasthisquantity sofarallourdiscussionsl thetarightbutnowsuddenlyi haverealizedthatmytrueerrorisactuallylthetaplussomethingelserightyouseewherei amheadedwiththisoksothatiswhatweneedtoseenowoknowthinkitwouldbeweshouldbutiamprettysureitispositiveicannotworkitoutrightnowbutiampretty sure it is positive and you cansee and ifyou find it is not then let meknow\n\nrefer slide time seventeennine\nsohowis allthisrelatedto modelcomplexitywe startedoff withthisideathatmodelcomplexitytellsyouhowmuchisthebiashowmuchisthevarianceandbecauseofthatyougetthesetwocurvesthatyouarenothappywithonecurvebeingveryoptimisticandtheothercurvebeinga bit pessimisticnow howdoesthisdiscussiontie up to modelcomplexity"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.2 (Part-2) Train error vs Test error (Recap).wav", "duration": 1040.4, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  eightthree \nlecture  eight \ntrue error and model complexity \nso now we will try to see that how does this true error tha t we see depend on the model \ncomplexity \nrefer slide time zeronineteen \n \nso using steins lemma and some  trickery we can show  the following what is steins \nlemma so i had this deal with my students last year  you do not ask me what steins \nlemma is i will not ask you what steins lemma is ok so it is some lemma which tells \nus that this quantity what was this quantity the last  term which was  troublesome rate \nthat covariance  term which was  troublesome that i s this quantity  t his quantity is \nactually equal to this quantity  \nso let us buy that let us all of us agree  that steins lemma is correct and it  tells us that \nthis is the case ok and you saw the quiz one paper ok fine from last year i mean ok so \nnow we will work with this premise and we will see what it actua lly tells us now when \nwill this quantity be high  so what this is telling us i mean jokes apart  let us try to \nfocus again that this quantity is actually equal to the summation of this quantity \nnow let us take one term in  this summation when would  dou f hat x  i by dou y i be \nlarge w hat does it actually  tell you  if i  change one of  these y i\u2019s a bit when  the \nprediction for it is going to change by a lot do you get that how many of you get this \nsome of you do not get this just think about it when would this be high what does the \nderivative capture  i f the derivative is high  that means a small change in  the \ndenominator is going to lead to a large change in the numerator  \nwhat is the denominator  a ctually the true y  that we have observed  w hat is the \nnumerator that is t he predicted y  so what you are saying is  that if  there is a small \nchange in y i then there is going to be a large change in  the prediction ok when would \nthis happen  would this happen for simple models or complex models  complex \nmodels how many of you say complex models  so this is the link to model complexity \nrate and i will make a more intuitive case for this but at least some of you get this that if \nyour model is very complex  that means it is even one of your data points changes and \nthe prediction of the model is going to change largely \nso now relate this back to that sinusoidal model  that we had and we had  this complex \nmodel every model  that i was training which was strained on a different set of twentyfive \nexamples the model was vastly different and that  is exactly what was happening  when \nyou were changing even one data point  your predictions were changing largely  t hat \nmeans your model was changing largely  d o you get  that intuition  so indeed a \ncomplex model will be mo re sensitive  to the changes  in the observation whereas a \nsimple model will be less sensitive  to it  and hence  we can say  that the true error is \nactually equal to the empi rical t rain error plus something which relates  to the model \ncomplexity \nrefer slide time threeseventeen \n \nnow let us first verify  that indeed a complex model is more sensitive  to minor changes \nin the data so this is some data  that i had sampled from  the same distribution and  i \ntrained one simple model which is  the green line which you see  that was a linear model \nand i trained one complex model which was a  twentyfive degree polynomial which you see  ok \nnow what i am going to do is i am going to take one of these points and change it a bit \nand i retrain the model  \nwhat happens to the simple model  it does not change much  but what happens to the \ncomplex model it is more sensitive to these observations that i have and that is exactly \nthe quantity  that we were interested in  th at means a complex for a complex model \nwhich is more sensitive  that summ ation that we care about is going  to be high  t hat \nmeans that difference between the true error and the estimated error is going to be high \nrefer slide time fourten \n \nso that is  why instead of minimizing  the train error  we should always minimize  the \ntrain error plus some quantity which is linked  to the model complexity this is the basis \nfor all dash methods regularization method \nso now you see where  this comes from  so ok where omega theta would be high for \ncomplex models and simple for simple mod els ok you get the intuition for  this and the \nrest of the lecture we will spend in taking various cases where we will actually show that \nomega theta would be high and we are  trying to control f or omega theta this quantity \nfor the rest of  this lecture an d for the rest of  this course i will assume that we all know \nhow to deal with  \nwe have done enough of  this we have done a lot of back propagation  we have done \nenough derivations of  the laws with respect  to the output layer and  so on everything  \nright so all of us understand how to deal with l train theta where l train theta is this l \nequal to one to m squared error loss or your log likelihood or any of  these losses right so \nwe all know how to deal with this today we are going to focus on this other term which \nbrings in the regularization \nrefer slide time fivefifteen \n \nso what omega theta does is actually acts as an approximation for this so what i should \nhave actually tried to minimize i s not just l train theta but l train theta plus this other \nquantity which was there in my equation you get this my true equation was that my loss \nis equal  to l trained theta plus  this term right which we approximated using steins \nlemma so i should have  tried to minimize  this quantity but i do not know how  to \nreally compute this quantity  \nso i am going to just substitute i t by o mega theta and ensure  that omega theta is such  \nthat it is high for complex models and low for simple models  do you get  the recipe \neveryone gets this how many of you understand this fine so we can show  that lone \nregulation ltwo regularization early stopping all of  these are actually  special cases of  this \nparticular formulation that we have \nrefer slide time sixnine \n \nand remember that this is  the sweet spot  that we were aiming for  ok and this gap is \nactually this quantity because we are making a very optimistic estimation of  the error \nwhereas there is actually this quantity which we have been ignoring and  that is why we \nsee that the validation error is high ok so is the full picture in terms of the diagram and \nall the equations that we have seen \nso we should ensure using o mega theta that this gap is also minimized  therefore our \nfunction should be minimized l theta plus o mega theta so  essentially what we are  \ntrying to do is minimize this gap and hence the model would generalize better on the test \ndata is this intuition clear to everyone \nrefer slide time sixfiftyseven \n \nwhy do we care about this bias variance tradeoff model complexity this is not a course \non machine  learning t hey are highly complex models  they have many parameters  \nmany nonlinearities in fact now can you relate this back to the universal approximation \ntheorem what is the universal approximation theorem say give me any data  i will give \nyou a deep neural netwo rk which will exactly over  fit the data right and that is exactly \nwhat we want  to avoid that is why regularization is important in  the context of deep \nneural networks fine it is very easy for them to over fit the data and derive training error \nequal to zero and that is why we need some regularization \nrefer slide time seventhirtyfive \n \nso today we are going  to look at different forms o f regularization starting with  ltwo \nregularization some simple tricks so some of  these are going  to be mathematically \nmotivated some of these are just going  to be heuristics or empirical stuff  so data set \naugmentation is one such empirical stuf f how many of you tried data set augmentation \nfor the immunized assignment or the back propagation as parameter sharing and  tying is \nsomething that no i am not  \nplease do not give me that look yeah i am not suggesting that adding noise the inputs \nadding noise to the outputs early stopping ensemble methods and drop off right so \nthese are the things that we are going to talk about this and all of this is in the context of \nregularization where you want to avoid some kind of model complexity"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.2 Train error vs Test error.wav", "duration": 643.98, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  eightfour \nlecture \u2013 eight \nltwo regularization \n refer slide time zerothirteen \nso let us start with ltwo regularization so i have seen this before \nrefer slide time zerofifteen \n\nso all of you see that  this is ltwo regularization right what does ltwo regularization does \nnow tell me in the context of things that we have discussed today what is this empirical \nestimate of the train error  ok and what is this  is that fine right  so everything that we \nare going to write is l  because of its w  but fine right ok  now why does this relate to \nmodel complexity what am i doing here actually by adding this \nso they are going to see a very detailed anal ysis of this  but  i just want to see first \nwhether you get an intuition behind this so by doing that what you are trying to do not \nallow the model to become very complex  right you do not want a model where your \nweights can take any possible value  you just want the weights to be small  so you are \nreducing the freedom on the model right less freedom less complex you get the intuition \nat least we will see this in more detail  but at least you get the intuition why we are \ndoing this \nso we are using omega remember that we are using this omega theta  as a surrogate for \nmodel complexity  so if you add something in all omega theta  just make sure you \nunderstand that this relates to model complexity ok fine and now for sgd what would i \nneed for gradient  descent just in case you have forgotten what sgd is  what do we \nneed nothing we have done it fl gradient of this which is a sum of the derivatives \nof the two quantities of which you know one  right you know this already  and what is \nthe other guy alpha w right \nrefer slide time onefiftynine  \n \nso you see this  ltwo regularization right one reason why it is preferred is now imagine \nyou have already written code for gradient descent all you need to do is change it at one \nplace add this to your update rule  that is all you need and you can think of the vector \nform of this where you have a vector of parameters  you can think of the matrix form of \nthis variable vector  matrix of parameters  all you need to do is add one term to your \nupdate rule  so  it can be done with very minimalistic change and this would be your \nupdate rule now let us see geometric interpretation of this \nrefer slide time twothirtysix \n \nnow from here onwards some of you will start getting a bit uncomfortable with some of \nthe math because of these assumptions that it only works for squared eggs in a vacuum  \nright so you will see those kind of things  i will not tell you upfront what is the \nassumption i am making because that will just spoil the analysis  you will just not enjoy \nit as much a s you would ignorance  is bliss  right so if you do not  know what the \nassumptions are you will probably enjoy it more \nbut for some of you will pick it up just keep it to yourself at the end i will tell you what \nare the assumptions  i had made  ok there are some tricky assumptions that  i want to \nmake but just live with it and just try to enjoy it while those assumptions last right ok  \nso now let us assume that w star is the optimal solution for  l w what is l w the train \nerror not our regularized error just the train error \nand so if w star is the optimal solution what can you take tell about the derivative with \nrespect to w star or derivative at w star sorry  it is going to be zero from basic calculus  \nright so which i say minimize x square  the minima is where derivative of x squared \nwith respect to x is equal to zero right \nso now consider one point which is ok  so what i actually want to consider is that  let \nme just see how to see this so let us see my w star ok and i want to consider some point \nin the neighborhood of w star ok that is what i want do so one way of saying it is that h \nis equal to w minus w star  is that fine ok  so that is what i am going to use in the next \nfew steps \nrefer slide time foursixteen \n \nso suppose i have such an h whi ch is equal to w minus w star  that means i can move \nfrom w star to some point in its neighborhood by using h  and what does taylor series \ntell us  this is what taylor series tells us right  that the value of the function at this \nneighborhood point is equ al to this  all of you know taylor series well no w it is that \nfine i do not need to really go over this right \nthis is approximation up to the second term  second order derivative  now what was h \nactually w minus w star so i will just substitute that and  this is what i get is that fine \nwhat is this quantity  one minus zero infinity minus infinity zero right we just did that ok  so \nthat term will disappear what am i left with this quantity ok and i have forgotten what \nis next \nnow again i am interested in th e derivative of this ok  so what will happen if i take the \nderivative what would i get i am interested in computing grad l w what will the r h s \nbe how many of you fine with this  remember this is a quadratic form right so this is \nof the form x squar e that is i mean that is  roughly how  i remember it is not correct  \nbecause of the form x square  so when you take the derivative  one of the x is will \ndisappear and this quantity will remain ok so everyone gets this ok \nso now what do i have is i have the formula for the gradient with respect to  l w and it \nis in terms of the gradient with respect to  or rather the gradient at l w star that is what i \nhave achieved so far but what am i actually interested in  the regularized loss i am \nwhat i am stil l dealing with  is the non regularized loss  this is just the empirical \nestimate of the training error that is not what i am interested in  i am interested in the \nregularized loss \nhow many of you lost at this point   h is the second order derivative oh  so these are \nbrackets just for clarity  but i see it is making it more unclear  yeah actually we should \nhave used u and then call it u transpose h u so it is the brackets here are not indicating \nfunction ok this is just h transpose h now let us say it  i realize how bad it is  so last \nstep what are we taking gradients with respect to is w right is it fine \nso we have a so i mean do not get too confused right so up till this point we have a \nformula for l w right and i am just interested in the derivative of that ok and all i have \nachieved by this is that i have ok in fact i have one more step right \nrefer slide time sixfifty \n \nwhat is this quantity zero ok so we now know that the derivative of the loss function \nwith respect to w can be written  as this quantity is it ok and i have just derived it step \nby step there is nothing great about it  anyone is can why i am doing this is not clear \nthat will become clear hopefully  but what i am doing is clear right  is that fine can  i \nmove ahead  \nnow what we are actually interested in is this quantity  because this is the true loss that \nwe are going to deal with right and we just saw in the previous slide that this quantity \nwhich is on the  l h s is equal to this thing on the r h s  this is what we saw  on the \nprevious slide can i just go back to the previous slide  because the derivative of this was \njust alpha w  now let us start with this  so on the next slide  let me just see if there is \nanything else that i need to see here ok \nso far everyone is clear what i have derived so far why is not clear  but what is clear  \nwhat is being derived so far so i have said that the derivative of the loss function or the \nregular is loss function can be written as this quantity ok  is that fine where w star is the \noptimal solution for with respect to the un regularized loss  function ok and now i have \nwhat i am interested in this solution with respect to the regularized loss function ok \nrefer slide time eighteight \n \nnow let w tilde be that solution for the regula rized loss function so that means the \nderivative of the loss  the regularized loss function at w tilde is going to be zero nothing \ngreat about this but i just told you on the previous slide that  i can write this quantity as \nthis quantity that is what we derived on the previous slide ok  just take my word that is \nwhat we derived on the previous slide ok  let just no confidence in me  ok that is fine \nnow can you are you if i write it as this just rearranging some terms oh sorry \nso i am just grouping al l the w tilde  some terms and this is  a matrix is needed here \nright because i need to i can only add two matrices so what i am just doing is putting \nthe elements across the diagonal everyone understands this everyone gets this step ok \nrefer slide time ninenineteen \n \nso now i have a formula for w tilde in terms of w star ok  i am going to go a bit further \nand be a bit bold and compute the inverse also  so now  i have a exact formula for w \ntilde in terms of w star  so what is this actually what is th is relation that i am trying to \nestablish suppose i know the solution with respect to the un regularized loss  and now i \nhave added regularization what happens to the new solution \nso i am telling you the new solution would be smaller weights and so on that is what ltwo \nregularization tells you now you are just trying to make an interpretation for that  so i \nhave given you a closed form solution that w tilde is actually equal to this quantity that \nyou see on the right hand side ok why you are doing this is still not clear but right now i \njust focus on the  what part of it this is just some mathematical steps that  i am doing  \nanyone who is not comfortable with this \nnow notice what would happen if alpha tends to zero what would be w tilde be w star what \ndo you mean by alpha equal to zero no regularization right so that is just one corner case \nthat i want to do but that is not what we care about anything what that is stupid to do all \nthis and tell you that if you do not use regularization you will get the same  solution but \nthat is not what i am going to tell you  right we are interested in the case when alpha is \nnot equal to zero ok so let us look at that case \nrefer slide time tenthirtyfive \n \nnow i am going to assume that h is a symmetric positive semi definite ma trix squared \negg in a vacuum ok so if that is the case then  i can write h as this i have just done the \ndash of h eigenvalue decomposition all right ok  and i know that since it  is a squared \nsymmetric matrix the eigenvalues are going to be eigenvalues a re going to be orthogonal \nyes eigenvalue vectors are going to be orthogonal  and that is why i can write this that q \ntranspose is the inverse of q \nnow let us start with whatever we had on the previous slide and substitute what  what i \nam going to substit ute instead of h i am going to use q lambda q transpose ok so i \nam doing that so is that ok i will just go over the steps and let me know at any point if \nyou have a problem  what i have done is  i have replaced this  i by this and its valid  \nbecause q q transpose is just equal to  i i have just taken q and q transpose as common  \nright so this is a c b plus some a z b so i have taken a and b out right is that fine ok \nnow what is the next thing i am going to do this is of the form a b c inverse  so i am \ngoing to write it as and the inverses are neat right \nrefer slide time twelveeight \n \nthis is fine what will happen to this quantity i what is this quantity q and this is what i \nam left with  but there is still something more  i can do i guess let us see ok so i can \nwrite this entire thing as a diagonal matrix  how many of you see that it is a diagonal \nmatrix because lambda is a diagonal matrix  i of course  is a diagonal matrix  i is \nmultiplied by a scalar which is also going to be a diagonal matr ix and the whole thing is \nagain multiplied by some diagonal matrix ok  what is the inverse of a diagonal matrix  \nthe reciprocal of the diagonal elements \nso i its fine so i have a very neat formula for what w tilde looks like in terms of w star \nok again why am  i doing all this and god knows  but and here d is equal to this \nquantity \nrefer slide time thirteenzero \n \nso what exactly is happening here  in terms of linear algebra or in terms  of geometric \ninterpretations so let me just see if  i have to do som ething first ok  so what is \nhappening to w star is getting \nstudent refer time thirteenfifteen \nrotated remember what happens when a matrix where hits a vector  it gets rotated and \nscaled also  and then what is this diagonal matrix going to do  scale it  element wise \nscaling actually everyone gets this operation  ok and then i am again rotating it by q \nagain the same stupid question if alpha is equal to zero what would happen  q transpose \nwould rotated by something and then q would rotate it back way  that means you will \nend up getting the same solution ok if alpha is equal to zero we understand \nnow if alpha is not equal to zero first let us see what does this matrix look like so what is \nthis matrix actually it is a diagonal matrix  what are the diagonal elements  the what is \nthe first element in the diagonal  one by everyone agrees with this  what is  the second \nelement ok fine and what is the other matrix that  i have lambda so d is equal to the \nproduct of these two things right so what is d going to be what is the first element of \nthis matrix is going to be  how many if you say lambda one by one lambda one plus alpha this \nmuch is clear everyone gets this \nso this is a diagonal matrix of the form a b c  let us consider a three by three matrix now i am \ngoing to mult iply it by another matrix x y  z which is also a diagonal matrix right  \nbecause this is also it so this matrix i have already told you what it looks like  the other \nmatrix is also a diagonal matrix  now what is this product  actually a x  b y  c z and \neverything else has zero now everyone gets it  now can you say what would this product \nlook like if you can actually make out  it would be a diagonal matrix and what would the \ndiagonal elements be  \nrefer slide time fifteeneighteen \n \n  \nso now what is happening  so first this rotation is happening that no one is denying  \nafter rotating what is happening this is a this product is actually a vector that is fine ok \nwhat are we doing to every element of the vector  scaling it scaling it by what quantity \nthese quantities th at every element is getting scaled by the corresponding entry in the \ndiagonal in this diagonal right \nso the first entry is getting scaled by this  the second entry is getting scaled by this and  \nso on ok  i just want you to take some thirty seconds and try t o figure out where  i am \nheaded from here \nrefer slide time sixteenthree \n \nlet us see if i can yeah maybe look at this sentence and see  first of all everyone agrees \nwith this sentence right is there anyone who does not agree with the sentence  i am just \ntrying you to figure out the implication of the sentence  you get it  some people are \nnodding their heads just in because if you scale it right  then there is no guarantee that \nwhat the vector has changed ok  what happens in the following case  that means that \ndimension will be left as it is ok  but if the eigen  if this condition holds what would \nhappen that dimension is almost getting multiplied by a zero right \nso see these two extremes  when the eigen value is very large you will end up staying \nwhere you were so those dimensions will not be affected if the eigen value is very small \nthen you are almost getting scaled down to zero so now what will happen is actually only \nthe significant directions  larger eigen values will be retained  so what is the effect ive \nnumber of parameters in your model now \nsee remember that this w vector is a vector of all the parameters  what am i telling you \nthat some of these are going to disappear  when which condition holds the third can the \nthird bullet hol ds some of these  are going to disappear  that means the effective \nnumber of parameters which remain in your model is going to be less right and you see \nthat it is going to be given by this quantity right \nso that is sometimes known as the effective number of parameter s in a neural network  \nif the effective number of parameters in your neural network is decreasing  that means \nwhat you are doing  making the model less complex  right so that is  what we have \nachieved you see that ok \nrefer slide time seventeenfifty \n \nnow let me end with a pictorial interpretation of this you see two figures here and there \nis only one figure but you see two different things here can you tell me what this is and \nwhat this is that is the first question i want to ask you the hint is that in this lecture we \ncare about the other hint is what was w star the solution for the \nstudent refer time eighteentwentythree \nunregulated loss which means which loss  l theta you need any more hints  sorry this \nbox is the contours of l theta this box contours of omega theta so this thing just ignore \nthis part of the figure for now ok this i have marked as w star  w star was the solution \nwhen i only had the un regularized loss ok  there is the solution when  i had the  un \nregularized loss ok \nso remember the contou r maps that we had seen  so this is the minimum of that \nparticular function  so this is the contour map for  l theta that is clear  now what \nprobably is not clear is why is this the contour map of omega theta  let me just go ahead \nactually \nrefer slide time nineteentwenty \n \nplease do not read this this is the prestige ok so do not read that so this is the contour \nmap of omega theta right  because omega theta in the  what is the minima for the omega \ntheta it is a function of the form w square  what is the  minima zero and what does that \nfunction look like and what is this point  zero the origin right so that is why this is the \ncontour for omega theta ok \nnow what is happening this was the solution when you had without regularization and \nnow this is w tilde w hich is a solution with regularization  so can you make some \ncommentary on this  with respect to not just general commentary with respect to the \nthings that we saw in the derivation  we talked about rotation scaling dimension \nspecific scaling so what is happening this was my original solution vector this was my \noriginal solution vector when  i did not have the regularization term  now what has \nhappened the rotation has happened and we saw that there is a rotation operation \nhappening more importantly what has happened scaling has happened \nmore importantly what has happened dimension specific scaling is happening  right one \ndimension has not  this dimension has scaled down this dimension has not scaled down \nenough that is exactly what we wanted  right we wanted the less important weights to \ngo down and the more important weights to stay there  we did not want a uniform \nscaling down we wanted a dimension specific scaling down \nso the weight vector has been rotated yes each dimension after rotation has been scaled \nsome dimensions have been scaled down more  the other dimensions have been scaled \ndown less  how many of you can make this interpretation from the figure  now that  i \nhave told you this interpretation \nrefer slide time twentyonethirtythree \n \nnow still if you do not how mean if you can still have a doubt with this you still have a \ndoubt what is doubt fine so so this was the original solution vector  right the map told \nus that what actually happens is when you add this omega theta the solution vector gets \nrotated ok at the same time there is also some scaling down and that scaling down is for \ndimension \nhow many dimensions do you have here  two dimensions right so this is one \ndimension this is the other dimension  now in the original case both thes e weights \nactually seemed almost equal right i mean if you look at the w one coordinate and the w two \ncoordinate they were same now after this regularization what has happened is  what are \nthe new coordinates for w one and w two this is the coordinate for w one right this is the value \nof w one and this is the value for w two \nboth of them are admittedly smaller than the original values for w  one and w two in the \nabsence of regularization or both of them equally smaller  no they are being scaled \ndifferently one rate has been scaled down more  the other weight has been scaled down \nlesser right and that is  exactly what the math was telling us that they get scaled in \nproportion to those lambda  one by lambda one plus alpha and that is exactly what we see in \nthe figure is that fine \nhow many if you get this interpretation now is that  ok so all of its elements are shrink \noh you have a question so this final resultant right it is so what would have happened \nis that there would have been first rotation then scaling down and then again rotation so \nwhat you are  seeing here is the final rotation  right so it is not  it should have been \nshowed in three steps by just shown the final step \nso its question was that we first had a rotation  then had a scaling and then again a \nrotation but  i even as explained in the figure  i spoke only about one rotation  so  i \nbasically clubbed both the rotations  and so what you see finally is rotations  scaling \ndown and again rotation"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.3 True error and Model complexity.wav", "duration": 487.76, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule  eightfive \nlecture \u2013 eight \ndataset augmentation \nso how with that heavy math i will just interline with this  something very simple  \nwhich is something known as dataset augmentation \nrefer slide time zerotwentyone \nso what is dataset augmentation mean  so you always given some training data  so in \nthe case of mnist you had this training data  where you are given these digits  images \nof digits and you  wanted to train some classifier  so in dataset augmentation  what we \ndo is  so now we have  what is happening here right conceptual is that there some \nseeing some training data  and try to build a classifier and what you doing actuall y is \nminimizing the empirical train error  \nthat mean it will ensure that whatever you have seen in training is going to look  it is \ngoing to be perfectly classified  whatever we have seen in training that is going to look \nvery good  it is  going to be the t raining error  on that is the error of those training \nexamples it is going to be very easy \nnow my question is this  if a training time you are seeing all this twos which are roughly \nvertically drawn right and a test time  you see at two which is written l ike this which is \nslightly tilted what would happen  it will not be able to do a g ood job on that  that \nmeans your model is not think of terms that you have used in this lecture not \ngeneralizing \ncan you think of a simple trick based on your domain know ledge of how people right \ndigits to kind of overcome overcome this problem you get the question right i am telling \nyou that it is possible that someone writes to in a very tilted manner  can you prepare for \neventuality eventuality the title of this modul e was dataset augmentation  so what \nwould happen is are given some training data \nyou can always generate for training data from that  see here is another training instant \nthat i have created i have just rotate it to two by some random angle  i took this image i \njust rotate it and this is a simple operation that all pixels are moving by a certain angle  i \ncould have rotate it more  i could have shifted it vertically  that means in all my image \nthe two was actually exactly at the centre i just shifted at a bit vertically \nso i am so think that you are reading  one of those kyc forms or bank forms  most \npeople would write at the center of the block provided  but some people could write to \nthe extreme right or extreme left right so you are preparing for that they saying that ok \nall my data the digits are well written at the centre but let me just shift them bit so that \ni can also deal with people who write it at the corner \nleft align or right align instead of center align  i could have even shift i t horizontally \nmost people would write at the center  but some people would write at the top or at the \nbottom i could blur the image but someone has taken a photo and send it to me and the \nphoto is not very clear or  i could just change  some pixels randomly right i could add \nnoise all of this is dataset augmentation  with the hope that  i am capturing with these \nvariations i am capturing enough variations in the data \nso that i have a better chance of doing something better on the test data  is that fine \nthis is all still training data  mind you i am still going to compute the empherical train \nerror it is  just that  now i have blown up my data  but much more than what  i had \ninitially do  you all see by doing this  you could have done better on the mnist \nassignment you could have done better again i am not asking you to do this \nso now i will do this then i will have supervised data because i know that by this small \nvariations the label i s not going to change and what am  i using there i am using my \ndomain knowledge right  i cannot do this always right  i hope you appreciated that \nsuppose that changes the domain a bit and i am given images of defects of motor parts \nright where i have taken a image and there is a  black spot somewhere which indicates \ndefect i cannot go about doing the same thing there i cannot change some other pixels it \nwill just means that the defects is at a different location right but in many cases you can \ndo that \nso if you are given picture  because of dogs and cats  because the entire world case \nabout classifying cats and dogs then you could do some rotation s you could blur them a \nbit you could occlude certain questions of the picture and  so on and still generate \ntraining data right and what you are trying to do is  trying to take care of cases that you  \nwould end up dealing at test refer time fourtwentyseven  right is that clear ok  and please be \naware that we are exploiting some domain knowledge here \nrefer slide time fourthirtytwo \n \ntypically more data is better learning  works well for image classification in object \nrecognition these are the task where this is already been tried out and they have shown \nto work very well in the se tasks also shown to work well for speech  where the people \nhave some speech training data they try to augment it for some task it may not be very \neasy to generate such data right \nso you could think of various nlp applications whereas given you a data  document \nright because always do what joe does in that friends episode do you remember what i \nam talking you  see what i am talking about see you wants to write a recommendation \nletter for monika and chandler ok and he has a letter written  any replaces every word \nby it s best synonyms from the thesaurus refer time fivetwenty  right that says a w ay of \ngenerating noisy data and in that case  it was actually noisy right so you could think of \ndoing here but as happened in that case it will not result in very good transformation \nnext for example i remember something right they are very warm hearted people got \ntranslated as they have some war m cardiograph or something  like that which do  not \nmake sense so it is not very easy  in almost all application should do it  but in some \napplications typically in vision application  this is easy to do an d you would gain a lot \nby doing this right"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.4 L2 regularization.wav", "duration": 1412.84, "text": "deep learning \nprof sudarshan iyengar \nindian institute of technology madras \ndepartment of computer science and engineering \nmodule  eightsix \nlecture  eight \nparameter sharing and tying \nthe next thing that i wo uld like to talk about and this quickly g o over this parameter \nsharing and tying \nrefer slide time zeronineteen \n \n refer slide time zerotwentyone \n \nso parameter sharing and tying  i will just quickly go on this because for  the sake of \ncompleteness it is there in this lecture  but it should it would really make s ense when i \ndo convolutional neural networks so for the time being just take my word for it that in \nconvolutional neural  networks you do a lot of parameter sharing where as the other \nplace that you have seen parameter tying so that is again something that i am not going \nto talk about \nso this is typically used in auto  encoders where the encoder and decoder  weights are \nshared and that effectively reduces the number of parameters  in the model which \neffectively reduces the complex ity on the model  i f th e complexity of the model goes \ndown omega theta goes down because that  is what which wise man told us that time \nsteins lemma"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.5 Dataset augmentation.wav", "duration": 342.36, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 eightseven \nlecture \u2013 eight \nadding noise to the inputs \nwe go down the next module which is adding noise to the inputs right \nrefer slide time zeroseventeen \nso we have some kind of a noise process  and now can you relate that how that was \nrelated to regularization that was exactly the motivation in that case that we could have \nan over complete auto  encoder which is a very complex model because it has a large \nnumber of parameters \nand to avoid that  we were adding this noise to the inputs  so that even if it tries to \nminimize the training error it is not actually minimizing  the true training error right \nbecause you have fed some noise to it everyone gets this  right ok now actually we \ncan show that for a simple input output neural network right that means you do not have \nany hidden layer you just have a set of inputs and you have the output layer then adding \nnoise to the input or rat her adding gaussian noise to the input it is equivalent  to weight \ndecay \nso this can also be viewed  so we will do this part right so we will just quickly do a \nsmall derivation where we show that adding gaussian noise to the  inputs is the same as \ndoing a l two regularization  that is a very neat  idea so this can also be viewed as data \naugmentation right exactly what i shown on the previous slide you added two you just \ncorrupted some inputs of it that is the same as adding noise to the data \nso the essen tially augmenting the data right you have some training data  and just \naugmenting it so to get more training data is that fine \nrefer slide time onefortyone \n \nnow about this  smallest derivation this is again just a set of steps  i will go over it \nreasonably fast i will give you the set up and then it is quickly work through the \nderivation right \nso what i was trying to say is that if you have a simple input output neural network that \nmeans you just have inputs and the output you do not  have a hidden lay er right then \nadding a gaussian noise to the input units where the noise comes from this distribution  \nit is a gaussian distribution zero mean  i want to show that doing this is effectiveness the \nsame as doing ltwo regularization \nnow again see this is the same thing squared eggs in vacuum because this is not the kind \nof networks that we deal with but it is good to see what happens at least in these neat \nconditions because we will never have a simple input output network at least not in this \ncourse we will have a deep neural network always so but at least see what happens in \nthe simple case right so what we are doing is from the x i\u2019s we are creating a noisy x i \nby just adding some epsilon noise to that  and what is our model going to be  it is just \nan aggregation of all the inputs  ok so this is what our original model would have been \nwithout the noise fine \ni would have just aggregated all the inputs i am assuming there is no nonlinearity at the \noutput and i am just taking y i is equal to summation  of all my inputs everyone fine with \nthis side or this is too simple for you guys to understand because  we have been doing a \nlot of deep neural networks so suddenly one layer network  i do not know what it is \neveryone gets it right \nand instead of y hat  now i have y tilde because instead of x i i have x i tilde ok but \nwhat is x i  tilde x i plus epsilon i right  so i can write it as this  just fine so actually y \ntilde is nothing but y hat plus some quantity  ok what are we interested in  always this \nquantity the expected mean square error ok i mean expected squared error  and why not \ny hat \nso we have added noise to the input so now y tilde are the outputs that we are going  to \ntilde so let us see what that quantity is  and again just going to be some simple stuff \nso i replaced y tilde  by this that we just derived on the right hand side on the left hand \nside ok so i am going to take these two terms together  so i can write it as this plus this \nthe whole square fine  and i am going to keep this as it is what is this quantity the \noriginal squared error expected squared error right when  i was not adding noise to the \ninputs ok  and you see how we  got these two quantities this is just a plus b the whole \nsquare is equal to whatever it is equal to ri ght now let us look at the last term this is a  \nsquare of a sum right \nso what kind of terms would you have inside  you will have some terms which are \nepsilon i squares and you would have some terms which were epsilon i epsilon j right ok \nso we will hav e some expectations which are going to be something into epsilon i \nsquare and some expectations which are going to be epsilon i  epsilon j everyone gets \nthis some terms there now which of these terms would disappear  \nstudent refer time fivenine  \nthese terms right why because the noises are independent ok i am not  if i have drawn \na noise for one instance  it does not have any influence on the noise that  i am going to \nadd to the next instance  if i have taken one x i corrupted it with some noise  there is no \nbearing on the noise that i am going to use for the next epsilon i right all these features \nare the noise added to the features are independent \nrefer slide time fivethirtyeight \n \nso now from these terms only the square terms are going to remain is th at fine and \nsimilarly this quantity what can you say about this we just did something similar why \ni am a saying that this is going to zero again i can show that this is the covariance between \nthis random variable and this random variable ok and now are these two random variables \ndependent what is epsilon i  the noise that i am adding to the input does it have any \neffect on y hat no right because y hat does not depend on the noise what is y  true \noutput does it have anything to do with the noise no right \nso that is why these two random variables are independent so i can again write there the \nexpectation of their product as a product of expectations and then the expectation of this \nis going to be zero because epsilon i was drawn from a zero mean distribut ion is that fine \neveryone gets that the same trickery that we did earlier  so this is the quantity that we \nare left with you see how i got from here to here this is an expectation of a sum which \nis equal to a sum of expectations w i has nothing to do with it is not a random variable \nso it is just the expectation of sigma i square which is nothing but the variance right so \ni get this what does this look like i already told you the answer before starting right this \nlooks like l  two regularization thi s is the true error  i mean this is the empirical estimate \nfrom the training error  and this is the weight decay term everyone get this  how you \nsee that this is an equivalent thing  so at least in this neat set up you get the intuition \nthat adding noise to the inputs is a same as adding a ltwo regularization term"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.6 Parameter sharing and tying.wav", "duration": 51.07, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 eighteight \nlecture \u2013 eight \nadding noise to the outputs \nso now going on to the next module which is adding noise to the outputs \nrefer slide time zeronineteen \nso here when you are given some training data  this is the label vector that you have \nbeen given right where one of these elements is one  so these are  like zero to nine eight where \nwhich digit it is and in this case it happens to be digi t two so that element is one right that is \nthe true training data given to you \nrefer slide time zeroforty \n \nso what you could do is actually and actually what you try to do is minimize this \nquantity p i log q i where what is p i p i is the vector which was given and what is q i \nthe predicted probabilities ok so now when you try to add noise to the output what \nyou actually do is you see that i do not trust the true labels they may be noisy  \nwhatever data you have given to me that is one way of look ing at it that i do not trust \nit i will just say that it is noisy the other way of looking at it is that in some way  i am \nensuring that i do not try to over fit to this label right because now my true whatever  i \nam trying to optimize let me just go to that and let us see so instead what we will do is \nwe will use soft targets \nrefer slide time onenineteen \n \nso this is what i mean by soft target  assume that there was some epsilon noise in your \nlabels so instead of treating this as one and all zeros trea t the true label as one minus epsilon \nand divide that among the remaining nine entities right  that probability mass divided among \nthe remaining nine entities \nso now when you are trying to minimize this what is p i this soft distribution right and \nq i is the predicted distribution so you see why this acts as a regularization  why does it \nact as a regularization what is the aim of regularization do not over fit on the training \ndata right to over fit on the training data what should it have done  it should ha ve \ntreated only the correct label now if i am giving it this information then  i am not \nallowing it to over fit on the training data right \nbecause now with this distribution this quantity will not get minimized when q i is \nequal to the onehour distribution where all the masses on two do you get that so in some \nsense we are making sure that now if it tries to over fit on the training data  it will not \nget the minimized error right so you have this corrupted the outputs of it everyone gets \nthis is ok the trainer no that is the whole point  \nstudent refer time twoforty \nno \nso that is thing right  so some of these are heuristics based so now we have started \nwith this whole derivation where we try to show the relations between trainer error \ntested o r not  but things that we have seen some of these things  right even whatever \nunfortunately i tried to prove on the previous slide the weight decay thing  even that is \nonly for these neat networks where you do not have any hidden layer and so on right \nso most of these are just heuristics you are just saying that the principle is that you will \nnot allow the true training error as computed from the training data to go to zero  if you do \nthat you know that you are going to over fit so try whatever you can to avoid that ok \nthat is the idea do you agree that doing this is going in that direction  \nstudent refer time threetwentyfive \ntraining data the hope is that if you do not do that then it will not  under fit on the test it \nright \nthere is no i mean i have you are you looking for a proof where i say that doing this we \nwill ensure that a training error does not go to zero but the test error comes close to the \ntraining error there is no such proof right just a heuristic it is going by the principle \nthat if i do not allow the training error to go to zero then hopefully i will over fit i will not \nover it as much as i would have otherwise right  \nso that you can think of it as this way right  so this is the curve that you are seeing it \nthis was a training curve this was your test curve you are preventing from entering this \nregion where the error is zero that means you will end up somewhere here right and you \nknow that that is a more preferred point as compared to this that is the intuition that you \nare going right is that"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.7 Adding Noise to the inputs.wav", "duration": 442.1, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 eightnine \nlecture \u2013 eight \nearly stopping \ni will do will do early stopping where aga in we will ge t into some of these eigenvector \nanalysis so let us see that  \nrefer slide time zeroeighteen \nrefer slide time zerotwentyone \n \nso the idea been early stopping is actually very simple in principle what needs to be \ndone so we know that this that this trend exists between the training error an d the tester \nright so in practice what you will do is  you will continue to optimize the training error  \nthe empirical training error which is the sum of the errors on the m training points  \nyou will also continuously keep track of the validation error that means the same \nquantity you will compute over the n validation or test points everyone get this you can \ndo this and you are actually doing this in your back propagation assignment  keeping \ntrack of the training error as well as the validation error and you keep plotting them ok i \nwill keep running for various epochs and keep something known as a patients parameter \np  \nso if you are at the twentyth epoch and if your patients parameter p is equal to phi  and just \ndo a check whether in the phi last phi  epochs has my validation error ever gone down or \nit has been staying the same or has it been increasing ok now i will give you a condition \nthat it was either staying the same or it was actually increasing is this good or bad  \nwhat does it tell you while  your training error was of course decreasing may the more \nyou train your training error will keep going down  so what does this tell  it is just over \nfitting you are fitting the training error you are just making it zero or as close to zero as \npossible but th at is not helping your validation error  so the validation error is either \nworst case increasing or remaining the same right \nso this is a very commonly used trick which is known as early stopping  you keep this \npassions patients parameter and you make sure that if you have cross this patients right \nand the patients here is that i was waiting for the validation error to go down but it is not \ngoing down for some p epochs  so no point in continuing training anymore i will just \nstop it does not make sense \nrefer slide time twotwentyone \n \nso and this can also be used in conjunction with other regularizers right so in the quiz \nalso we had this question  sorry for bringing up the quiz but  we also at this question \nwhere you have the sparsity regularization and i was asking whether i can add the ltwo \nregularization along with it  so these regulations can be added or used in conjunction it \nis not that you can only use one of them  \nso early stopping is a way of regularizing but you could also use it in conjunc tion with \nltwo regularization or any other regularization technique that you do not want right so but \nhow does this act as a regularizer from the picture it is probably clear and is the same as \nthe explanation i was trying to give to his question right  t hat you are preventing \nyourself from entering in these regions and trying to enter into more favorable stop at   \nmore favorable regions \nbut can you think of  slightly more  in terms of what happens in gradient  and what \nwould happen if you stopped it early and  so on can you try it to connect it to the update \nrule of gradient descent what happens as you keep doing it for more and more epoch  \nno gradient descent has nothing to do with validation error or backtracking error \ngradient descent only works on the training data let us think in those terms \ngradient star diminishing to zero  so what happens how does gradient descent progress \nwhere do you start i started a random point at every epoch which is a collection of \niterations right or you go or many training points what happens to this i start moving i \nkeep moving now if i fix the number of epochs or do not allow it to change any more \nafter a number of epochs what am i doing  i am restricting the boundary around the \nweight right i am not allowing it to grow beyond a certain boundary do you get that \nrefer slide time fourthirteen  \n \nlet us see that  so we will first see an intuitive explanation and then go to a more \nmathematical analysis are update so the update rule for gradient descent is  i always \nmake this mistake this has to be minus oh the t h  have disappeared ok is there so sorry \nother to have disappear  \nso now what would actually happen at the t h step is we have w naught three plus or minus \ndoes not matter it just tells you that how much it is go ing to change  this is what is \nhappening actually at the t h step right  you have just subtracted all the previous \nderivatives that you had so far  right from where you started off  now you are looking at \nt steps so at every point you are computing a cer tain gradient but had a certain \nmagnitude \nnow let me say that across all these steps the maximum gradient that you had i will \njust call it by tau  right so that means in this summation there are t terms  i am saying \nthe maximum of those was tau that  was the maximum rate gradient that i got at any one \npoint \nnow what i am going to do after this i am going to replace this by something  this \nsummation is always going to be less than or equal to this right because i am assuming \nthat each of my steps is less than tau there are t such steps so i could have at matched \nmoved t into tau right but i would have moved less than that because tau was th e \nmaximum gradient that i had \nso this is going to be less than equal to is that do you get the change from the equality to \nless than equal to  ok so now what am i restricting actually in early stopping what is \nbeing restricted there are only so many symbols there i just speak one t tau is of course \nnot in your hands w naught is not in your hands w so t is the one right so i am only \nallowing that many updates so that means from w naught you can only moves that \nmuch this looks you see that analogy that this is something similar to you not allowing \nthe weights to really grow a lot \nrefer slide time sixtwentyseven \n \nso now but will not end here you will of course do some  more stuff on this right  ok \nso we now see a mathematical analysis of this  so recall that a taylor series \napproximation for l w is the following the same thing which i wrote a few sl ides back \nor many slides back everyone remembers this right and now again i am going to do the \nsame thing that if i know the optimal w star then the gradient at that point is going to be \nzero  \nso this term disappears and now if i take the derivative this is what will remain this is \nexactly what we did earlier also right so we will have derivative of this and derivative \nof this  so the derivative of this quantity is just this and the derivative of this is zero \nbecause that is exactly what we started off with right that w star is the optimal solution  \nnow sgd update rule is the following  ok which i can write as this  i just replaced this \nby this ok i am just rearranging some terms is that ok how many if you are fine with \nthis how many feels to tired to even care about this  \nrefer slide time seventhirtytwo \n \nso this is what w t would be this is again some simple steps leading to some conclusion  \nthe conclusion is what matters the steps are very easy you can go back and look at them \nright so again i wi ll use the evd the same trick that i did earlier and it will give me \nthis instead of h ok again i will just do some  rearrangements and actually i can show \nthat if i start with w naught equal to zero then w two is actually given by this quantity ok and \nthere is a proof of this in the appendix you can go and look at it  \nnow what does this look similar to rotation  diagonal rotation  exactly similar to the \nanalysis that we did for ltwo regularization right and in fact if you can you can show \nthat if we compare this expression with the while we had for ltwo regularization and this \nis the expression that we had for ltwo regularization right rotation some scaling and then \nagain rotation right  then we can show that  early stopping is actually equivalent to  ltwo \nregularization if the following condition is satisfied  \nthis does not mean much because god knows how you will satisfy this condition \nright but all it is saying is that there is some equivalence at under certain conditions  \nand that is what is the intuition  was also telling us that  it is somehow preventing the \nweights from going large and it is doing this  in this very convoluted way where this \ncondition holds for it to be equivalent to ltwo regularization  \nas i said for you and me is going to be very hard t o create this condition right how do i \nmake sure that something like this is true right  but that does not matter what matters is \nthat there is some equivalence between them \nrefer slide time ninethirtyfour \n \nso when you are doing early stopping it is not j ust a heuristic or a blind  thing that you \nare doing you know that it is somehow related to ltwo regularization hence that you are \ndoing it and hence it also works in practice is it fine we will that work for all of you  ok \nright so the things to rememb er is that early stopping only allows  t updates to the \nparameters this is the important thing  rights so now if a parameter w corresponds to \na dimension which is important for the loss then what would this quantity be the partial \nderivative of the loss  with respect to that parameter it is going to be  if there is a \nparameter \nfor example let us take the amir khan  an example right that whatever weight you \ngives to whether the actor was amir khan or not if that is very important because if \nthat feature is on you are lost completely changes and so on right if you do not learn the \nweight correctly that feature is very sensitive \nso for important features the loss would be very sensitive to the changes in the weights \nof these features is that intuition correct right that means this gradient would be large  \nok and if a parameter  corresponds to a feature which is not important what would this \nderivative be small now what is the net effect of this  you have some parameter which \nare important so the derivatives are large some parameters which are not important \nso the derivatives are going to be small  and you are going to only allow t updates so \nwhat is going to happen  the parameters which are important  we will end up getting \neffectively more upda tes right because each of these magnitudes was higher and you \ndid t of those the parameters which are not important we will end up getting  effectively \nlesser movement \nbecause each of these gradients were small and you did only t of those right  so you \nagain see this that it is a weird way of ensuring that your important parameters get more \nupdates than your n on important parameters right  so it is very important to see these \nconnections between these different regularization methods  all of you are fine with this \nfine"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.8 Adding Noise to the outputs.wav", "duration": 250.6, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 eightten \nlecture \u2013 eight \nensemble methods \nso next we look a t on ensemble methods and thi s is just to buil d the intuit ions for \nsomething known as dropout whic h is very popular tec hnique in dee p neural networks \nand convolution neural networks and even recurrent neural networks \nrefer slide time zerotwentythree \n\n refer slide time zerotwentyseven \n \nso how many you have seen ensembles before see n it in machine learning ensemble \nwas not done in machinery done with ok ravi did it so as a combine so the ensemble is \nessentially just the combining the output of different models to reduce the generalization \nerror right why does that make sense  have these different models all of these would \nhave different biases and variances right \nso now you are combining them so i will end up with a bette r thing on the test error \nright so that is the idea behind ensemble now the models could correspond to d ifferent \nclassifiers right for example here  i have a logistic regression and svm and a naive \nbayes i have trained them independently using the same data or different subsets of the \ndata and a test time  i am taking a prediction from all of them and then  taking an \nensemble of those predictions that is the basic idea \nnow it could be different instances of the same classifier trained with different hyper \nparameters i could have the same neural network a three layer neural network  but trained \nwith different hyper parameters so the hyper parameters could be learning rate it could \nbe batch size it could be the number of neurons in each layer and so on right so it \ncould be same classifier but different hyper parameters  different features right so \ninstead of looking at all the one hundred features that i have given  i could train these classifiers \nwith different subsets of the features ok or different samples of the training data \nrefer slide time onefortysix \n \nso bagging is one such ensemble method where you have different instances of the same \nclassifier which are trained on different samples of the training data ok so i have one \nclassifiers trained on a subset t one of the training data another classifier trained on a \nsubset t two of the training data and so on right  and so each of these model is trained with \na different sample of the data \nrefer slide time twotwelve \n \nnow when would bagging actually work  what would you want these classifiers to be \nso each classifier is going to make certain errors \nwhat do you want these errors across classifiers to be dependent independent  \nstudent independent \nindependent right so if one classifier makes the errors on certain test instances other \nclassifier makes errors on a different set of test instances and the th ird classifier makes \nerrors on a very different set of instances that is the condition that you are looking for \nright there is errors if all of them make error on the same instance then all of them are \ncollectively going to make an error on the final prediction also right \nbecause it is like  i asked three guys all of them gave me the wrong answer so my final \nanswer is going to be wrong  but at least two of these three guys gave me the correct answer \nthen my final answer is going to be correct right  so that mean s the errors that these \nmodels make i want these errors to be independent  if i treat error as a random variable i \nwant these errors to be independent \nso so consider a set of k such logistic regression models  suppose that each model \nmakes an error epsi lon i on the  test example now let epsilon i be drawn from a zero mean \nmultivariate normal distribution  so the variance is equal to v  and how many such \nepsilons do i have how many such distributions i am considering \nstudent k \nk right because for each c lassifier there is a distribution so then i can compute the \ncovariance between these random variables ok i will add that let that covariance be c is \nthat fineok now the true  the error made by the average prediction of all the models is \ngoing to be given by this model one made an error of epsilon one model two made an error of \nepsilon two \nso the average error  is going to be given by this  now what is this  expected squared \nerror this is the error this is the expectation this is the square  that is the exp ected \nsquared error is that fine again this is a square of a sum  so it will lead to a lot of terms \nof the form epsilon i squares and what will happen now which terms will go to zero \nrefer slide time fourtwentythree \n \nthe terms having epsilon i epsilon j again th e same thing they are independent so i can \nwrite the expectation of a product as the product of expectations and those expectations \nare zero so this is what it is going to look like what is this  oh sorry actually we had not \nassumed that the covalence \nwhat is this right  and what is this covariance i am sorry i have not we had assumed \nthat there is some covariance said wed not assume they are independent right we would \nwant it to be independent but in the general case we will assume some covariance an d \nthen i will show you the special case where they are independent \nso then how many vs do i have here k right and how many cs do i have here \nrefer slide time fiveeight \n \nthis summation is k into k minus one right or i equal to one to k and j equal to i plu s one to k  \nfine and so this is what it looks like now can you make some inferences from this \nequation this is what the expected mean square error is going to be now think in terms \nof variance covariance and tell me when would this be beneficial i have already told \nyou the answer if the errors are independent what would covariance be zero right \nso then what is the mean square error one by k one by k into v right so that means bagging \nwould work when your classifiers the k classifiers that you are combining \nrefer slide time fivethirtythree \n \nif the errors are independent then the mean square error should actually have been v \nright for a single classifier it was v right because mean square error is nothing but the \nexpectation of the error expectation of epsilon i square which is nothing but v \nbut if you are  if you are combining k classifiers and if these classifiers are independent \nin terms of their errors then your mean square error is going to be one by k into v because \nthis term is going to disappear  ok now if your classifiers are perfectly correlated then \nwhat would happen and basically c is equal to v right is that fine so now  what would \nhappen what is the net result if i substitute this as v going to be v right \nso if you are all your classifiers ar e perfectly correlated that is the other case we had \ntried taken and all of them are making errors on the same test instances and the same \nerrors right then you will not get any benefit of doing bagging  but if you look at the \nother extreme where all your errors are independent or all your classifiers are making \nindependent errors then you will get a benefit your expected mean square error would go \ndown from v to one by k into v everyone gets that \nrefer slide time seventen \n \nso this was just to devel op an intuition that taking an ensemble helps right  and using \nthis intuition now we are going to see at how to  do this ensemble in the case of deep \nneural networks"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 8.9 Early stopping.wav", "duration": 677.52, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 eighteleven \nlecture \u2013 eight \n dropout \nrefer slide time zerofifteen \nso in this module we will look at dropout now \nrefer slide time zeroseventeen \n\nso the intuition that we have developed in the previous  module which was about \nensemble methods is what that  is that ensemble makes sense in most cases  because \nyou do not  expect the errors of these k models that  you are using to be perfectly  \ncorrelated and we saw that whenever they are not perfectly correlated you are going to \nget some advantage \nnow how do you do this in the context of neural networks  so remember what was \nbagging multiple instances of the same network trained on differe nt subsets of the data  \nwhat is the problem with this in the context of neural networks  each of these neural \nnetworks is very complex training each of these is going to take time and i going to train \nk of them is that fine right \nso you decide ok sorry so one option that you have is you train several different neural \nnetworks having different architectures right  but this is going to be expensive because  \nyou have to train k of them  the other option that you have is  you train the same \nnetwork but on different subsets of the data this is also going to be expensive \nso whatever ensambling sampling techniques you can think  if in the think of in the \ncontext of neural networks which are essentially  these two techniques different  \narchitectures and take an ensemble or train the same architecture on different subsets of \nthe data both of them are going to be expensive right \nso now how do you go about it  and it is not just training time expensive it even if we \nmanage to train it at test time again  when you are given a test instance you have to pass \nit through all of these complex neural networks each of which is going to take some \ncomputation and then take the ensemble of the outputs right  so even at test time  it is \nexpensive it is not just that that training time it is expense \nrefer slide time onefiftyseven \n \nso now dropout is a technique  which addresses both these issue s which issues train \ntime computation as well as test time  computation so it effectively allows training \nseveral neural network architectures without any significant computational overhead so \nwe will see how that  works and it just not training time as i said it also allows us to do \nthis quickly at test time \nrefer slide time twotwentyone \n \nso again let us see so again here ok i will get to it when i know so drop out actually \nrefers to dropping out units from the neural network \nso this is my original neural network  and i am just talking about one neural network  \nforget about ensembles just one neural network is what i have now what dropout says \nthis you dr opout some units from this neural network  that means dropout some \nneurons and when i dropout some neurons i am also going to drop out the incoming and \nthe outgoing edges otherwise where are they headed right so i am just dropping out \nso basically what is effectively happening here i am getting a new network architecture \nright at least that is clear that is what dropout effectively does but i have already made \na case that i do not want so many architectures that because it is a headache to train all \nof them and again a test time i have to pass it through all of them right \nso i need to still fill that gap  but drop out says that drop some units and you  will get a \nnew architecture but how does that simplify life we will see that and now each node is \nactually retained with a fixed probability  for the hidden nodes and even further input \nnodes \nso then we were not wrong in actually dropping out the visible node  because you can \ndo dropout at the visible nodes also ok anyways yeah so for the hidden units you would \ndrop them with a probability fifty percent and the input units you will drop them with a \nprobability of twenty percent typically it again is some hyper parameter that you will have to \ntune but typically this is wha t you will do and i hope you see that dropping nodes from \nthe hidden unit from the input unit is same as corrupting the input data right  it is same \nas adding noise to the input data is that fine  \nrefer slide time threefiftyeight \n \nso this is the idea  now let us see how to actually implement this idea  okso suppose a \nneural network has n nodes using the dropout idea each node can be retained or dropped \nan example in the above case i have dropped some five nodes to get a thinned network \nso if there are n nodes what are the total number of thin networks  that i can get from it \nand so that means i can get two raise to n different neural networks  am i happy about \nthis or sad about this sad there is just too many neural networks how can i train them \nactually right \nso how do i do this i am just creating a lot of suspense without giving you the answer \nok so first trick is  share the weights across all these networks  ok we will see what  \nthat means and the second trick is sample a different network for each tr aining instance \nok none of which is clear at this point i can see i can read your faces i am good at it ok \nso let us see how to do that \nrefer slide time fourfifty \n \nso we initialize all the parameters of the network randomly or whatever may be used \nand start training  when i start training i will pick up the first training instance  or the \nmini batch or whatever i am doing we apply dropout resulting in this network \nwhat will i do and they forward prop  forward propagation right ok now ok we \ncompute the loss and back propagate how  some weights are missing right how do i do \nback propagation now  i have deliberately dropped up some of these connections  they \ndid not participate in the forward propagation  this back propagate which are the \nparameters which will update now only the ones which actually participated right \nso i will just do back propagation  just look at the red arrows  i will just do it over the \npaths which are actually present in my network fair enough right that is what you meant \nby normally ok that is normal ok so i will just do it over the  weights which actually \nparticipated that is fair enough that is the only thing you could obviously do \nrefer slide time fivefiftyone \n \nnow i take the second instance  again i apply dropout and qu ite naturally i will get a \ndifferent thinned network as you see the figure three in this slide  ok what would i will do \nnow \nstudent forward propagation \nforward propagation then compute the loss back propagate to compute the loss  ok and \nthen \nstudent back propagate \nback propagate again back propagate only to the \nstudent active nodes \nactive nodes so these other nodes which will get activated  so what is happening here \nis now trying to relate it to what we were doing in bagging right  where we are trying to \ntrain these different networks on different subsets of the training data right  do you see \nsomething similar happening here  there are many such thin networks each time i am \nsampling a different network and updating it right \nso it is equivalent to training these large number of networks on different subsets of the \ndata right  but then the problem is that some of these networks may never even get \nsampled there are two raised to n of those my amount of data is definitely to be less than  \ntwo raised to n \nso some of these networks might just not even get sampled  then what is happening  \nor they would get sampled very rarely  right for example what is the probability that \nagain i will end up with the same network we are computing it good it is very less ok \ni am fine with that at seven hundred and thirty right \nso it is a very less right  so it is quite likely that this  network will never be sampled \nagain that means for that network the parameters are getting updated very few times  \nam i fine with it  yes i am why because the same weights will get updated for a \ndifferent network i am just using the same weight matrix throughout remember that  my \nw matrix or w one w two is the same throughout \nit is just that at different depth  subsets different instances  i am just touching some \nportions of this w one  and i am not touching the other portions of w one  so now  what \nwould happen so i have shown you two training instances right what would happen to the \nweights which were active for the first training instance as well as  the second training \ninstance it will get updated twice and which are active only once \nstudent refer time sevenfiftynine  \nonly once right  so over a period of time  many of these weights are shared across all \nthese networks that  i am sampling right  so even th ough a particular network is \nsampled only a few times  its weights will get updated many times  via these other \nnetworks which are similar to it do you get that how many of you get this ok good \nso what is happening i will just repeat that i have just one weight matrix i am sampling \na thinned out network which only uses some of these weights \nso for that training instance i will update those weights now i know that the likelihood \nof the same network getting sampled again is very less  but i do not  care about it \nbecause i could sample a different network  but i am sure that some of these weights \nwill again repeat in that right  and in that i told they will get updated so even though \neach of these networks is seemingly getting very few updates  overall all the weights \nshared by these networks are getting updated as much as they should be is that fine \n everyone gets this idea ok fine and while i am also taking care that similar things like \nearly sto pping or weight regularization  ltwo regularization where i am not allowing a \nsingle weight to continuously grow or something  otherwise because these weights will \nbe off for some networks  is that fine you see the connection between early stopping  l \ntwo regularization and this is that ok \nrefer slide time nineeighteen \n \nand so each thinned network gets trained rarely or sometimes even never  but i am not \nworried about it because it is weights will get updated through some of these other thin \nnetworks \nrefer slide time ninetwentyseven \n \n this is all finite train ing time at training time what is happening is this is one of these \nblue guys introduce on with the probability p  that means the weights going out of it  \nwho are available with a probability p right and other times they were not available \nnow what do i do it test time i cannot let me finish this ok i cannot take an ensemble \nof d ok  the answer would have been that  at test time  instantiate all these  two raised to n \nnetworks pass the training passed the test example  through all of them and then take a n \nensemble right but of course  that is probablitivly expensive so what will i do at test \ntime what is the simple trick that i will do so he says that just use this network \nand just use the final net matrix that you had no but then you have guessing out of the two \nraised in the  sample some small number of those and do it  actually dropout uses \nsomething very simple than this what it says is that each of my nodes was present only \np fraction of the times in the training data ok that means one way of looking at it is that \nso imagine that you could think of this as the analogy is that all these nodes are \nparticipating in a discussion right  where they trying to see how to do this job properly  \nbut with probability p they all sleep off right  \nso at the end of the meeting you will trust each of them only with probability p so that \nis the simple trick with dropout uses  it says that just scale their weights by p  because \nthat is how much i trust this node  it only participated in p faction of th e decisions so \nthat is the confidence that i have in it \nso if it is saying that with  wone weight do this i will only do it with p into w one weight \ndoes that make sense  ok and there is again a  squared egg with vacuum kind of \nexplanation for this ok which was there in the quiz  last year which is very convoluted \nit does not really give you the true picture because  you can derive some math  and so \nthat this is mathematically proper but that again works in very specific conditions but at \nleast if you get the intuition that is fine that what we are saying is that  these nodes will \nleave an active a few number of times so i will only trust them that much and i will just \nscale their weights by that factor \nso at test time i will just pass my test instanc e through one network which is the full \nnetwork with the weights scaled according to the rule which  i just said that is exactly \nwhat dropout does \nrefer slide time twelvethree \n \nso what dropout actually does is  we will apply some kind of masking noise to the \nhidden units right since the same as seeing that you  are computing the hidden unit but \nthen you are masking it off ok \nso what is the effect of this  i will give you the answer and i like i like you to think \nabout it the answer is that it prevent s the neurons from becoming lazy  what do lazy \npeople do they depend on others yeah actually yeah they depend on others now so let \nme answer that give the answer for this and then tell me whether  that is still contradict \nok \nso let us see right consider this layer of neurons all of these are collectively responsible \nfor what happens to this guy right  now you see what i mean by neurons becoming \nlazy i could just see ok i will not give my input these other ne urons will take care of it \nthey will adjust their weights \nso that they eventually it will fire or not fire or whatever right you see that could \nhappen but now these neurons cannot rely on their neighbors because they do not know \nwhen their neighbors are going to ditch them right  they will suddenly drop off ok and \nnow i was waiting for my neighbor to actually do something and he  is not going to do it \nso i have to be alert always do you get the analogy \nso these guys are collectively responsible for something and they know that some \npeople in the collection are going to betray them  so each of them has to be more \ncareful so the more technical term for this is that d oes not allow the neurons to co \nadapted \nso it does not allow them to  get into this mutual agreement  that you take care of certain \nthings i will take care of certain things and together we will do the job right  you do \nquestion one i will do question two i am ok it does not allow them to do this \nso let us just concretize that intuition a bit for  so essentially a hidden un it cannot rely \ntoo much on other units  as they may get dropped out at any time  each hidden neuron \nhas to learn to be more robust right it has to do the job as if it is the only guy responsible \nfor the job ok and let us consider one of these neurons h i \nrefer slide time fourteenzero \n \nand let us see that a h i learns to detect faces  sorry it learns to detect a  nose so i am \ntrying to do face detection whether an image is about a face or not and  h i is the feature \nwhich fires if there is a face somewhere  if there is a nose somewhere in the image  is \nthat fine \nnow if all these guys start acting lazily ok this guy is going to detect a nose  that \nmeans definitely face will be there  so i do not need to do anything right what would \nhappen now suddenly this guy is going to go away dropped out  so then these other \nguys need to do one of  two things either add redundancy that means one of them should \nalso take responsibility for detecting a nose or do it in a different way  take \nresponsibility for detecting the lips or the eyes or some other part do you get that  right \nbecause you know that i cannot co adopted with my other neurons i cannot say that ok \nin these front facing faces you just detect the nose and wi ll be done and we  will all keep \nquiet right \ni do not  know whether  you will do your job properly  so i will have to add more \nredundancy you detect a nose  i will also detect a nose or you detect a nose and i will \ndetect something else which helps detecting the feature right so that is  why thes e \nnetworks become more and more robust as you add this dropouts \nrefer slide time fifteentwentysix \n \nso that is all that i had to say i still do not know whether i have answered your question \nor not all of them try to detect nose see as long as that helps reducing the final loss it is \nfine it is just the case that you would have some training images where the nose is not \nvisible maybe that person is drinking something right \nso for at least for those training instances someone else has to take care that you d etect \nfrom the other images right otherwise a loss would not be zero for that training instance \nso as long as you have some training instances see  if all your training instances can be \ndetected just by detecting the nose  then there is nothing wrong in all of them trying to \ndetect the nose so if the training it is like that it will happen but the hope is the training \ndata is not like that right is that fine so we will end here"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 9.1 A quick recap of training deep neural networks.wav", "duration": 340.52, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \na quick recap of training deep neural networks \nlecture \u2013 nine \ngreedy layerwise pretraining better activation functions better weight \ninitialization methods batch normalization \nwelcome to lecture nine of csseven thousand and fifteen today we wil l talk about greedy layer wise pr e \ntraining bette r activati on functi ons bette r weig ht initializati on methods and batch \nnormalization s o today\u2019s lecture is more like ti ps a nd tric ks t o make dee p learning \nwork \nso when you are actually experimenting with deep learning in practice what  are some \nof the things that you need to take keep in mind and it is also my way of connecting the \nhistory that we saw to  where we are today  right so there were  certain things which we \nsaw in the history and now i will  try to bring those back and connect to where we are \nheaded from here right  where we have reache d today and where we are heade d from \nhere \nso that is with that in module one i will do a very quick recap of training neural networks \nand not take more than five minutes and i need it for a specific purpose \nrefer slide time onesix \n\nso we already saw how to train such a very shallow neural network what was the  \nlearning algorithm  gradient descent  and this was the update rule right  in particular i \nwanted you to notice that the gradient actually depends on the input \nso when you compute the gradient formula you have this multiplication by x so it is \nproportional to the input and this is one fact that we will use it  at least a couple of c ases \nin the lecture today so this was a very shallow single neuron  network what if we have \na wider network still which algorithm  \nstudent gradient descent refer time oneforty \ngradient descent ok  and we just have  these three different formulae and for each of these \nformulae note that the gradient or rather this gradient depends on the input that you are \nfeeding in ok i did not keep this in mind \nrefer slide time onefiftytwo \n \nand what if you have a deeper network  so we saw a very shallow network  we saw a \nwide network and  i am showing you a deep network  what will you do  again gradient \ndescent \nbut you will apply the chain rule for computing the gradients and again here in general  \nyou will notice that for any  of these weights wone wtwo w three the gradient formula will have \nthis h i minus one what is h i minus one  \nstudent refer time twoseventeen  \ninput from the previous layer right and h zero is the actual input  so the gradient at any \nlayer is actually proportional to the input from the previous layer and this could  either be \nthe input from the hidden layer or the actual input \nrefer slide time twothirtyfour \n \nand finally we saw this thin  so we saw a wide network  we saw a thin network  now \nwe will see a wide network and a deep network right sorry we saw earlier we saw a wide \nnetwork and a deep network  now we see a wide and deep network  and here again you \nhave compute the gradient by applying this chain rule across multiple paths  and that is \nwhat we use and we call it back propagation  and remember again they are the same \nthing holds that the gradients at some point are proportional to the input at that layer \neveryone remembers that ok \nrefer slide time threeseven \n \nso this is important  so what we have is things to remember from  what we have seen \nso far is that so training neural networks is basically a game of gradients right so you \ncompute the gradients and everything depends on those how will you update the weights \nand everything from there on is about the gradients \nand these gradients actually tell you  the responsibility of the parameters towards the \nloss and you  appropriately update them  and we saw a variant way  different sorry \nvarious variants of how to use the gradient so we saw t he gradient descent we saw nag \nmomentum and all \nbut in all of these the underlying core thing was to compute the gradient and then do \nsome manipulations based on that  and the other key thing is that the gradient at a \nparticular layer depends on the input to that layer ok  \nrefer slide time threefortyeight \n \nso now let us go back and just  retrospect a better and see what is it that we have learned \nso far so so far what  i have taught you gradient descent  oh sorry back propagation is \nsomething which was proposed way back in one thousand nine hundred and eightysix right \nso in fact it was existing  before that but it was popularized by this work of  rumelhart \nand others in one thousand nine hundred and eightysix right  so but then in the one thousand nine hundred and ninetys or early two thousand  if back propagation \nalready existed and we could train deep neural netwo rks then why did not we here so \nmuch about deep learning at that time  of course you guys were busy with school and \nall at that time but why did the others or older people like me not hear about it \nstudent computational power \ncomputational power is that the only thing \nstudent refer time fourthirtyfive \ncomputation and memory is are the only thing \nstudent convergence \nwho said convergence ok good  so actually what happened right in the late eightys and \nearly ninetys and even early two thousand  when you used back propa gation to train really deep \nnetworks it was not very successful and what do i mean by not successful actually \nwhat are the two things that could happen someone gave the answer already \nstudent refer time fivezero \nit does not converge right that means  you do not reach the optimum  solution right in \nfact till two thousand and six it was very hard to train very deep networks \nand typically even a  after a large number of epochs these networks did not converge \nthat means they were still at a very high loss and although in principle everything is fine \nyou have a deep neural network you have an algorithm that can train it but you are still \nnot being able to train it properly and you are not being able to make any practical use of \nthat \nso that was the story till two thousand and six so today is about what happened in two thousand and six what it led to \nin the next few years and th en where we are currently right  so that is the journey that \nwe need to make ok and that is why we started off with this quick recap of back \npropagation because that is what i want to tell you that why did it not work earlier and \nwhere are we today"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 9.2 Unsupervised pre-training.wav", "duration": 1456.67, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 ninetwo \nlecture \u2013 nine \nunsupervised pretraining \nso with that we go on to the next module in which we will talk about unsupervised pre \ntraining \nrefer slide time zerotwenty \nso this work which i am going to talk about  they trying to understand what has changed \nsince the late ninetys or the early two thousand how did deep learning become so popular despite \nthis problem with training them right this problem was there \nso what happened  to them solve it right  and this field actually got revived by this \nseminal work by hinton and others in two thousand and six \nrefer slide time zerofortysix \n \nso let us see what that idea was  so this is the idea of unsupervised  pre training in the \noriginal paper they introduce idea in the context of something known as r b m\u2019s  which \nwe will do in the last thirtythree percent of the course  but we could do the same with auto \nencoders which we have already done so in this lecture i am g oing to talk about t his \nidea in the context of auto encoders \nrefer slide time oneeight \n \nso consider the deep neural network shown in this figure so the a module name and the \nidea was unsupervised  pre training so that itself is a giveaway of what is g oing to \nhappen ok so suppose this is the deep neural network  that i have designed for a \nparticular classification task so what it is doing is this taking an input which is the red \ncolored neurons that you see at the input it has four hidden layers that means it is four layer \ndeep and then you have the output layer which tells you whether positive or negative \nright that is the network that i have  and i know that this is hard to train such a network \nthe loss will not converge and i will not get anything meaningful \nso what these guys suggested is that  forget about the supervised criteria that you have \nthat means you are trying to minimize a classification loss just forget about that just take \nthe first two layers of this network  ok which is x and h one righ t so you  take the original \ninput x you feed it to some transformations and you get the hidden representation h one  \nand now try to reconstruct x from h one what is this \nstudent auto encoder \nauto encoder ok what is the objective of the auto encoder \nstudent refer time twonineteen \nit is exactly this  for each of the m training examples  look at each of the dimensions of \nyour input and minimize the square difference between the actual input and the predicted \ninput right is that fine that is what an auto encoder does so this is what they suggested \nok so right now i am not telling you why this makes sense and all that that is what we  \nwill do later right now i am just telling you the trick then we will analyze by that trick \nworks and why is this objective unsupervised \nstudent refer time twofiftytwo \nbecause we are not using any labels we just giving an input and we just reconstructing \nthe input we only have x\u2019s we do not have y\u2019s of course eventually we will use the y but \nat this stage when i am calling it unsupervised pre training i am not using the y \nrefer slide time threeten \n \nnow at the end of this what would happen yeah what would h one learn \nstudent refer time threenineteen \nit will learn an abstract representation of x was that our original task wh at were we \ninterested in \nstudent refer time threetwentynine \nin the classification task but we are doing something very different why  we will see ok \nnow guess what would the next step be does this make sense \nrefer slide time threethirtysix \n \nnow at the  end of t he first unsupervised pre training  i have ensured that h one which is \nthis layer  has learned some abstract representation of the input  right and that i know \nfrom the auto encoder i mean the auto encoder which we have learned earlier right that at \nlearns an abstract representation of the input \nnow i have this so that means given an input i know how to compute an  extract \nrepresentation and i am also sure that it captures the important characteristics of the data  \ni will just repeat this process  i know th at i have four layers in my original network  so i \nwill now take h one  try to compute h two and then reconstruct h one from it so the  in effect \nwhat am i doing in plain english learning and even more \nstudent refer time fournineteen \nabstract representation of the  input h one was already one abstract representation now \nfrom this i am learning an even more abstract representation and does the objective \nfunction makes sense right all i have done is replaced x by h one right in both these \nplaces the rest of it is the s ame for all the training examples for all the dimensions and \nthroughout i am assuming that we are n layers i mean sorry n neurons and every layer \nincluding the input layer \nnow what would the next step be  fix the weights in h one layer fix the weights i n it is two \nlayer and now try to reconstruct h two from this h two right  and in this way we will continue \nand learn all the hidden representations  does that look  ok right so at least this much \nwe believe it because we know that auto encoder works and you are  just using an auto \nencoder and we are using it incrementally from every abstract representation learn an \neven more abstract representation \nnow at the end of this what will i do what was my original task \nstudent classification \nclassification so what will i do \nstudent refer time fivetwentythree \nwhat is a network that i have when i finish this unsupervised pre training \nstudent refer time fivetwentynine \nno tell me of the diagrams that you see on the slide how much of the network would i \nhave right everything except the green output layer right because the last step would be \ntake h four or sorry take h three and reconstruct h three from it and in the process learn h four right is \nthat clear \nso i would have learnt till that point and now what i am going to do is some thing very \nsimple \nrefer slide time fivefiftynine \n \ni will  after this layerwise pre training is done  i will add my output layer  now all the \nweights in my network for every layer have been initialized \nand they have been initialized in a way that  that layer le arns a good  abstract \nrepresentation of the input right that is the thing that we have achieved at the end of \nunsupervised pre training that every layer has learned and more and more abstract \nrepresentation of the input right now i will keep all these weights initialized to whatever \ni learned in the pre training setup does that make sense \nso that means instead of taking this big network with the output layer and initializing \nthe way it is randomly i am just going to use whatever weights i learned using  the \nunsupervised pre training ok so can you tell me what has happened in terms of the error \nsurface and so on or my movement in the w b plane or in this case  this very high \ndimensional w plane \ni have reached some configuration for the w\u2019s  where i know that each of these layers  is \na good meaningful representation of my original input  right is that  a fair statement in \nenglish how many of you agree with this ah anyone has any questions at this point  \none layer weights that is what you do in answer bec ause if you train all the four then you \nare again entering the same problem which you had earlier right \nyou cannot back you cannot back propagate through all the four layers because now it is a \ndeep network and we know that  does not work so at every layer you fix whatever you \nhave learned so far and at a time you are training only one layer so that is one \ninteresting way of looking at it right you know that the deep neural network with four layers \nwas not trainable \nso now we have reduced it to one layer at  a time i knew that one layer at a time works \nright is that fine  now i will add the output layer and what will i do  train the weights of \nthe \nstudent output layer refer time sevenfiftythree \n i will not just do that i will fine tune the entire network that mea ns i will train the \nweights of the output layer  and i will also fine tune the  entire network but now i am \ncontradicting myself i just gave an answer to him that again i am doing this deep \ntraining and i know that deep training does not work \nbut this a ctually works do you get the difference  right one is that when i start from i \ntake this big network  i start from random weight initialization and try to train it that is \nthe story from one thousand nine hundred and eightysix to two thousand and six that in most cases these networks did not converge \nso now in two thousand and six we came up or someone came up with this idea of unsupervised pre \ntraining where you train the layer network one layer at a time you do up till the last layer \nnow you add the output layer and then fine tune the entire network that means back \npropagate over the entire network is a set up clear to everyone how many you understand \nthe setup \nnow again when i am doing the last step which is known as fine tuning i have to back \npropagate over the entire network because i am saying i will adjust all  the weights but \nsuddenly this works as compared to starting from scratch you see the problem and you \nsee why this is important then because this has now given you a way of  training deep \nneural network i still not told you why it works \nwe will delve into it but not really give any concrete answers because concrete answers  \ndo not exist but we will at least try to get some intuitions behind why it  works so you \nget the setup now that this is what was happening till one thousand nine hundred and eightysix to two thousand and six and now with this \nidea suddenly deep neural networks were being able to train well \nso in effect what we have done is  we have initialized the weights of the network  using \nthe unsupervised objective right so now initial starting with random weights  we have \nsome weights whic h cater to the unsupervised objective that  we had and the \nunsupervised objective was us layer wise reconstruction so that is what has happened in \nplain english is that fine everyone gets that \nrefer slide time ninefiftyfive \n \nnow the question is  why does this work better and i give you two options and i want to \nthink about both these options ok is it because of  better optimization or is it  because of \nbetter generalization no that is not an option but i of course we will relate it to that but \ngiven these two  i want you to think whether there is any difference between these two \nstatements or not that is the first thing i want you to see  how many if you get the \ndifference between these two statements not many why is it so what is optimization deal \nwith dash data or dash data \nstudent refer time tenthirtyfive \nthe answer you can give dash right  dash one data or dash two data  what is optimization deal \nwith \nstudent training data \ntraining data optimization remains on training data what does generalization depend on \nstudent it as zero \nit as zero so you get the difference between these two questions  fine so let us try to answer \nthis again here right  this is two thousand and six to two thousand and nine  period that i am going to talk about  there are \nsome answers and just bear with me i will give you those a nswers some of them will not \nlook very convincing but what happened after that or as a result of these investigations \nthat is more important right whether these answers make sense or not  they will make \nsense to an extent i am not saying that we will just be bluffing \nbut it will not be very convincing because there is no theory behind it right so what is \nconvincing if i give you a proof that this less this is equal to that right then if we give \nyou a proof and everything you do not have any other  questions that is not what i am \ngoing to give you i am going to give you some intuitions  because that is all these \nexisting works from two thousand and six to two thousand and nine had and then i will make a commentary on that which \nwill lead us to some other things so just bear with me for a few minutes right \nstudent refer time elevenfortysix \nthat is the optimization problem  if that was the case the  i will just come to that  that is \nwhat i want to talk about  ok so it is so these are the two questions that we are dealing \nwith right and the answer is depends so we will see what it is \nrefer slide time elevenfiftyeight \n \nso let us first examine the case when it is because of better optimization \nrefer slide time twelvethree \n \nso let us first understand what is the meaning of this question when i a sk is it because \nof better optimization then the question that i am asking you is that the first set up where \ni was trying to train everything from scratch compared to the second set up where i had \nthis unsupervised pre training  is it that the optimizat ion problem becomes easier in the \nsecond set up now if the optimization  problem becomes easier what do i actually mean \nby that that i was able to drive the dash to dash \nstudent loss to zero \nloss to zero right so is it that this is the optimization problem that we were interested in \nso is it the case that in the absence of unsupervised pre training we are not able to drive \nthe loss to zero for the training data  and hence poor optimization right that if you do not do \nthis unsupervised pre training even for the training data we cannot drive at  loss to zero that \nmeans our optimization problem itself is not working properly right i mean the problem \nis fine \nbut the solution is not good  you get that do you understand what is the subtle meaning \nof this how many if you get this so let us see this in more detail right \nrefer slide time thirteenthree \n \nso the error surface of the supervised objective of a deep neural network  is highly non \nconvex it looks  something like this or even nastier than this and in particular it has \nmany hills and plateaus and valleys we saw this even in the toy examples that  we were \ndealing with right and given the large capacity of deep neural networks it is still easy to \nland in one of these zero error regions  on what basis am i making the s tatement which \ntheorem \nstudent refer time thirteenthirtytwo \nuniversal approximation theorem that is what the universal approximation theorem told \nus in fact there is a study the paper which has been cited  it showed that if the last year \nhas a very large capac ity then you can drive the loss to zero even without pre training  do \nyou get the meaning of this what does is mean so i have the input  i have a series of \nhidden layers what do i mean by the last layer has a lot of capacity  what do i mean by \nthat it has a lot of dash \nstudent parameters \nparameters now how do i create these parameters i will just grow the size of the last \nhidden layer right and using that then i will predict this one y \nso so that is how i could increase so that is exactly what they di d they took a very deep \nneural network and made sure that the last layer was given a very high capacity and then \nthey showered that even if you do not do an unsupervised pre training you can still drive \nthe training loss to zero right \nso this was hinting that maybe this is not an optimization problem this is something it is  \nstill not very conclusion but we will just go with these studies  we will just all i am \nsaying is that do not shoot the messenger this is what the study says i am just relaying it \nback to you right and they will have questions on these which will try to address  but if \nthe capacity of the network is small then the unsupervised pre training helps \nso if you do not have these large capacity networks but you have very deep networks  \nin that case unsupervised pre training helps and this is all empirical observation right \nthere is no proof which says that  given a capacity k with so much error bound  i can \nguarantee that the loss would be epsilon within the zero loss and so on it nothing like that \nthat is what it should have been ideally the case in which case life is much easier for me \nbut that is not the case this is just an empirical study as are most of the studies done in \nthe period of two thousand and six to two thousand and nine \nrefer slide time fifteenthirty \n \nso that te lls us something about what optimization means and whether this was an \noptimization problem or not \nrefer slide time fifteenthirtyeight \n \nso let us look at the other question is it because of better  regularization so what does \nregularization do or you gave the exact answer it constrains the weights to lie between lie \nin some regions so it does not allow the weights  a lot of freedom right  and so you \nknow what l one regulation does it constrains the weights to this box  and l two \nregularization constrains us to this circle why no why this i know this but why \nstudent refer time sixteenone \nin why the circle i am pretty sure most of you do not know what you are saying but you \nare saying the right answers but anyways i will test this in the quiz so i have given yo u \nanother quiz question on camera so yeah so a prevents a loss from taking large values \nrefer slide time sixteeneighteen \n \nso indeed pre training also constrains to the way to lie in certain regions of the \nparameter space why am i making this statement what is the meaning of the statement \nso i told you that what regularization does  and from there i am making this jump and \nsaying that even with pre training the same thing happens that your weights are actually \nconstrained to certain regions of the parameter  space why am i making this statement  \nand what are these regions that the weight is constrained to think l theta think omega \ntheta any regulation is of the form l theta plus omega theta \nlet us see  so it constrains the way to lie in regions where the characteristic of the data \nare captured well  that is what unsupervised pre training does  it is trying to train the \nnetwork in a way that each layer actually captures the important characteristics of the \ndata and this is based on our understanding and bel ief in auto encoders  so you could \nactually think of this that the unsupervised objective that you had for all these layers that \nwas actually omega theta you are first trying to optimize omega theta \nso in a normal regulation problem you put l theta and omega theta together and then you \ntry to balance them but here you have done it slightly differently you first gave it omega \ntheta which is the lost of reconstruction and you asked it  to minimize this loss across for \nevery layer \nstudent refer time seventeenfortyeight \nno is this fine tuning so now what that means is that see remember that this is a very \nhigh dimensional region  where you initialize makes a lot of difference so with this \nunsupervised pre training you are at least ending up in reason so you  could think of it \nas a constraint  that ok move wherever you want to but start from here  which \nautomatically means that i have  i mean i have how to it is some other regions in that \nparameter space you get that \nstudent refer time eighteensixteen \nas you typica lly that would be one thing  and it would also mean that you are starting \nfrom there so with this early stopping and other criteria you will not be able to grow \nmuch out from here right  so just  if that makes sense geometrically from here you \nwould not be able to move all the way there you get that  everyone gets this question and \nthe answer \nso you see what the answer per is object was  and you also see the difference between a \nnormal regularization and this regularization in the  normal case you had l theta plus \nomega theta put together and then you are trying to minimize the sum of these two it was a \njoint optimization here you have first done omega theta ensured that the weights that you \nlearn minimize this objective  and now you  add in the supervise o bjective which is l \ntheta right \nso now this makes sure that your network cannot be too greedy with respect to l theta \nbecause it has been constrained  that has to first honor the omega theta because that is \nwhere you started  and now from there on it has to decide how to do l theta does that \nmake sense you see how this is acting as a regularizer is that ok and that links back to \nyour weight initialization thing right fine \nrefer slide time nineteentwenty \n \nso some other experiments have also shown that  pre training is more robust to random \ninitializations \nnow what do i mean that mean by that  so in these two graphs that you see here  so this \non the x axis you have the number of layers that you add to your deep neural network  \nand on the y axis you have the error that your network gives  when you try different \ninitializations right so this box actually tells you the variance in the error \nso that means i tried training a network with four layers and i tried different initializations \nand the error varied in t his range ok is that good or bad what would we want typically \nsomething which looks like the plot below right where all these variances are little that \nmeans even once you do unsupervised pre training  right it is more robust to random \ninitializations random initializations of what \nstudent refer time twentyfifteen \nthe original random initializations from which point you  started the unsupervised pre \ntraining ok because once you have done the unsupervised pre training that is your \ninitialization everyone gets that \nrefer slide time twentytwentynine \n \nso these are some let us see ok so these are some empirical studies and let me just \nmake a comment on these \nso what happened from two thousand and six to two thousand and nine is  people showed that see this is possible  you can \nactually train a  deep neural network using some of these tricks  we do not have a very \nclear answer for why this works and you could argue different way so this is \noptimization this is regularization and so on  but i do not have any theory supporting it \nthere is no pro of for why unsupervised pre training works all of these are empirical \nobservations  \nbut what it at least established was that it is possible to do this so now if it is possible \nto do this  let me see if there are better ways of doing this  do we actually need to do \nunsupervised pre training oh i think it is better regularization then why not i try better \nregularization techniques and see whether that helps  so that led to the  evolution of \nwhich thing that you have already seen yeah which regularization technique that you saw \nin the last class \nstudent drop out \ndrop out right so drop out was something specific to neural networks which was \nintroduced in the context of neural networks  so this is because people started believing  \nit is possible so let us try even better ways of doing that  so that is how dropout came \nout right  then people said  maybe optimization is the problem maybe these earlier \nalgorithms which up till that point was which algorithm \nstudent refer time twentyonefortyeight \ngradient refer ti me twentyonefortynine maybe that was not good so let us try to decide and \ndesign better optimization methods and that led to the evolution of  adam adag ard \nrmsprop so on right so although these studies were not so theoretical in what they \nwere trying to prove  they created this hope which then led to a lot of pr olific work in \nthat field right  so at least you get the context now  right the some of these might look \noh this is one data set people did experiments on m l s but i could have  taken a different \ndata set and showed that these results do not hold and so on  you could always ask those \nquestions \nbut at least what happened is people started believing these and people started \nquestioning that ok unsupervised pre training is one thing what else can i do  and now \nwhat has eventually happened is today no one uses unsupervised pre training right  that \nmethod which led to the revival of this field and you would have hoped that  that would \nactually survive for many years that is out \nnow hardly anyone uses unsuperv ised pre training it is only used in the context of \ntransfer learning so what i mean by that is that if you have a model  trained for one \nclassification say classification of images on one data set right \nnow you have a very small amount of data in some other domain so instead of training \na network from scratch for this domain you will just initialize  it with the weights for \nwhatever you have trained on data set one so that is more of transfer learning rather \nthan unsupervised pre training  so that i s still very prevalent  but this reliance on \nunsupervised training to make sure that the network actually trains that is largely phased \nout \nrefer slide time twentythreethirteen \n \nbecause what has happened  since two thousand and six and two thousand and nine  is that we have better optimization \nalgorithms which are rmsprop ada grad adam even so on right many various and \neven now that research area is active as i was saying just in december  there was a paper \nwhich pointed out some flaws in a dam and how to improve it and so on  we are better \nregularization methods the most prominent among those being \nstudent dropout \ndropout so these two are things which you have already seen  today we are going to talk \nabout better activation functions this is again something which evolved that maybe \nsigmoid tanh are not good so maybe something else is needed  and then better weight \ninitialization strategies  so then people took this inference oh one way of looking at \nunsupervised pre training is that  it actually initializes the weights in a better way from \nwhere on it becomes easier for me to reach convergence \nso why do not i come up with better weight initialization methods itself instead of \nrelying on this indirect way of initializing the weights so you get this so get the whole \npicture now what we have been doing in the past few lectures and how it connects to the \nhistory and these studies which were done from the period two thousand to two thousand and nine how many if \nyou get the whole picture  ok so that is where we are now  so today we are going to \ntalk about better activation functions and better weight initialization methods"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 9.3 Better activation functions.wav", "duration": 1648.25, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 ninethree \nlecture \u2013 nine \nbetter activation functions \nlet us start with better activation functions \nrefer slide time zerosixteen \nrefer slide time zeroseventeen \n \nso before i get into activation functions right  let me first tell you why i care about \nactivation functions why do i actually want to come up with better activation functions \nso will start with the following question what mak es deep neural networks powerful \namong other things what is this one thing which makes it powerful so let me give you \nthis intuition \nrefer slide time zerothirtyfour \n \nthis i have a deep network  ok and do not worry it is a thin network but i could have \nhad a wide network also but just for illustration i have taking a deep network  thin \nnetwork  \nnow imagine that each of these  neurons that you have if i replace the sigmoid in each \nlayer by a simple linear transformation  a by the way this is technically incor rect so \norange is always input so this should not be a sigmoid there right either add one more \nlayer there or let us change the figure \nso suppose i replace all these s igmoids by linear transformations what would y be can \nyou write y as a function of x what would it be give me the function will we just be \nthis right so first we will do w one of x which is this right  then will take w two of that then \nw three of that and w four of that \nso i could actually have written this just as y equal to w x where w is equal to w four w three \nw two w one so there is no depth here there is actually  only one weight which i could have \nlearned you get that right if you just have all linear transformations then essentially you \ndo not have so many weights you just have one weigh t throughout you get that make \nsense \nrefer slide time onefiftythree \n \nso what you are learning eventually we will just y as a linear function of x and initially \nat some point we started off with such linear functions right w transpose x in the case of \nperceptron and mp neurons \nso what does that lead to what kind of decision boundaries does that lead to  linear \ndecision boundary  so if you do not have these  nonlinearities we cannot have these \narbitrary decision boundaries will only be left with linear decision boundaries \nrefer slide time twotwenty \n \nin particular will not be able to solve this problem that we had right we were given some \nred and blue points  and there was no way to  draw a line such that the red points are \nseparated from the blue points what  we needed is some kind of circles or ellipses to \nseparate the red points from the blue points that cannot be done with linear decision \nboundaries that can happen only if you use a deep neural network with non linear \ndecision boundaries and we actually h ave a proof for that  what that proof the universal \napproximation theorem actually towards right \nrefer slide time twofiftyone \n \nso that is why nonlinearities or the activation functions clear a very important role in the \nsuccess of deep neural networks  right hence you want to examine them very closely \nand see  what are the newer kinds of nonlinearities that have been proposed so we \nalways start with the basics so will start with sigmoid see what are the problems with \nsigmoid and then see what we can do to solve some of these problems \nrefer slide time threethirteen \n \nso this is what the sigmoid function looks like you have seen it a million times and it \nactually constrains the input to zero to one right  so it takes some input and it constrains it two \nvalues between zero to one  now since we are alway s interested in gradients right  because \nthe entire training and that is why i did that precursor in the  first module the training \nalways depends on gradients \nso it is always important to look at what does the gradie nt look like so we know what \nthe gradient looks like we have computed this is just sigmoid of x into one minus sigmoid \nof x so now let us see what happens if you use such a sigmoid ne uron in a deep neural \nnetwork \nrefer slide time threefortynine \n \nthis is a deep neural network and without loss of generality i am going to use a thin deep \nnetwork but the same holds for a deep  for a wide deep network also  so suppose you \nare interested in computing the gradient with respect to w two  right at some point in the \nchain rule you will have this term how many of you are convinced about this  ok and \nthat will lead to this could that cause a problem \nso at some one of the terms in your chain rule is going to be  this dou h three by dou a three i \nam assuming all of you are co nvinced about that and i have given you the exact formula \nfor dou h three by dou a three will that lead to a problem \nstudent refer time fourthirtytwo \ngood so what is the consequence of this to answer this we need to understand the \nconcept of saturation right \nrefer slide time fourforty \n \nso a sigmoid neuron is said to have saturated if it is output is  one or zero or rather close to one \nor close to zero ok what would happen in that case to the gradient \nstudent refer time fourfortyeight \nit will vanish right because sigmoid of x into one minus sigmoid of x so it either \nextremes is going to vanish and you do not even need the formula for that you can just \nsee it from the diagram right because the gradient here  is going to be zero that is obvious \nright it just a what horizontal line \nso this gradient would be zero  so fine why does it bother us  what is our entire training \npremise based on gradients right what does our update rule what happens if this guy \nis zero no update e the weights just stay where they are right that me ans the training \ngets stalled right \nso think about this right i f all the neurons in your network have saturated  that means \nall the weights the gradients will be zero that means all the weights will remain where they \nare you pass another input nothing  is going to change right it still be zero so if this \nneurons have saturated your training will just stalled  ok so that was one of the reasons \nwhich is to cause problem in training deep neural networks earlier right \nrefer slide time sixfive \n \nso that is one of the reason why it was not converging because these weights used to  \nthese neurons is to  saturate so this is one problem with sigmoid neurons a saturated \nsigmoid neuron can cause the gradient to vanish \nrefer slide time sixfifteen \n \nbut why would the neurons saturated i mean  what would cause them to saturate ok \nthis saturate find their gradients will vanish but why would they saturate we should be \nable to get some hints from the figure that has been drawn  so this is actually  that x \nneeds to be changed \nso on the x axis we have x quite obviously but that has to be something else   so what \nit is what is happening is what does the  sigmoid neuron do it takes this aggregate it \nor someone just disappear refer time seventwentysix  so is it very b oring today no right  so \nyou have this aggregated sum of the inputs once you have that aggregation you applied \nthe sigmoid now tell me when would it saturated \nstudent refer time seventhirtynine \nwhen the aggregation is very large that means one of the two thi ngs could happen either \nthe x\u2019s are very large or the w\u2019s are very large  would the x is x is be large i see a lot of \nyou saying no why \nstudent refer time sevenfortyfive \ngood we normalize them right  we make sure they are between zero to one  so we do not \nallow those arbitrary large values of p ressure density and so on right  we make sure \nthey are between zero to one  so then the weights can be a problem right  now why would \nthe weights be lies move later first \nstudent refer time sevenfiftyeight \nif i initialize the wei ghts to a large value if i initialize all my weights  in my infinite \nwisdom to a large value  what would happen right from the first training example itself \nw i x i would take on a very large value and your neurons will star t saturating  so \nimagine if all the weights throughout my network are initialized to large values \nthen right from training instance zero my neurons will start saturating and i will not be \nable to train anything how many of you experienced this while doing back propagation \nand the othe rs did not do the assignment they copied it  please raise your hands how \nmany of you experienced it now many more hands will be raised still now ok honest \npeople that is a paradox \nrefer slide time eighttwenty \n \nconsider what would happen if you use sigmoi d neurons and initialize the weights to a \nvery high value they will start saturating and hence you will have this problem of \nvanishing gradients ok everyone gets this so this is a problem at this sigmoid neurons \nrefer slide time eightthirtyseven \n \nthe other problem with sigmoid neurons which is very interesting is that they are not zero \ncentered what do i mean by that  they are not zero center that is what it  ok so sigmoid \nis are not zero centered what do i mean with that mean  by that they are not zero centered  \nthe value is between zero to one  right so the average cannot be zero it is always going to be \nabove zero ok sigmoid neurons are always going to take on positive values between zero to one \nso why is that a problem  so that is  an interesting explanation  oh did i say th at did i \nput the acknowledgements somewhere so all of this material that i have been talking \nabout it is taken from andrej karpathys lecture notes so here is this interesting \nexplanation for this \nso now consider this particular network  ok and i am go ing to focus only on this part  \nthat means the output layer and just the layer before that and the layer before that  has \nthese two weights wone and w two i am going to focus on that \nso to update these weights i need to compute so what do we need to comput e \ngradient ok now you will answer  so we need to compute the gradient with respect to \nwone and w two  and this is what it is going to look like  what is the red part and blue part \nwhy red and blue the red part is dash for both common for both right \nso this is going to be common i do not know why i did that ok  refer time ninethirtynine so \nthis red part is common for both and what is the blue part actually what is dou a three by \ndou w one h two one and dou a three by dou w two \n so dou a three by dou w one  is just h two one and dou a three  by dou w two is just h two two  ok so let me \njust plug in those values and note that h two one and h three are between zero to one so can you make \nsome interesting commentary on this  interesting but useful not just philosophical stuff \nthat these two derivatives are for t he weights at a given layer i have just taken two weights \nbut i could have taken n weights and the same thing would have hold \nbecause i know that the derivative is proportional to the input that it gets and the rest of \nthe part is going to be constant because that is coming from the chain rule up to the \nprevious layer right \nso now what is happening because of that  just to make fun of you guys i mean if you \nget that  sorry good yeah it is not very straightforward but let us see  so if the first \ncommon term in red is positive right  then what would happen to these two guys  they \nwould both be positive right because h two one and h two two are positive \nnow the first common term in red is negative then what would happen to these two guys  \nboth negative so th at means the gradients of the weights at a particular layer where \neither all be positive or  they will all be negative you get that that is because of this \ncommon part and the blue part the blue part we know is positive \nso what matters is the common p art and that common part can  either be positive or \nnegative for all of them together right that means for a given layer all the gradients at a \nlayer are either positive or they are all  negative so let us see what is the implic ation of \nthat right \nrefer slide time twelvefourteen \n \nso this actually restricts the possible update directions \nso which is the quadrant which has all positive  first ok sorry for embarrassing yeah \nand all the negative is the third quadrant that means your movements can only happ en \nin the first quadrant and the third quadrant so do you see a problem with this right so \nyou are going to actually try to move that your theta \nwhich is a collection of w one and w two is theta minus eta into the gradient  right and you \nknow that this vector which is the gradient vector can either be positive that means can \nlie in the first quadrant or it can lie in the third quadrant  these movements are not \npossible that means there are certain turns or certain movements or certain directions \nthat i am not allowed to take so what would this mean  it would take a dash time to \nconverge \nstudent longer time \nlonger time to converge right because i am restricting my movement s o imagine you \nhave to go from destination to destination b  and i say that you can never take a right turn \nright and there is some going to be some problem  it will take longer to reach there \nunless the directions are to our left right unless your destination is refer time thirteentwentytwo  \nbut that will not happen \nrefer slide time thirteentwentyfive \n \nso suppose this is the optimal w star \nrefer slide time thirteentwentynine \n \nand we start with some random initialization because that is why we are going to start  \nthen the only way i can reach it is i may by making a series of this kind of movements \nright as the exact pattern is what will have to take because these are the only \nmovements which are allowed or some movements which are allowed and it will lead to \na certain cryptic pattern and i will not be able to have the complete freedom of moving in \nthe direction which would have directly taken me to the optimal \nso that is a problem with  something not being zero center  and lastly sigmoids are \nexpensive to compute because  you have to do this exp right it is not something as \neasier as something else  that we will see in the  lecture today ok so these are some \nproblems with sigmoid functions  \nstudent refer time fourteenfifteen \nso this is some issues that were they with sigmoid functions so this pointed that  ok \nmaybe we should try better activation functions \nrefer slide time fourteentwentythree \n \nthat is why tanh become very popular but tanh is not something which happened post \ntwo thousand and six right so this was like ninetytwo or ninetythree  when i think yan lacunae had started  moving to \ntanh from sigmoid functions right  now again here other inputs are compressed between \nminus point to one  ok where inputs are now zero centered  which takes care of this problem \nwhich i mentioned at the end \nthat these directions of movements are constrained and was the derivative of this \nfunction one minus tanh  square right what happens at saturation even without looking at \nthe formula the gradient would vanish to zero right so the vanishing gradient problem is \nstill there \nwhat you have solved is a problem of zero centering and that itself used to give better \nresults than just using a sigmoid function  but it is still computationally expensive \nbecause you still have to do these e raise two components right the you still have to \ncompute these exponential powers so it is still computationally expensive \nrefer slide time fifteenseventeen \n \nso then in around two thousand and twelve i guess  is when this relu was introduced in the context of \nconvolutional neural networks right  and this is what the relu function actually looks \nlike is this a non linear function it just looks like a line rig ht why is it a non linear \nfunction it is a nonlinear function right because x is you cannot write x the output as \na function of i mean a s a linear transformation right  so you have this zero in fact if you \ntake two relu functions smartly \nrefer slide time fifteenfifty \n \nyou can actually get the  sigmoid i mean you can get an approximate for the sigmoid \nfunction so you can go back and check this right so if you take these two functions and \nsubtract one from the other  what is this  this is a relu function  this is also a relu \nfunction right \nso i define relu as max of zero comma x so both of these are relu functions some \nvariant of that and now if you subtract one from the other you will actually get a  \napproximation of the sigmoid function right and this cannot happen if you have two linear \nfunctions take any two linear functions you will not be able to get this kind of an \napproximation \nrefer slide time sixteentwentyseven \n \nso relu is a nonlinear function what are the advantages of relu one is it does not \nsaturate in the positive region right  it is computationally very efficient the output is \neither zero or x there is no powers nothing like that right and it practice it converges much \nfaster than sigmoid and tanh so that is what this two thousand and twelve paper show and now relu has \nactually become more or less the standard in all convolutional neural networks \nrefer slide time sixteenfiftythree \n \nbut there is still a caveat  while using relu ok so the derivative of relu we can see \nthat if x is less than zero then the derivative is going to be zero right and if x is greater than \nzero then the derivative is going to be one and that straight away  follows from the definition \nof relu which is zero or x \nso when it is zero the derivative will be zero  and when it is x the derivative will be one  so now \nconsider this given network  and let us assume and this is not a very far faced assumed \nassumption it can happen in practice that at some point a large gradient causes the bias \nb to be updated to a large negative value so what i am saying is that something happens \nand b gets updated to a large negative value \nrefer slide time seventeenfortyone \n \nnow what would happen to this quantity  remember this quantity which i have circled is \nactually the input to the blue colored relu neuron that i have so i am asking you w hat \nwould happen to that input that input would become negative \nso the neuron would output zero and i am calling it a dead neuron why  if the input is zero i \nmean is a input is negative then the relu functions  output would be zero what would \nhappen to the g radients during back propagation zero that means what would happen to \nthe weights \nstudent refer time eighteensix \nwould not be updated right now but that is fine right if you give some other input this \nwill recover why am i calling a dead means permanent  right unless you are in some \nfantasy world but dead is dead right  so why am i saying that it is dead i could might \nas well i would give it a next input and then probably things would be  ok bias is still \nvery negative because nothing is getting updat ed right or bias is still very negative  you \nknow that x one and x two are constrained because you have normalized them right and w one \nand w two have not been updated \nso still the situation does not change so what happens is that once a relu neuron dies \nbecause somewhere in the chain rule you got a zero  it will stay dead forever ok it will \nnever be able to come out of that it will  always produce a negative output that means  \nthat output will be clamped to zero that means no gradients will flow back and that means \nall the weights will not get updated connected that neuron \nrefer slide time nineteensixteen \n \nso in practice when you train a network with relu  you will observe that a large \nfraction of the units can die if the learning rate is set too high why this if condition \nstudent refer time nineteenthirty \nwhat was the assumption that i made that the bias receives a large negative update and \nthat is possible if your learning rate is very high because you got some small negative \ngradient but your learning rate blew it up \nnow what is the practical implication of this if a training a network and a large number \nof your relu neurons have died what does it mean  most parts of your network are  \ndash useless they are not learning any feature nothing right is all zero that means you have \nthis large number of parameters versus getting wasted  because they feed into a relu \nyou function and the relu function just keeps outputting zero \n so if you have n neurons in the particular layer and most of them are zero that means y ou \nare not really learning an n dimensional feature representation you are just learning a \nmuch smaller feature representation  right so can you give me a simple way of one \nsimple way of avoiding this among many other ways \nstudent refer time twentyoneseven \nno dropout is statistical right it is probabilistic  this is like always dead  one thing is to \nupdate the weight to a large to a positive value and zeroone mind you is a large positive \nvalue right later on we will see y but zeroone is reasonably large ok so were going to \ninitialize the bias to a positive value \nso that even if this large negative gradient flows through there is still a chance that it \nwill not become very negative and hence it will not  mess up the things the way it does \nthat is one solutio n to that right but still you will find that even after that the relu \nneuron a lot of those can die but still in practice they work better for a deep \nconvolutional neural network ok and we can also use other variants of relu \nrefer slide time twentyonethirtyone \n \nso there have been to avoid this dead neuron problem there are other variants of \nrelu which have been proposed  and that is what we look at next  so there is \nsomething known as a leaky relu  is it obvious from the equation what it does right \nso inst ead of producing zero  it will just produce a very small value proportional to the \ninput now what would happen to the gradients  they will not saturate right will have \nthe gradient would be if the input is negative what would the gradient be \nstudent refer time twentyonefiftysix \nzeroone right so that means some gradient will still flow through how many if you get \nthis right so that means if you use a leaky  relu neuron some gradient would still \nflows through so just understand this trend right that ah and this is i mean all this stuff \nis simple there is nothing great in this but just put it in context right \nso in two thousand and six to two thousand and nine people realized ok now we can trained networks and maybe \nwhatever we have done with unsupervised pre training  actually corresponds to better \ninitializations or better optimizations or better activations and so on \nso now let us try doing research in that so that led to the discovery of relu now \npeople started observing problems with relu and then proposed a variant of it which is \nleaky relu right so that is how this area has now become very prolific and grow right \nso we started off with this seed idea that it is possible to train these deep neural \nnetworks and now we are trying to make arrive at better and better ways of doing it \nmaking it more and more easier to train them and take care of some of these \nirregularities which existed earlier so one of them being sigmoid not being a very neat \nfunction to optimize with right so that is what all this is about  individually all of these \nare probably easy for you to understand once you go back and look at the slides you all \nthis is nothing great in this \nbut what i want you to really understand is this bigger picture of what is happening here \nas long as you get that time frame w ith and of course leaky relu is again \ncomputationally very efficient there is no exponents no squares nothing like that and it \nis close to zero centered and it is still not zero centered but close to zero centers because you \nhave outputs on both side and then someone came up with a generalization of this which \nis parametric relu so y zeroone make it alpha x and alpha will also be a \nstudent parameter \nparameter it is a trainable parameter it is not a hyper parameter ok how many of you \nknow the difference b etween parameter and hyper parameter  ok you have used this in \nthe back propagation as i am right  so it is a trainable hyper parameter it will get \noptimized along with your other parameters in the network \nrefer slide time twentythreefiftyfive \n \nso then someone sa id leaky relu fine parametric relu is fine let us try to do \nexponential relu ok so it has all the benefits of relu  it ensures that at least a small \ngradient will flow through even when your inputs are negative that means it avoids this \ndead neuron problem again close to zero centered outputs  but it is expensive because now \nwe have added this exponential right \nso these are all ideas which came out during this period and all of them were shown to \nwork better than the other and so on and of course at t he end i have to tell you a final \nconclusion right whenever i give you so many possibilities \nso i have given you sigmoid tanh relu parametric leaky exponential now what do \nyou use right this the idea is not to  confuse you but to give you one s olution which \nwould largely work yeah what regularization \nstudent refer time twentyfourfortyfive \nyeah you could have done yeah that refer time twentyfourfortyeight there is  exactly so a lot of \nthis research right which has happened in this period  it is not a lot of it is juristic right \nyou solve one problem with relu ok the neurons and saturated ok just make it \nsomething which does not saturated \nrefer slide time twentyfivefive \n \nso that is there it is possible that the other solutions would also go there is not that thi s \nis the only solution which works  now then someone came out with max out neuron \nwhich is a generalization of relu and  leaky relu why do i say it is a generalization  \nwhat was relu that means w one equal to b one equal to zero w two equal to one \nso it is a spe cial case of the max out neuron what about  leaky relu  this was \nparametric value but again what about so now what is happening  w one equal to alpha  b \none equal to zero w two equal to one b two equal to zero so you see how it generalizes right so this is \nhow these variants keep kept coming up  \nrefer slide time twentysixfive \n \nnow the problem of course is doubles the number of parameters right because you \nearlier had only w transpose x plus b now you have w one transpose b one  w two transpose b \ntwo and so on right so it is actually doubling the number of parameters that you have \nrefer slide time twentysixeighteen \n \nso now coming to the final conclusion of all of this right what you need to remember is \nthat sigmoids are bad \nso no one uses sigmoids in convolutional neural  networks they still use somewhere  i \nam i am sorry about this relu is more or less the standard unit for convolutional neural \nnetworks \nso any standard cnn that you will pick up it will use relu as the activation function \nif you want you can explore leaky relu max out elu and so on but it will require a lot \nof careful tuning say if you want to use something out of the bulk box relu is just fine \nrelu just works fine in practice despite all this dead neuron and other problems \nstudent refer time twentyseventwenty \nyeah so then the argument for that is that how often when you reach the point x equal \nto zero right so the chance of that having is happening is very very low and if you get \nthere you can always approximate it by some epsilon or something  and for that training \ninstance just go on right  any ways you are making so many approximations with \nstochastic and mini batch and so on \nso this is one more approximation that is how people typically deal with it but in most \ncases it will not come in that point  appearing is very low  but the question is valid  and \ntanh sigmoids are still used in lstm\u2019s and rnn\u2019 s which you will  see at some later \npoint in the course  ok so there are a couple of more modules that i need to do so we \njust take a break here"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 9.4 Better initialization strategies.wav", "duration": 1550.54, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 ninefour \nlecture \u2013 nine \nbetter initialization strategies \nso in this module we will talk about better initialization strategies \nrefer slide time zeroseventeen \nso this is where we are in the story right we saw that deep learning  has evolved and at \nleast these are the four things which have happened so we have  by the way this slide  is \nincomplete what are the other things which have happene d actually which you already \nsaid in the beginning  two more things which are not technical but  which happened more \ndata right \nand more compute  but these are not really technical in the sense that i mean this just \nhappened we have large amounts of data t hat means more data means what if you are \nmore data for training you would have complex networks but not over fit right because \nyou have so many so much of data right  and more compute of course it speeds up \nsome of these matrix computations which happen \nso remember in a deep neural network  most of the things which are doing are matrix \nmatrix operations right  you are taking are that is what exactly you did in your back \npropagation assignment you did a lot of matrix vector computations  and so on and the \nadvent of gpu\u2019s this became very very fast rate orders of magnitude fast \nso this two which are here as nothing much to talk about that is just something  we all \nunderstand what has happened they and so now we will talk about better weight \ninitialization strategies \nrefer slide time onetwentyseven \n \nso let us start with this question right we will take this network  and we will ask this \nquestion what happens if we initialize all the weights to zero  i like it when you all try to \nvisualize it and  ok so you have to see what happens right so let us start with a one one  \nwhich is w one one x one plus w one two x two \nso i always start small l i do not try to see what will happen everywhere just start with \none neuron and see what happens  take a one two  what would a one mo nth\u2019s value b if all \nthe weights are initialized to zero  zero and a one two  again zero right and same for a one three  it is all the \nway all the neurons in this layer are going to be zero till is it \nso that means they will all get the same activation so if the as are sam e the h s are \nalso going to be same and that is obvious irrespective of what non linearity you use now \nwhat will happen during back propagation  what will delta w one one b  this again i do not \nknow why you do this ok anyway that will be erase it into x one \nso remember that the gradient is always proportional to the input and you have \nsomewhere along the lines  along the chain rule you have this h one one and a one one just \nremember that and what would delta at gradient of w two one b is that fine \nnow can you see some  things on the left hand side and make some comments on the \ngradients we have seen that a one one is same as a one two and h one one is same as h one two that means \nthese gradients are going to be  equal right that means the weight started off at the same \nvalue they are going to get the same updates and again remain at the  same or different \nvalue but this same right then of course move from where you started will not be zero \nanymore but they will all be at the same value \nrefer slide time threetwentyfour \n \nboth the weights will get updated with the same value and they will remain equal  so but \nfine as i keep training they will move  away from each other right this is what i told \nyou when you feed in the first example take a both the weights remain the same but \nnow if you feed another example and you keep feeding batches there there is no dearth \nof data that you have and eventually these weights will move away from each other \nthe update is the same again the weights are the same  again the same situation will \nhold right again your w one one x one plus w one two x two is going to be the same as w two and x one \nplus w two two x two and the same argument repeats \nhow many if you get this  ok so once you initialize the weights to zero  in all subsequent \niterations the weights are going to remain th e same i mean they will move away from \nzero but they will all be equal  ok and this symmetry will never break during training so \nwhat actually is happening in terms of the capacity of the network this is same as \nstudent single line tying the weights \nthe same as tying the weights so this symmetry will never break during training so  \nasking what is the net effect which is happening \nso you have so many weights in your layer  but all of them are moving together will  so \nin essence you do not have the same freedom as you have with n different weights right \nhere in some sense unintentionally tied them because it started off with the same value \nnow you are all moving at the same  values rate you are all going to the same value so \nyou do not really have  the amount of freedom that you would actually expect with n \ndifferent parameters all of you get this \nand the same is true for w one two and w two two also which are the weights connected to the \nsecond neuron  and this is in fact true for all the weights in laye r two  you can actually \nmathematically verify it that means whatever this  small analysis that i did here  just go \nback and do it for all the weights in the network  and you will see that all of them if you \nare going to initialize them to zero all of them are going to remain equal \nthis is known as this symmetry breaking problem  this is are known problem this is \nexisted much before two thousand and six and so on if we initialize all the weights to zero you will have the \nsymmetry breaking problem  is there a nything sacrosanct abo ut zero or would this happen \neven if you initializer to same but non zero values and that should have been cleared from \nthe iteration right because after the first iteration we were at non zero weights  and after \nthat the story repeated right \nso even if you in itialize it to non zero weights the same story is going to report repeat \nso that means as long as you initialize all the weights to the same value  you are going \nto end up with this symmetry breaking problem ok which is not good  so what is it that \nwe have learnt about initializing weights \nstudent refer time sixthirtytwo \ndefinitely do not initialize all weights to zero  definitely do not realize them to the same \nvalue ok this is the first thing that you have learned so we are seeing different ways of \nnot making the light bulb and then we will come to a way of making it  so zero and \nequalist no bad yes some weights will not get updates in that case right \nso then that that should be fine so that is the other thing i wanted to make at some point \nright these four things right initialization  optimization regularization and  activation \nfunction these are not independent things they are all tied to each other  \nso as you said now if you use regularization then probably you could be a bit careless \nwith the init ialization even if you had initialize the weights  together drop out would \nhave ensured that some of these weights are not active at a particular training instance  \nthat means they will not get weight updates that means they will move away from the \nother weights \nso that is this is not that only one of  these things can be done right  you are going to \nuse a combination of these things but while analyzing them we will just look at  one of \nthese things assuming that the others are not being right  so will assume that we are \nnot using drop order anything is that fine \nrefer slide time sevenone \n \nso this at least  this in practice you are not supposed to initialize the weights to zeros and \nequal values that is what we have learned so far  now for the rest of the to convince you \nabout some other weight initialization methods  what i am going to do is i am going to \ntake a feed forward network  where you have as input some thousand points  each of \nthis point is five hundred dimensional  and the input data is drawn fro m a unit gaussian what i \nmean by that is you have this x one two x one five hundred rate for the data instance one \nso all of these five hundred dimensions come from a unit gaussian is that fine so this comes \nfrom a unit gaussian this comes from a unit gaussian and so on ok  that is what i am \ngoing to assume \nrefer slide time sevenfifty \n \nand the network has five layers each layer has five hundred neurons the input is five hundred neurons each \nof the five layers is also five hundred neurons  and now we will run forward propagation  no \nbackward propagation no loss nothing and i am not even giving you an objective this is \njust some input  and i just want to see what happens up to the last layer i am not even \nbothered about the actual last layer that means i am not trying to minimize any cost \nentropy squared error loss anything \nrefer slide time eighteighteen \n \nso let us try a few initialization strategies so we realize zero is not good  realize equal is \nnot good so let us try some random initializations but small weights  ok and this is my \nway of randomly initializing with small weights \nso my w is a matrix of size fan in into fan out rate which is n cross n ok the number of \nweights coming in and out rate so n cross n  and i am drawing from a uniform \ndistribution and then multiplying it by point zero one which en sures that all the weights are \nvery small you get the setup  now with this i am going to start with the input and then \nkeep doing these transformations \nso i will do w transpose x plus b pass it through a sigmoid and do this five times \nbecause i have five de ep layers now this is what happens to the activations across the five \nlayers so the first layer remember that we had drawn from a unit gaussian right so \nthat is what the data input data looks like so this is the first layer which is the input data \nbasically and then this is what happens across the different layers \nso what is actually happening  and this is for the tan h activation function  there is no \nvariance in the output of so this tells me so this basically tells me that  for all the \nneurons what is the average value that i am getting right and i should ideally get some \nhistogram that for some neurons i am getting the value minus one for some neurons minus \nzeronine zeroeight and so on  but what this is telling me is as i keep progressing across the layers \nall the neurons have very similar values and they are all close to zero \nthis is what actually happens in practice  i have just actually run it and computed the \nhistogram \nrefer slide time tentwo \n \nand if i use sigmoid activation functions again something similar all the values tend to \nbe close to the center which is zerofive  so this is zerofive and although i had started with a nice \ngaussian distribution \nrefer slide time tenfifteen \n \nnow what will happen during back propagation  so do not try to think for now tha t \nwhy this happens i am just telling you have actually run the code and this is what \nhappens now given that this has happened  what will happen during back \npropagation so all the activations in a layer are very close to zero all the gradients are \ngoing to be close to zero  that means no gradients are going to flow back that means \nwhich problem are we dealing with vanishing gradient problem \nso if you initialize your weights to very small values  and this is easy to see in the case \nof tan h so for tan h this is my function right and  this is zero now remember that this is \nwi summation wi xi  if all my weights are close to zero or very small v alues what is \nsummation w ix i going to be it is going to lie somewhere here right \nso all these inputs are actuall y going to be very close to zero now if my inputs are going \nto be close to zero i know that during back propagation at some point my gradien t is \nproportional to the input t hat i have given and when i say input here i mean layer one \nlayer two layer three and so on \nso that means all my inputs are very close to zero  now my gradients are actually \nproportional to the input so all my gradients are also going to be close to zero that means \nmy gradients are  vanishing right because remember that  across five layers you will have \nthese products of gradients right all of them are very close to zero  so you will end up \nwith something very close to zero raise to five how many if you get this right so our \ngradients are going to vanish \nrefer slide time twelvesix \n \nif you do this very s mall initialization of the weights and that is exactly what is \nhappening so this is the histogram for the gradients and i see that all my gradients are \nactually very close to zero  that means no effective training is happening my weights are \nnot receiving any updates this is what happens in practice if you initialize your weights \nto very small values \nrefer slide time twelvetwentyfive \n \nnow let us try to do the opposite of this very small values did not work so let me try \nlarge values and for large values i j ust sample from the uniform distribution  i will get \nsome numbers between zero to one  now can you guess what wil l happen remember \nsummation wi xi all your weights are large so why am i saying that number between zero \nto one is actually large  it is not by all pr actical because this is going to give me this \nfunction is actually going to give me numbers between zero to one why am i calling them \nlarge weights \nstudent refer time twelvefiftyfour \nno i will i just talked about the weights assume there is no biases how many of you get \nthat answer remember there are five hundred neurons so if you have five hundred small values that \nsummation is going to be still large right if you all of these are zerofour or zerofive which still \nlooks small but if you have two hundred and fifty of these or if you have five hundred of thes e the resultant sum \ncould be somewhere of the order of two hundred and fifty right and that is very large because if you pass \nthat to a sigma and neuron what will happen  saturation right so you get this why i am \ncalling these weights as large \nrefer slide time thirteenthirty \n \nso and this is actually what happens so when i have these tan h activations across all \nthe five layers i observe that my neurons saturate i either get minus one as the output or plus \none as the output and same thing happens if i use sigmoid activations i  either get zero as the \noutput or i get one as the output right neurons saturated means what will happen gradients \nwill vanish right \nso even if you initialize the weights to very large values all your gradients are going to \nbe close to zero because they are going to vanish and again you have a problem so what \nhave we seen so far zero is not good equal is not good small weights is not good large \nweight is not good then what do we do  \nrefer slide time fourteenfifteen \n \nso let us see what to do so let us try t o arrive at a more principled way of initializing \nweights and this again do not should the messenger i am going to give you a proof under \ncertain assumptions ok so just bear with me i just tell you what those assumptions are \ngoing to be as we go along so as i said right \nso i mean you would argue that in practice these assumptions do not hold true  but at \nleast they give us some insights into what is happening  right what is the overall idea \nbehind what is being proposed so let us start with tha t so now consider this deep \nneural network and i am just considering the first layer of it  where i have this neuron s \none one and i am talking about things before the activation \nso i know tha t  sone one is equal to this quantity  right so all the incoming we ights to the \nfirst neuron which is w one i into x i now for some reason i am not telling you why i am \ninterested in the variance of this can you tell me why i am interested in the variance \nwhat did you see in the previous examples there was no variance right there was \nhardly any variance so let us see what happens if you compute the variance of this \nso i am just taking the variance formula a variance of a sum is equal to the sum of the \nvariances right this is of the form variance of a into b where  a is w one i and b is x i \nwhat is the formula for this or if you know it or do not know it do not care  so this is \nthe formula \nso this is the generic formula for variance of a into b  where you have to assume that a \nis wonei and b is x i so this is just  a formula there is no trick here no math i mean no \nnothing fancy here just  apply the formula for variance of a b and substitute a is equal to \nw one i and b is equal to x i \nrefer slide time sixteenseven \n \nnow i will assume that all my inputs are zero mean fin e we have been assuming that \nforever and all my weights are also from zero mean ok what is the effect of that which \nquantities will disappear  this will disappear because mean as zero means the expected \nvalue of the weight is zero so the square of that is zero an  expected value of the input is zero the \nsquare of that is zero \nso what am i left with summation where i variance of xi into variance of wonei ok now \ni am going to assume that the variance of xi is equal to the variance of x that means it is \nthe same for all the i\u2019s so i had this remember i had these five hundred inputs \nso i am assuming that for all the inputs the variance is the same they all come from a \nsimilar variance distribution  and i am also going to make the same assumption for the \nweights fine and then i end up with this nea t formula that the variance of sone one is equal to \nn times the varianc e of w into variance of x right  because i assumed that all these \nterms are equal and there are n such terms everyone is fine with the maths so far with \nthe assumptions that we have \nrefer slide time seventeenseventeen \n \nso in general for any of these neurons right instead of just seleven i could take any sone i and \nthis is what the variance is going to be variance would turn out to be because i have \nassumed that all the weights an d all the inputs come out from the same variance \ndistribution ok from a distribution having the same variance \nnow let us what would happen if this quantity is very gr eater than one the variance of \nsonei would be  very large  right and what would happen if this variance tends to zero \nvariance would be very low so i am just giving you two extremes to build the intuition \nand let us see what we are going to do with that intuition fine \nrefer slide time eighteenone \n \nnow let me add one more layer and see so i have added one more layer and using the \nsame procedure as above  he will arrive at variance of stwo one is actually given by this \nformula and actually what has happened here  is that this is s i had x i earlier but now \ninstead of xi i have sonei because those are the inputs to this layer right \nso this is exactly the formula that we had arrived at earlier assuming zero mean and the \nsame variance for all the weights and the i nputs and i am  arriving in the same formula \nfor the next layer where instead of x i have sonei \nso this will result in this quantity ok but i already had a formula for a variance of sonei \nwhat was that  n into this quantity so i will just substituted it there so i can say that \nvariance of stwoi is actually equal to this i just substituted this value \nso that turns out to be i have a square here when i have two here  so you see where i am \nheaded with this what will be the variance of s k i this raised to  k and is on everyone \ngets this ok i can just continue the same analysis  and i have assumed that these weights \nand always are the same variance right \nrefer slide time nineteentwentynine \n \nso in general i can say  this ok now can you tell me something about when would this \nvariance vanish when n variance of w is \nstudent less than one \nless than one ok a nd which is the thing that we should aim for you would want this \nquantity to be equal to one in which case it will neither blow up nor shrink fine so so it \nto ensure that the variance is the output of any layer does not blow up or shrink  we \nshould ensure that n into variance of w is equal to one right  so what is this this just take \na minute to understand this i am saying that i am going to initialize my weights \nso i should initialize them in such a way  that the weights are coming from some \ndistribution like we saw that the distribution was a uniform distribution from where i \nwas drawing the weights \nso they are coming from some distribution i should try to draw them from a distribution \nsuch that this condition holds  if this condition holds the n across layers my activations \nwill not blow up or shrink  though that is exactly what was happening in the earlier case \nwhen i was doing those bad initializations with small values and large values \nso let us see how to do that so what i am going to d o is i am going to consider a \nrandom variable z ok where is z comes from a normal distribution ok and i am going to \nscale it is value i will draw from there and then i am going to scale it by one by square root \nof n what is n  number neurons in each layer right here it is the same across all layers \nbut it could also be different so i am considering a particular layer  and n is the number \nof neurons in that layer \nand now if w is actually equal to z by square root of n then i can write that n into \nvariance of w is actually equal to this quantity  everyone is fine with this there is no \ntrickery here i am just saying that why i am doing this is not clear  that will become \nclear but at least what i am doing is clear  i am drawing the weights i am taking a \nrandom variable z which comes from a normal distribution  and then i am setting my \nweights to whatever values i have drawn  i just divide them by the square root of n \nrefer slide time twentyonefortynine \n \nnow let us see  what is variance of a z  a square into var iance of z  hey that is a basic \nformula all of us know this  so now what is variance of z by one in z into one by square \nroot of n one by n into variance of z right so the n and n cancel and what is variance of z \nwhat did i assume about z it came from a normal distribution zero mean and unit variance \nso variance of z is one that means this quantity n variance of w is going to be one if i have \ninitialized my weights such that  they are equal to this  r ight and now do you see \nwhether the weights are very small  very large or what are they some now they made the \nweights dependent on the number of neurons \nso if you have very large number of neurons  you are drawing drawing weights such \nthat or you are initializing weights  such that it is some normal variable di vided by the \nsquare root of n right so now when you do this summation w i x i  your summation \ncannot blow up because you have already divided it by n \nhow many if you get this  so this is a standard way used for initializing weights how \nmany if you tried this for your back propagation assignment why did you try this ah \nstudent refer time twentythreetwelve \nbecause you are having some problem s with saturation i guess right  so this is how \nyou should initialize your weights this is  more or less the standard  technique and some \nvariant of this right  because instead of n you would have this fan in and fan in out it \nhow many weights are coming in and how many weights are going out \nso you make it proportional to the square root of n into k or something like  that right \nso but in general this idea right of course this proof we arrived at it with lot of \nassumptions but we at least got to some principle way of initializing weights and this is \na largely used standard this and some variants of it \nrefer slide time twentythreefortythree \n \nso now let us see if i actually take the same network that means five layers five hundred neurons at \nevery layer and then initialize it using this so this exactly what i had told you right that \ntake it from a unit distribution  sorry a normal distri bution and then divided by the \nsquare root of the number of neurons in that layer  and now let us see what happens \nacross the five layers \nyou see what happens we get this good variance in the activation functions they are not \nall going to zero or one or  point five right so this solves the purpose for tan h activation  and \nalso for the sigmoid activations \nrefer slide time twentyfoureighteen \n \nyou see a good spread in the weights  and remember actually for sigmoid  although \nthese values look close to each other but this is the zero to one range this is actually minus one \nto zero which will not happen for sigmoid so within the zero to one range you get a good spread \nif you initialize the weights this way \nrefer slide time twentyfourthirtyfive \n \nbut it turns out that this initialization does not work for the relu function in the relu \nfunction you still see this effect that you started off with a good spread but as you keep \ngoing across dep ths this spread disappears  why would that happen  to someone gave an \nintuition for this and is again one of those heuristic things that in the case of relu you \nneed to account for this divided by half because half of the relu is not active right \nhalf of the relu is zero \nrefer slide time twentyfiveten \n \nso you need to account for that fact and do this simple trick  that instead of taking the \nsquare root of the fan in you take the square root of fan in by two because you know that \nhalf the times it is not go ing to produce any output right  so that is a very simple \nheuristic that someone tried  and that leads to bett er activation functions  better \nactivations across all these layers right so as you see across all the layers the spread is \ngood now so the same idea ok so now you have a good way of initializing neurons \nso this should help you in your future assign ments fine so this is how what you have \nlearned about how to initialize your weights and it makes a lot of difference to how \nyour network will behave  right and that is what the  i was trying to show that by \ncomputing these activations across differe nt layers and i showed that as you change \nthese initializations strategies you get better activations"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 9.5 Batch Normalization.wav", "duration": 903.76, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 ninefive \nlecture \u2013 nine \nbatch normalization \nnow we will end with something known as batch normalization which is again almost \na defacto standard at least in convolutional neural networks  so if you are dealing with \nconvolutional neural networks you will use something known as batch normalization \nrefer slide time zerotwentysix \nso let us see what it is  so this is again something which is  some method which allows \nus to be less careful about initialization so let us see why that happens \nrefer slide time zerothirtysix \n \nso to understand the intuition behind this  let us consider a deep neural network  ok and \nlet us focus on the last  two layers h four and h three now typically will use some minibatch \nalgorithm for training right  so  we will use mini batch version of gradient descent or \nminibatch version of adam or any of these algorithms right \nnow what would happen if there is a constant change  in the distribution of h  three no just \nthink about the question that  i am trying to ask you  so as far as these  two layers are \nconcerned h three is the input and h four is the output it does  not matter what has happened  so \nfar or in particular does not matter  what x was whether it came from a normal \ndistribution or whatever distribution right \nat this point my input is h  three and my output is h  four now i am training it in mini batches  \nwhat if across batches my distribution of h three looks very different what would happen is \nit a good thing or a bad thing  it is a bad thing right so if you have training data right \njust think of  this as i said just focus on this layer  if you have an input which is not \nfollowing a fixed distribution is constantly changing  during your tra ining then that  is \nalways a bad thing right  because you try to adjust to  one distribution and now again the \ndistribution is completely changing  so that always makes our training very very \ndifficult right so if you have a very fluctuant distribution then a training is going to be \nhard ok so that is the intuition that i want to build \nrefer slide time twosix \n \nso now this could actually happen  so it would help if the pre activations at every layer \nare you need gaussians because for the input we made a case that will make the input as \nunit gaussian right \nso that things are very nice they are all coming all the inputs are coming from the same \ndistribution but we now realize that at every layer we have an input right it is not that the \noriginal input the only input even h three is an input even h four is an input and so on so why \nnot ensure that at every layer your inputs or  your h one h two h three also is something which \nlooks like a gaussian distribution which comes from a gaussian distribution why not \nensure that that is the basic idea behind batch normalization  and how do you do that is \nthe following  that you had computed this s i k just as we had done in the derivation  \nearlier right so s i k is one of these guys  \nnow if you do this what are  you actually doing you just normalizing it right you a re \nsubtracting the mean and dividing by the variance so that means you are making it zero \nmean unit variance and that  is the intuition which  i was trying to build that why not at \nevery layer have this good di stribution which is zero mean unit variance by even if you \nare feeding it multiple batches for that batch you will ensure that by this subtraction and \ndivision or the normalization process the  data will become unit  variance and zero mean \nok so now  at every batch the data is coming from the same distribution even if it was \noriginally from a distant different distribution  but how do we compute this mean and \nvariance \nso did you understand the question that i am asking i am focusing on this s i k i want to \nsubtract the mean of that  s i k how do  i do that  so the name gives it away batch \nnormalization it cannot be more explicit than that  so compute the mean for the current \nbatch and the variance for the current batch and normalize your inputs or normalize the s \ni k according to that you get this so now end up with a situation where all your inputs at \nevery layer across different mini batches seem to come from the same distribution is it \nfine the current batch so you take the average value from the current batch \nso then it will become zero mean for that batch and unit variance for that batch  and this \nyou are ensuring for every batch  so every independently every batch you are ensuring \nthat it comes from a  zero mean unit variance distribution  right so overall the effect is \nthat all the batches are coming from the same distribution no  so at validation time you \nwill compute the mean and variance from your entire data  entire training data once after \nthe training is done right \nso now we will computed from a minibatch and this is ensure that across mini batches \nnow your input always comes from a  zero mean unit variance distribution across all the \nlayers \nrefer slide time fivetwo \n \nthis is what a deep network will look like with batch normalization right so what will \nhappen is you passed an input you computed this tan h then you wi ll have this batch \nnormalization layer watch is what is the operation that the batch normalization is going \nto do this is the operation that it is going to talk  ok everyone gets that and now it gives \nme a unit normalized distribution sorry it gives me a input coming from a zero mean unit \nvariance distribution and then  i pass it to the next layer again at a  batch normalization \nlayer \nso after every layer you will actuall y add a batch normalization here  now my question \nis is this legal what is legal in this course anything that is differentiable  right so you \nhave to make sure that if we have added this operation it should be a differentiable \noperation so that you c an come  so now  the gradients have to flow all the way here \nright so that means i should be able to compute the gradients with respect to this  so \nnow this is  one of my a i and i should be able to compute dou a i with respect to \nsomething or rather the loss dou of the loss function with respect to a  i by turns out that \nthe operation that you have done is actually differentiable \nrefer slide time sixfour \n \nyou can actually work that out and it is not important i am not going to derive it because \nit is just yet another derivative that you wi ll take but it is a you should get the intuition \nfrom here right what you a re doing is this simple operation  and this just looks \ndifferentiable right \nso the operation that you a re doing is differentiable  so that is why you can add these \nbatch normalization layers and you can back propagate through this layer  but now what \nis the catch here it  somehow ties to the question that he was trying to ask  you are \nactually enforcing that all your are  zero mean and unit varia nce right so this is again \nsome sort of a constraint that you are enforcing right what if that is not the best situation \nin which the network can learn what if to distinguish between some classes it was ok if \nthe distribution was not same across all the batches they get this they are enforcing a \ncertain consider they are enforcing a certain condition on all the layers and all of them \nhave to be zero mean and unit variance but that may not always be good \nrefer slide time seventwo \n \nso they do something which is counterproductive let us see what that is why not let the \nnetwork decide what is best for it so after the batch normalization layer so this is what a \nnormalized s i k was after you have done that you compute a y k and this is not the fina l \noutput this is the output at the k\u2019th layer this is equal to this  why do they do this  and \nremember that gamma and beta are going to be  learnable parameters what are you doing \nactually you are again scaling it  and shifting it this is the same as adjusting the variance \nand the mean right \nso now what happens if the network learns the following  you get back the s i k so you \nhad taken s i k and you had normalized it but now if you allow these gammas and betas \nto be there in the network then the network can decide that maybe at this layer  i do not \nwant this normalization i just want to stick to whatever output i was getting \nso it could learn the gammas and betas in this way  and ensure that you get back the \nunnormalized s how many of you get this fine lot of you do  not seem to get  this but i \nam pretty sure if you go back and look at it you will get it  right so what is happening \nhere is that  is why i said it is counterproductive that you first forced it to make at unit \nmean and zero variance and now you added no zero mean and unit variance and now you \nadded this operation which is again a scaling and shifting operation  so remember that \nwhen you make the data  zero mean and unit variance that  is exactly what you do  you \nshift it so that it become zero mean and you scale it so that it becomes unit variance \nso you are again introducing parameters which again introduce  the same flexibility that \nyou could learn gamma and beta in such a way  that you could get back the original data  \nwhich was not normalized ok so if the network wants to learn that  and if the network \nfees that is the right thing to do  then it has the flexibility to learn those parameters  and \nyou can recover si \ni think the rationale is that your first  making is something which is more stand ard right \nand then from there trying to learn it instead of just trying to let it learn in the way  do \nyou get the difference between the  two the first bringing it to all of these things to some \nstandard value which is between i mean which is the normal d istribution and then from \nthere allowing it to learn wherever it has to learn right  that is the idea but it could be the \ncase that the other thing also works here \nrefer slide time ninethirtythree \n \nso now what we will do is we wi ll compare the performance with  and without batch \nnormalization on mnist data using two layers \nrefer slide time nineforty \n \nso here in this figure what i am going to draw is the validation loss am i no the training \nloss as i keep increasing the number of epochs and here  i am showing you the histogram \nof the activation functions at layer one so  i have trained a deep feed forward neural \nnetwork and i am showing you what do the activations look like at layer  one with and \nwithout batch normalization  so remember that we started with this intu ition that \nwithout batch normalization there would be this constant fluctuation  and the data would \nseem to come from different distribution at every training instance \nwhereas with batch normalization you are ensuring at your data comes from  zero mean \nunit variance distribution right and so that is one thing which i want to see another thing \ni want to see is that  how does it affect training right so that is the animation that i am \ngoing to show you so focus on all these three things i do not know how you will do it but \nfocus on this focus on this and focus on this with two eyes \nrefer slide time tenfortynine \n \nso let us see to see what happened right so this so now look at the focus on the leftmost \nfigure so that does not seem to change much with respect  to it is mean and variance  \nright but if you look at the middle  figure that  is constantly changing  it is mean and \nvariance right and you see the effect on the training loss  that the first  one which was \nwith batch normalization  that converges faster as compared to the second  one right \nagain an empirical result i am not really proving that this will always happen this is what \nempirically we observed \nso this was the story that we covered from one thousand nine hundred and eightysix to two thousand and six where back propagation was \nalready it was already discovered but was not working well  and there was this spark in \ntwo thousand and six that showed that  we could do some things  to make training really work for  deep \nneural networks but maybe that something is not sacrosanct  we could try different \nthings what we tried at that time was unsupervised free training  which is almost \nnonexistent now \nbut that lead that led to these thoughts that maybe this is because of optimization  \ngeneralization regularization activation functions and so on right so there was a lot of \nresearch in these different areas  and that led to a lot of developments which was better \noptimization algorithms  better regularization  better activation functions  better \ninitializations and batch normalization right \nso these a few concepts that you ha ve seen in the past few lectures  one being dropout \nand the other being  weight initialization using this xavier initialization or  he \ninitialization and this batch normalization right this is something which is all prevalent \nright so this is something tha t you will see in all deep neural networks that get trained  \ndefinitely in convolutional neural networks and more often than not even in recurrent \nneural networks so these are the two most popular types of neural networks so in both \nof these you will se e that these ideas are regularly applied  and they always lead to more \nstable training or better generalization \nrefer slide time twelvefortyseven \n \nso now this was all which happened till two thousand and sixteen or seventeen what has happened still since then  \nso there is still continuou s research in designing better optimization methods so  as i \nsaid after adam there was this eve which did not become very popular  but there is still \npeople looking at better optimization  methods and there is something which has been \ndeveloped on adam and came out in december last year \nnow people have also started looking at data  driven initialization methods right  so  \ninstead of having this fixed initialization which is  drawn from a unit or just  which is \ndrawn from a normal distribution and then just div ided by the square root of n  why not \nthink of data driven initialization methods that so there are some works on that again not \nvery popular because most of the shelf things that you will try will not really do any data \ndriven initialization \nbut if you really think that you are stuck at some  point then you could look at some of \nthese works and see  how they try to come up with initializations based on the data that \nyou are dealing with and now after batch normalization there have been some other types \nof normalizations which have been proposed  which seem to work better than batch \nnormalization but largely the stable configuration which has  kind of prevalent is  adam \nin terms of optimization xavier or he initialization in terms of initialization  relu in \nterms of activation functions  what else is there batch normalization in terms of  again \nregularization plus initialization and dropouts in terms of regularization right \nso these are roughly the key terms that you wi ll almost see in all the deeply living  deep \nneural network people that you see right  you will  always see when they describe the \nhyper parameters they will say that this is how we initialized is this is the drop out that \nwe use this is the batch normalization  and the training algorithm more oft en than not  is \ngoing to be adam \nso they have seen some very crucial elements of training deep neural networks over the \npast two to three lectures right and now  we will build on these and we will assume that this is \nwhat you are going to do  so now when i talk about neural networks like convolutional \nneural networks and  so on i not go back a nd tell you use adam or use batch \nnormalization or assume that you already know these things and you will try to train your \nnetworks using these tricks that we have your learned \nthe last couple of lectures have been about tips and tricks for deep neural networks and \nfrom here on in the next lecture will move on to  what wordtwovec because that is what \nyou need for your assignment  so in the next lecture we will do  a word representations \nso that is essentiall y seeing an application of feed forward neural  networks and from \nthere on we will move on to convolutional neural network"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.1 One-hot representations of words.wav", "duration": 525.57, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nlecture \u2013 ten \nlearning vectorial representations of words \n so today we are going to talk about learning vectorial representations for words \nrefer slide time zeroeighteen \nso these are the acknowledgement slash references for where are the things that i have \nreferred to by preparing for this lecture  so you can just go this some of these are also \navailable as video lectures on youtube can take a look at them also \nin the first module we are going to look at one hot representations of words \nrefer slide time zerothirtynine \n \nso as usual we will start with this motivation or motivation  motivating question why do \nwe need to learn representations f or words  vectorial representations for words  when \nwords are there right you can write them using alphabets and characters and so on so \nwhy do we need vectorial representations mention whatever you have seen so far in the \ncourse i have seen anything like this let us see \nso suppose you are given an input stream of words and it could be a sentence or \ndocumented if i say documented pretty much covers  almost all the text that you see \nright you can always abstract everything as a document and email is  also a document a \nmanuals are also documents and so on \nand we are interested in learning some function of it and so i am given a document \nand i am interested in the function y hat say y hat is equal to sentiments of the words in \nthe document or the sentiment of the document itself \nthis is imaginable this is not something that i am cooking up this is something that you \nwould want to do you would log on to for example if you are a movie maker you would \nwant to know once the movie is released people h ave written reviews about it what is a \nsentiment is positive or negative similarly if apple has released a new product or a new \nfeature you would want to know what are the reviews written about this product and \nwhat is the feature what is the sentiment coming out of it is positive or negative \nnow sentiment is a binary thing or it could be rated also right it could be on a scale of \none to ten also but let us consider it is binary that either people liked it or did not like it \nso now i am trying to lear n this function which gives me which takes as input words \nbut as output gives me real numbers either zero to one or on a scale or one to ten or whatever  \nright and this is not something that we have dealt within the course so far let how do we \ntake as input wo rd so all inputs have already always been  numbers right they were \neither coming from rn or they are coming from zero to one raise to n or something of that \nsort \nwe never had the situation when we have words as written so right so now how do we \ndeal with the situation and also i have made a case for that learning this function is a \nvalid thing to do you have several news cases where you will need this \nrefer slide time twofortyone \n \nso now if we employ a machine learning algorithm that some mathematical mo del so \nwe saw that we could have several such models logistic regression  svm and neural \nnetwork and feed forward neural networks and so on right and at the end we are trying \nto learn this function y hat is equal to f of x but in our case the x instea d of have be \ninstead of x being numbers it turns out that x is actually a collection of words \nso now how do we reconcile with the situation where we have suddenly have words \ninstead of numbers so the way to do that would be we need a way of converting t hese \nwords or documents into some number into some vectorial representation and once we \nhave this vectorial representation right so now we have r r raise d to n and we know \nhow to deal with r raise to n given r raise to n how to predict r or even r square or rm \nin general that we know right  we can design  neural networks or any other machine  \nlearning algorithm should do that but how do we go from here to here that is the \nquestion right and that is why we need to learn vectorial representations of words \nthis is a motivation clear to everyone okay now let us start getting with a refer \ntime threefiftytwo how to do that right \nrefer slide time threefiftyfour \n \nso now we will start hearing this word corpus have you heard this word before that is \nexactly what you are collecting for the word to like assignment right you are collecting a \ncorpus in specific languages and you have taken a very toyish corpus for the purpose \nof illustration so here is a corpus it just contains four sentences right so think of it that i \nhave a very restricted domain i have very small set of documents and i just have these four \nsentences with me this is the valid corpus \nthe corpus that you have constructing is probably much larger scale you are trying to \ncollect one hundred thousand sentence s or fifty thousand sentences or  something of that order \nright ah but we will take this toy example \nnow consider set v of all unique words across all these input streams so i just call \nthem input streams by input streams i mean sentences or documents o r whatever right \nyou could take it as any sequence of  words and v is set of all unique words across all \nthis input sentences that you have \nso can you tell me what v is here what would can you tell me some elements of v of \nthe set v human machine inte rface and so on so that is why in fact this is the entire \nset v which is written on the  left hand side right and v is called the vocabulary of the \ncorpus so that means everything in the corpus comes from this vocabulary all sentences \nare constructed by arranging words from this vocabulary \nsome of you might always i mean find this very trivial but i am just going over the \nbasics so that at least the terminology is clear to everyone and what we want is a \nrepresentation for every word in v so tha t is the title of the lecture learning vectorial \nrepresentations of words  so for every word in our vocabulary whatever corpus we are \ndealing with the vocabulary would change and for every word there you want to learn a \nrepresentation for that word so that is what our quest is today ok \nand now one very simple way of doing this is right you tell me  you want a vector and \nthat is all you care about here is a vector i will give you one hot representations so if a \ntotal number of words in my vocabulary is v i just construct a vector of size v ok and i \nhave assigned a number to ev ery word in my vocabulary right  so i will say human is \nequal to zero machine is equal to one interface is equal to two and so on \nand if you ask me for a vectorial representation of that word i will just say take this the \nor vector of size v and switch on the corresponding bit and anything else would be zero \nhence one hot right  at any given point of time only one of the elements in the vector \nwould be on so that is a simple one ho t representation as this is a very simple recipe to \nget a vectorial representation of words and for every word in your vocabulary \nrefer slide time sixthirtyone \n \nnow what is the drawback o f this v tends to be very larger right  so for example \nthere is a standard corpus known as the penn treebank corpus which is used in various \nnlp applications for various reasons and that corpus has a vocabulary of fifty k \ngoogle of course operates at it is own scale so they have a word one t corpus which has \nthirteen million  words so this is like all the most of the web pages that they have dr awn \nconstructed a vocabulary from that \nso now i am talking about for every word representing it by a vector of size thirteen million \ntheory does not good work right there is too much of s torage required for that and if \nyou look at that information in it is so redundant that is all zero except for that one bit which \nis on \nand the other important problem is that these representations do not capture any notion \nof similarity other three words that  i have shown you which  do you want to have similar \nrepresentations cat and dog why because both of them are domestic animals right  \nboth of them are mammals so there are some things that you would want at least at the \nminimum that the similarity between a cat and dog is more than the similarly between cat \nand truck \nor alternately the distance between cat and dog is less than the distance between cat and \ntruck so now once i start talking about vectors i can talk about similarities like cosine \nsimilarities or i can start talking about euclidian distance so once anything i convert it \nto a vector i can start asking these questions other two questions which i am asking are \nvalid right what would you expect to be the euclidian distance between cat and d og as \ncompared to cat and truck \nnow what happens with the one hot representations take any two words in your corpus \nany two what will be the euclidian distance what wil l it be square root of two right  for \nall the words take any two words in your corpus wh at will that cosine similarity mean zero \nbecause all these vectors are orthogonal  right so the cosine similarity is going to be zero \nbut this is that means these vectors are not really capturing any information abou t the \nessence of the word right \nso remember always we are interested even like that has been our philosophy right from \nauto encodes and so right or even principle component  analysis they are always \ninterested in learning meaningful representations which capture something fundamental \nabout the entity that w e are trying to represent right b ut here something like that is \nclearly not happened ok so that is not acceptable"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.10 Relation between SVD and Word2Vec.wav", "duration": 215.44, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 tentwo \nlecture \u2013 ten \ndistributed representations of words \nso what  we saw now was  sparse representations  or  one hot representation  sparse \nbecause only  one bit is  on from  there we  will  move on to something  known as \ndistributed representations of words and  you have  already seen that sparse  is  in \ntheory but it gives a very simple recipe of converting words to vectors  but it does not \nserve much purpose \nrefer slide time zerothirtysix \nso let us see what distributed representations of words are  so this around one thousand nine hundred and fiftyseven j r \nfirth made this very profound statement that  you shall know a word by the company it \nkeeps and this of course a play on some other similar code  but what does this actually \nmean so it means that  you want to know what does the word bank convey or what is \nthe essence of the word bank  right what this code says is that  if you want to know \nabout bank you should say you should see the company that it keeps  that means what \nare the other words  which appear typically in  it is neighborhood and of course when \nyou have a large amount of corpus given say the entire wikipedia \nof course at that time wikipedia did not exist but any large corpus and does this led to \nsomething known as distributional similarity based representations  so to understand  \nthis we first have to understand the idea of a cooccurrence matrix \nrefer slide time onethirtyeight \n \nso the basic idea is to use the accompanying words which in this example happen to be \nfinancial financial deposits credit etcetera to represent bank and to do that we will \nconstruct something known as a cooccurrence matrix which looks like this  right so a \ncooccurrence matrix is a terms cross terms matrix  that means every row in the matrix \ncorresponds to a term or a word and every column in the matrix also corresponds to a \nterm or a word \ncan you guess what how many rows would there be  size of the vocabulary how many \ncolumns would there be size of the vocabulary  right okay so here is how we construct \na cooccurrence matrix so we take a word we are interest interested in constructing the \nrow for that word the number of columns is the same as the size of the vocabulary \nnow for every column we will make an entry which tells us whether or how many times \nthis this word appeared in the context  of the target word right for example if i look at \nmachine i am looking at the row for machine i am trying to construct the en tries in that \nrow i know that the number of columns is equal to the all the unique words in my \nvocabulary \nso i look at the first word which is human and in that cell  i enter the value which is the \nnumber of times human appeared in a window of k words around machine is that fine is \nthat straightforward and that is how i will construct this cooccurrence matrix where i \nhave taken the window size as  two that means in any given cell  my entry would be the \nnumber of times human appeared within a  twoword window of machine is right the ig\u2019th \ncell clear  the definition of the ig\u2019th cell clear  so this tells me that user actually \nappeared two times around the word system in a window of two words around it is that clear \neveryone gets this how i construct the cooccurrence matrix ok now you could use the \nsame so this is known as words and this is known as context  that means the rows we \nrefer to them as words and the columns they refer to them as context now as you said \nthat the number of rows and the numbe r of columns can be same that we can consider \nthe same words in the context as well as the same word as the target words right \nbut you could also do something different you could say  that i do not want to consider \nall words that is context words because for example the word for appearing with any \nother word does not really give me much information because  it is just a stop word right \nor the word the or an d or a these are known as stop words in the language  these do not \nreally give me much information \nso if you go back to the bank example  financial credit deposit are the words which i \nreally care about and these are the  financial bank or with deposits  and also these words \ndo not really matter a lot ok do you get the intuition \nso you could choos e to have fewer columns  which are only the important words that \nyou consider and you ignore the sto p words ah in this discussion  i will alternately \nswitch between considering the columns as the same as the rows and sometimes are \nrestricting the columns to fewer number of entries \nrefer slide time fourthirtyeight \n \nso now each row gives us the vectorial representation of the word so we have seen \nhow we have moved from sparse representations to distributed representations  so now \ntake a guess now would this vector be sparse \nso we saw the extreme sparse right  which was one hot now the vectors which you get \nhere are they going to be dense or still sparse  sparse right because every word does \nnot appear with every other word right  you still have these v  dimensional vector and \nthere are some words which will appear with very few words  right so you expect to \nhave non zero entries in very few columns right so these representations are also sparse \nrefer slide time fivenineteen \n \nso there are some problems some o f which are fixable  so we look at the fixable \nproblems first the first thing as the stop words are very frequent  so these counts should \nbe very large \nso if you take the entire wikipedia corpus and you take the word machine or system  \nthen the words the and for and so on would have appeared like more than one thousand times \nin the context of the word machine  right and as compared to the other words like \nsystem or user they would have appeared much fewer times  so this kind of skews your \ncounts right it is like highly biased in the favor of stop words so how do you deal with \nthis \nrefer slide time fivefiftyfive \n \nso there are two ways one is ignore frequent words right  so that is the solution which  i \nsuggested earlier that your number of columns would be l ess than the number of words \nis that fine so you do not actually consider frequent words at all \nthe other is user threshold t so that means in these columns like for and with and so on \nwhatever be the entry if that crosses a certain threshold then i will just replace it by that \nthreshold is that clear \nso i am just saying that  this means that the word has appeared more than one hundred times \nand i am not interested in keeping the actual count which was more than one hundred i just \nsaying that  more than one hundred is enough for me right  because i know that all the other \nentries are going to be much less than this  so just like replacing it by a very large \nnumber instead of actually counting that number \nrefer slide time sixfortythree \n \n the other solution is instead of c ount you can use something known as pmi so this is \nhow you compute pmi even if you do not know it does not made a lot of difference \nbecause you know that it will always be there on the slide that is why you guys do not \nread anything  so pmi is comput ed like this  so intuitively tell me what does pmi \ncapture look i would say focus on this formula rather than the upper one when would \nit be high  the easier question to answer is when would it be low  remember you are \ndealing with a fraction \nso if independently the two words appear a lot of times  but together they appear very \nrarely then the pmi is going to be low is that clear  now if both the words appear one hundred \ntimes and together also they appear one hundred times that is the best case scenario  that means \nthese are very tightly tight words right they always tend to appear together \nso the pmi would be high for words which are very frequently cooccurring now so \nthis is what would happen  if you replace the counts  the cooccurrence counts by the \ncorresponding pmi ok now if the count of  two words is zero we have a problem because  \nthen the pmi tends to be minus infinity  right so how do you deal with this situation  \nepsilon or some we will use some hack right as usual \nso instead of pmi use something known as pmi zero which works like this if the count is \ngreater than zero then you use pmi if the count is equal to zero then you just put the entry zero \nin the set make sense there is also something known as  positive pmi which is slightly \nmore extreme it says that use the pmi only if the pmi is greater than zero otherwise use zero \ncan anyone tell me the rationale for doing this  you see the subtle difference between \nthe three things right one is of course doomed because you cannot handle zero counts the \nother one is sayin g that  if the count is zero then i will just substitute zero the last one is \nsaying that if the pmi is negative right then i will replace it by zero that means in the last \ncase all the cells  in your  or in your pmi matrix would be positive right  non negative \nrather \nso can you tell me the rough intuition for using this and there is only a rough intuition  \nbut can you tell me so the very rough intuition for this is that what does it even mean to \nsay that two words are negatively correlated i mean either they occur together right which \nmeans there is some relation between them  but a negative relation between words does \nnot make sense that is the intuition behind this  now of course i could argue that what \nabout antonyms and things like that  but that is also not the same right  because you \ncould have good and bad in the same sentence right \nbut that is the roughly the intuition that negative values do not mean much  so just \nreplace them by zerosok there is no again a formal reasoning behind this  but just the \nintuition so we have looked at the cooccurrence matrix where we started with counts \nthese counts were very sparse and there are also some other problems with counts  in the \nterms of some frequent words taking a lot of limelight and so on \nso we have fixed although then we have done some very minor and simple fixes  and i \njust very rush quickly rush through them because they are very simple but these were all \nfixable problems what a nonfixable severe problem with this what is the problem with \nthe one hot representations large \nrefer slide time tentwentyseven \n \nwhat about these representations  still large it is still of size v  it is still very high \ndimensional still very sparse not as parse as the one hot encoding but still sparse and it \ngrows wi th the size of the vocabulary  so now  remember that penn treebank at fifty k \nwords google onet corpus at tha thirteen million yeah so it keeps going with the size of the \ncorpus \nso now how do you know  how do you  fix this i wish i had that harry potter thing \nanyone remembers that spell to wipe out your memory how would you deal with it so \nyou now see how it connects so now again you have ended up in a situation where you \nhave a very high dimensional matrix  right and you are looking for ways to reduce the \ndimensions so we will go back and rely on things that you have learned and one of those \nwas svd right so you can use singular value decomposition \nwhy did i say svd and not pca because this is not necessarily a square matrix a this \ncould be a rectang ular matrix and for all practical purposes svd is just a generalization \nof pca"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.2 Distributed Representations of words.wav", "duration": 685.32, "text": "deep learningprof mitesh m khapradepartment of computer science engineeringindian institute of technology madras\nmodule \u2013 tenthreelecture \u2013 tensvd for learning word representations\nso in this module we will talk about using svd for learning word representations\nrefer slide time zeroeighteen\nsowhatdoessingularvaluedecompositiondoyeahtheseareallpossiblevariantssopeoplehavetriedvariousthingsandoneoftheppmioneistheisthemostreliablethingthatiswhatisgivenbutyoucanthinkofi meanyousaidonetherearetendifferentthingswhichwecandoforthecooccurrencematrixrightbutthisisthemostpopularand most stable thing to do\nyeahwhatisthesinglevaluedecompositiondocanyoureaditfromtheslidepleaseitgivestherankkapproximationofthematrixsoletmestartdefiningafewthingssofromnowonwheni refertothecooccurrencematrixi wouldmeanthexppmimatrixrightwhichwasthepositivepmiwhichwasreplacingallnegativepmisbyzeroandjustdo not have this nasty variable i will just call it as x\n\nsofromnowonwheneveri sayxi meanthepositivepmicooccurrencematrixoksothatiswhatthismatrixisokandweknowthatsvdgivesusthisreconstructionoftheoriginalmatrixandfineit givesusthebestrankk approximationoftheoriginalmatrixanditdiscoversthelatentsemanticsinthecorpuseveryoneremembersthislikethatiswhatwewerebywewereusingpcandsvdandautoencodersitwasabletodiscoversomelatentsemanticsandwewillconcretizethisintuitionwiththehelpofourcurrentexamplebutfornowi justwantyoutorecallthatit helpsindiscoveringthelatent semantics\nrefer slide time onefortynine\nnow noticethatthisproductandi thinki havedonethisinoneoftheassignmentsorsomethingcanbewrittenasasumofthefollowingproductssoicanwriteitassigmaoneuonevonetransposesigmatwoutwovtwotransposeandsooncanyoutellmewhatthissumisthisistheranktwoapproximationoftheoriginalmatrixandikeeptakingmoretermsigetmoreandmorerankapproximationsoftheoriginalmatrixoknowandweallknowthatokweallhopefullyknowthatwhatisthedimensionofthisitisascalarvectormatrixscalar vector matrix\nstudentmatrix\n\noknowofcourseyouwillsaymatrixbutwhatisthedimensionofthematrixwhyisitamatrixitisanouterproductoftwovectorsrightthiswhatisthesizeofthisncrossoneintoncrossonesothatsorryonecrossnthatgivesyouncrossnmatrixeveryonegetsthisotherwisehowisitarankoneapproximationyouhavetogettheoriginaldimensionsrighteveryone is clear with this is an outer product\nrefer slide time twofortynine\nanditbelongstormcrossnokandifwetruncatethesumatthefirsttermwegettherankone approximationandby svdtheoremweknowthatthisis thebestrankoneapproximation\nnow whatdoesthisactuallymeanthatthisisanapproximationwhatdowemeanbythatsowewillseethatonthenextslideandsimilarlyinthesamewayifwetruncateitin the second term you get the same best rank two approximation\n\nrefer slide time threefourteen\nnow whatdo we meanby approximationhereactuallyand i meanto sayapproximationalwaysinthiscourseatleasttrytothinkintermsofcompressionhowmanyelementsarethereintheoriginalmatrixmcrossnthatishowmanyelementsyouneedto describethematrixcompletely ifyoudoa rankoneapproximationhowmanyelementsareyouusingmplusnplusonerightsotheoriginalmatrixhasmcrossn entriesentriesandwhenyoudoa rankone approximationyouhavemplusn plusoneentry sothatthatistheapproximationrightsoyouaretryingtoreallycompresstheoriginal data using only these many variables you get that ok\nandifwedoaranktwotwicethisrightsoasmanyranki meanasdeeperasyougointhesumyouwillhavethatmanyelementstodotheapproximationokbutwhatisimportantisthatthesvdtheoremtellsusthatthisisnotjustanyrandomapproximationbutthisisthebestapproximationthatyoucouldhavedonethatmeansifyouwantedtouseonlythesemanyelementsthesearethebestelementstouserighteveryonegetsthat\n\nrefer slide time foursixteen\nsoasananalogyconsiderthisrightsupposeyouaregiveneightbitstorepresentcolorsokandthisishowyourepresentverylightgreenlightgreendarkgreenandverydarkgreen this is what your representation is\ninthisoriginaleightbitrepresentationthereissomesimilaritybetweenthecolorsbutitisstillabitlatentbutnowifi weretoaskyoutouseonlyfourbitstorepresentthesecolorswhatwouldyoudothelowestsignificantbitsifyouusethefirstfournothenuseonlygetverylightthatisnottheessenceofthatcolorrightyouneedthecolortobetheresoif youcompresswhatwouldhappenissothatiswhathappensinwhenyougofromtwo hundred and fiftysix bit colors to higher or lower right the distinctions between the colors go off\nsoallofthemwouldbecompressedtogreenwellthatisthemostimportantimportantinformationin termsof thecolorrightbecauseyouneedto beableto distinguishbetweengreenandredassupposetoverydarkandverylightthatisthemoreimportantinformationthatisthererightsowhenyoucompressitthemostimportantinformationin thatentityshouldberetainedandthatis exactlywhatsvddoeswhenit doesacompressionit retainsthemostimportantinformationinthecorrespondingentriesisthat clear is the intuition clear fine\n\nrefer slide time fivethirtytwo\nsoletusactuallydothissothisismyoriginalcooccurrencematrixxandi justrepeatwheni say xi meanxppmiandnowihavedonesvdandihavedonealowrankapproximationofiti donotknowwhatwasthevalueofk i selectedbutsomevalueof k it wasdefinitelygreaterthanone or two so now youseea lowrankapproximationofxwhatisthefirstobviousthingthatyounoticeitisdensenowitisthe longest sparse\nnow canyoutellmesomethingaboutthecoloredentrieswhatwashappeningintheoriginalmatrixxthewordsystemandmachinewasnevercooccurringbecauseofwhichtheirvaluewaszero samefor humananduser butrememberthereis someimportantinformationinthismatrixwhichalsotellsyouwhatarethewordswithuserappearswithandwhatarethewordswithhumanappearswithandthatactuallygivesyou intuition that these two words are actually related right same for system and machine\nsystemandmachinebothwouldappearinthecontextofwordslikeinterfaceinstallrunandsoonsoyouknowtheyaresimilarit justhappensthatthesetwowordsneverappearedtogether sothissimilaritybetweenthemwaslatentorhiddenintheoriginalcooccurrencematrixnowonceihavedonethesvdwhathashappenedbecauseihaveforcedit to compressthedatait hasretainedthemostimportantininformationandunderthatinformationthesetwowordshaveactuallycomeclosertoeachother right\n\nsoyouseethatnowyouhavea nonzeroentryforthesimilaritybetweenthosetwowordpairsdoyougettheintuitionandcanyouimaginethatthiswouldhappenwithsvd\nandwhatiswronginimaginingyoucanbutiguessrightthatiswhatishappeningwiththissoyouthinkaboutpcayouthinkaboutsvdyouthinkaboutautoencodersalltheintuitionsthatwehadbuildtherethesameisbeingappliedhererightallifyougetthisokfineyeahaftersvdyoucouldhaverightthatisnotnecessarythatit shouldbepositive in the original matrix you do not have negative entries\nrefer slide time seventhirtyone\nnow hereisaquestionrightrecallthatearliereachrowoftheoriginalmatrixxservedas therepresentationof a wordokthiswasmyoriginalx ppminottherankapproximationnowinthatcasewhatwouldxxtransposegivemewhatwouldtheijthentryofxxtransposebesoletuslookatthistoyexampleyouhavethisxmatrixyou have xi and xj now i take x transpose ok\nnow thisis xithisis xjjuststandingnowwhatwouldbetheij th entryofxxtransposeitwilljustbethedotproductbetweenthesetworightisthatfinesothisisjustthedotproductbetweenthemandweknowthatdotproductismoreorlessthesame\n\nascosinesimilaritymoduleoverthenormalizationrightyoujustneedtonormalizeitbythe norms of x and xi and xj in this case right\nsoiwilljustassumethatthisisasubstituteforthecosinesimilarity oksoeveryentryateveryij\u2019thcellinxxtransposeisthecosinesimilaritybetweentherepresentationsofthei\u2019thwordandtheg\u2019thwordisthatcleartoeveryoneokfineandintheoriginalcase which was the xppmi the cosine similarity between human and user was zerotwentyone\nrefer slide time eightfiftyone\nnow oncewedoinsvdwhatisa goodchoicefortherepresentationofthewordiaftersvdwhatisthedimensionofxhatitisagainncrossmbecauseitisasumofmcrossn matrixsothatthedimensionofx hatis m crossn althoughit hasbeenconstructedusingfewerinformationbutthedimensionismcrossnrightthatmeanswhatisthesizeoftherepresentationofeverywordstillhighdimensionalstillthesamen or v whatever everyone gets that is there any confusion with that ok\nnowyoucouldsaythatoki willjusttakethei\u2019throwofthereconstructedmatrixandusethatastherepresentationbecauseiknowthatnowthisrepresentationisbettersomeofthosezero entrieshavechangedtheyhavecapturedthelatentsemanticsbetweenthewordssothisisdefinitelybetternoneisdenyingthatthatthiscompressionhasgivenusbetter representation because we are only keeping the most important information\n\nnow if i doxhatxhattransposerememberxhatisthereconstructedmatrixthenagainbythesameargumenttheij\u2019thcellactuallygivesmethecosinesimilaritybetweenthei thwordandthej thwordandyoucanseethatnowthecosinesimilaritybetweenhumananduserhasactuallyincreasedsothisisjustformetoconvinceyouthatwehavelearnedmoremeaningfulrepresentationssonow whatdo wechooseas therepresentationi havestillwhilecomputingthiscosinesimilarityi havestillusedxiwhichishighdimensionalwhichhastheentirevocabularyasthenumberofcolumnsasa representation right\nsotherearetwothingscomingoutofhereoneisi reallylikethiscosinesimilarityi seethat it has improvedthatmeansthe representationswerecomputingsomethingmeaningfulbutontheflipsidei amstillnothappybecausetherepresentationsarestillhighdimensionalsocanyouconstructawishlistformebasedonthisi wouldwantthesamecosinesimilaritytobepresentasgivenbyxhatxhattransposerightbutiwould like to represent it by fewer dimensions that is exactly what my wish list is ok\nrefer slide time elevenone\nsoletusseehowdowedothatnowfornoreasoni amgoingtoconstructamatrixwwordequaltousigmawhatisusigmaitisthepartofthesvdrightthesvdtoldusitwasusigmavtransposesoi amjustconsideringthismatrixi amgoingtocallitw\n\nfornoparticularreasonoknowletmetakexhatxhattransposeicanwriteitasthisis that fine now what is the next step\nrefer slide time eleventhirtytwo\nwhatdoesthismeani wantananswerrightthisisthatahamomentshouldbethereorotherwisethereisnopointwhatishowmanyrowsarethereinwthesameasthenumberofwordsinourvocabularywhatisthedimensionofeachrowksonow wwordhaslowdimensionalrepresentationsforthewordsinthevocabulary butwhiledoingthiswhathavewenotsacrificedthecosinesimilarity thecosinesimilarityobtainedbythisisactuallythesameasthisdoyougetthathowmanyifyouseethisisvery very important that if you have not understood this everything is meaningless\nsoyouseehowfromsvdwegotalowrankoralowdimensionalrepresentationforthewordsrightwwordisjusttobeclearkandkisvery verylessthanv rightsonow wehaverepresentationsforwordswhicharemuchsmallertheyarenolongervdimensionalrememberin practicethisk wouldbe of theorderone hundredtwo hundredthree hundredandremember your vocabulary was of the order fifty k one thousand k and so on right\nsothehugereductionthatyouhavegotandyouhavestillbeenableto learnmeaningfulrepresentationswhichgiveyoubettersimilaritybetweenrelatedwordsright\n\nrefer slide time twelvefortynine\nsoconventionallywwordwhichisusigmaandbelongstomcrossksoi amsorryformessingthisupbuti haveusedm n andv areinterchangeably soyouwouldunderstanditfromcontextthatmisv andtheothermatrixwhichisvisknownasthew context matrix right what is the size of w context n cross k or k cross n right\nthatmeansit hastherepresentationsforall thecontextwordsandwwordhasarepresentationforallthetargetwordsrightsowehadthesewordsontherowsandthecontextwordsonthecolumnsowwordhastherepresentationsfortherowsandwcontext has the representation for the corpus ok\nsothiswhatwehaveseensofar andthisiswherewelearntodayiswhatanlpwassixyearsbackrightbeforetheadventof deeplearningif youwantedto usewordrepresentationsthisiswhatyouwoulddoyouwoulddoconconstructacooccurrencematrixtrythesetricksof pmippmipositivenegativezero andallthosethingsthoseheuristicsthendoa simplesvdretainthemostimportantone hundred thousand two hundreddimensionsandtreat that as word representations and use it for whatever you want to do\nnow whatneedstobeseeniswhathappenedwithdeeplearningandhowhavethiswayofcomputingwordrepresentationschangedoverthepastfewyearsrightsothatiswhat we are going to see in the next lecture right"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.3 SVD for learning word representations.wav", "duration": 840.9, "text": "deep learningprof mitesh m khapradepartment of computer science engineeringindian institute of technology madras\nmodthreepart twolecture \u2013 tensvd for learning word representations\nrefer slide time zerotwelve\nsowewillstartfromwhereweleftyesterdayahsowedidthiswholestoryonstartingfromcooccurrencematriceswelearnthowtogetbetterwordrepresentationsandthekeythingtherewasweusedsvdasadimensionalitycompreductiontoolandwecameupwiththisneatresultthatyoucouldusewwordastherepresentationofthemateofthewordsithas m rows and k columns where k is very less than the size of the vocabulary\nsoyouhaveachievedlotofcompressionandyouarestillabletolearnverymeaningfulrepresentationswhichyoucoulduseforseveraldownstreamtaskswhattousetheseforandhowtousetheseforyouwillseethatlatermaybefourlecturesfromnow i meani sayfourlecturesi meanfortytwohourlecturesrightsoitmightbemoreintermsofactuallecturessowe will get to that but for now we have a way of learning representations for words"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.3 (Part-2) SVD for learning word representations (Contd.).wav", "duration": 60.88, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 tenfour \nlecture \u2013 ten \ncontinuous bag of words model \nso from here on so none of this  that we  covered  had  anything  to do  with neura l \nnetworks say but it was important to understand  the context  and i will tell you why it \nwas important to go  over the traditional  way of learning  word representations and then \nwe will see how it  ties to the modern  way  or  the neural  network  way of learning \nrepresentations \nso we will star t with the fir st neural network base d model for learning word \nrepresentation which is known as the continuous bag of words model \nrefer slide time zerothirtyseven \nso just to set the context the methods that we have seen  so far are known as count based \nmodels because they rely on these co  occurrence counts for learning representations of \nwords and the methods that we are going to see now are called prediction based models \nand it will become clear shortly why the term predictio n and how they learn the word \nrepresentations \nso in a way in the original thing there was no learning involved of course you can say \nthat you were trying to learn these eigenvectors and eigenvalues and so on but it was not \nin the same way as it be as we have been learning parameters of a neural network and  so \non right it was not in the same spirit but now once i do these second type of models this \ndistinction would become very clear one why is there a learning involved and why they \nare prediction based models \nrefer slide time onetwentyfour \n \nso the story is we are going to look at continuous  bag of words model  then something \nknown as skip gram so this is the famous word twovec model which you guys have \nalready started looking at  then we look at g love word embeddings which is some kind \nof a hybrid between the count based models and the prediction based models  and then \nwe see how to evaluate word embeddings and then end with this depressing note that \ngood old svd is just fine  right  so all the progress that has happened in the past fivesix \nyears you could just use svd and still go by but if you do that you will probably not get \na job right you have to learn these things \nrefer slide time onefiftynine \n \nso now let us start with the continuous bag of words mode l so consider this task and \njust bear me for a few slides that when why this is connected to our problem and all that  \nso i am going to consider ka task we are here to predict the n\u2019 th word given the \nprevious n minus one words right  as this is something that you do regularly sometimes \neven in the class when you are whatsapping or smsing so this is what you do r ight you \nstart typing he sat on a and you get this prompt that the next word should be chair or \nsomething like that right  \nnow you can think of this as a classification problem  tell me why you can think of this \nas a classification problem  can you tell me what is  so remember that we have always \nthought of this that there is always a y there is always an x  and then we are trying to \nlearn this relation from x to y  so given this example can you tell me what is x here and \nwhat is y here  \nstudent refer time twofiftyone  \neveryone is clear about that  right  so this is x and this is y now  i made a statement \nthat i could think of this as a cl assification problem  right  so the minute  i say \nclassification what is the y that comes to your mind or dash hot by \nstudent refer time threeseven  \ny not i anything else would have been inappropriate  but  so one hot y is  what you \nwould expect there right and now what is the size of this one hot vector \nstudent size of refer time threenineteen \nsize of the vocabulary  so we are trying to predict one of the words in the vocabulary  \nso you see why this is a multi  class classification problem  you see that t here are many \nclasses and you want to select one of these class  now the moment i say classification i \ngive you an x and y i will start asking me who will give me the training data for this so \ncan you think of training data for this any corpus similar t o the one that you are creating \nright \nrefer slide time threefortysix \n \nin specific what you will do is suppose you have framed it as the following problem that \nyou are given for words and you want to predict the fifth word  so in general i have call \nit as that you are given n minus one words and you want to predict the n \u2019th word the n that \ni am considering here is four \nnow what is going to be the training data for this  if you take any corpus that you have \nbuilt anything right consider all  five word windows from there do not get too engrossed \nin the story so there are four the first four words you can treat as x and the fifth word \nwould be your y  so you can construct many such x comma y pairs from the raw corpus \nthat you are creating  any five word window and you can keep sliding this window right \nwhat i mean by that  this is your first training instance x comma y  this could be your \nsecond training instance so this could be these overlapping training instances you keep \nsliding this window and you will get many training instances ok you see that \nso this task the advantage is that given the size of the web and so on at least for popular \nlanguages the training data almost comes for free right  compare this to mnist or any \nother task where you have to actual ly acquire these labels that this is an apple  this is a \nbanana and so on here you get the training data for free just need to scrape it from the \nweb know no window size is something that you will set right whether you want to \nlearn four word windows or what do you mean we do not know the window size no \nso this again there is a lot of existing literature in the traditional nlp where various and \nlot of work has been done to figure out what is the right n so in most nlp task right if \nyou want to predict the next word a three word window is enough actually  if you know \nthe last three words and you can try this as a mental exercise right  if you know three \nwords you do not really need to know the words before that  so this is the mark of \nassumption with where this is a trigram dependency in the words right  \nso this n is not really  so difficult and in the default tool that you guys would try \nprobably they take the value of n is seven that is an overkill but that is again it comes from a \nlot of existing l iterature in nlp right this is not a this task is not deep learning broad \nright this task is a simple language modeling task which has existed for many years right \nfrom probably one thousand nine hundred and fiftys or sixtys or something \nso this is all n word windows in your corpus  as i said training data comes for free and \nfor ease of illustration we will now focus on the case when n is equal to two that means i \nam given one word and i want to predict the next word \nrefer slide time sixeighteen \n \nand we will see how to model this using a n eural network so these are the two questions \nwhich i need to tell you how to model this task and what is the connection between this \ntask and our original task of learning word representations  these are the  two things that i \nam going to answer \nrefer slide time sixtwentyeight \n \nso we will model this problem using a feed  forward neural network what is the input \none wordok so say the word is sat i am going to represent it using a one hot vector  ok \nand what is the output  i want to predict a distribution over all the words in the \nvocabulary and  i want to predict  i want to pick the word which has the maximum \nprobability that is how you did  so for example in the case when you had this \nclassification problem of banana apple orange mango you predicted a distribution over \nthese four classes and then picked the one which had the highest probability exact same idea \nhere it just that instead of four classes now you have v classes and your v is very large ok  \nit is trying to learn a distribution over there \nand you know that in this case or the example that you are considering on is the actual \nnext word so you type sat and the next word is on  and probably leading to sat on the \nchair or something like that so this is what you would want to maximize  ok i have \ngiven you the input i have given you the output give me a neural network to model this  \nthere are lot of hints in the diagram itself right you see some space between the input and \nthe output \nso what will you put in the middle layer we will put a middle layer the re right  is this \nan ok way of modeling this task i have an input i want to predict an output so i just use \na regular feed  forward neural network and let us analyze these parameters a bit more \ncarefully right  so i am something known as w context i have something known as w \nword i am already using some notations from the  svd lecture there at the end we \nended with  w word and w context right it is not clear why  i am using the same \nnotations but it will become clear in some time but let us look at their dimensions  \nso we have this one hot vector  i have a parameter w context which i am going to learn \nright and its size is k cross v so what does that mean this matrix is going to multiply \nby the vector and give me a  k dimensional output right is t hat clear so i have this is of \nsize v because always keep surprising me i do not know why you cannot do this r this is \na v dimensional vector you multiply it by a k cross  v vector so you do w into x so \nyou will get a k dimensional vector  so this is k dimensional you have a k  dimensional \nhidden representation  \nand from there now having captured this hidden representation  you are trying to predict \nwhich is the next possible class this is the same as any other thing right if you had done \nthe image classification or the mnist digit classification you had this seven hundred and eightyfour dimensional \ninput vector you pass it through a hidden layer and then you predicted one of the ten \nclasses there is nothing magic here it is the same thing that you have done seen before \nrefer slide time nineseventeen \n \nhow many if you get this and what are the parameters  w context and w word ok and \nwe are going to focus on these parameters and understand what they actually mean \nrefer slide time ninetwentysix \n \nso what is the product  w context into x given that x is a one hot vector  so i will tell \nyou this suppose the i\u2019th entry is hot here how many if you say it is the i\u2019th column of w \ncontext so it is simply the  i\u2019th column of w context why because you have this  w \ncontext matrix you take a one hot vector which has the second entry as hot if you do this \nmultiplication you basically get the second column of w and you can just see it everyone \ngets this now how many if you get this now \nso if you have a one hot vector if its i\u2019th entry is on u multiply it by a matrix  you will \nget the i\u2019th column of the matrix  ok so if the i\u2019th word is present in the input then the  \ni\u2019th element of the one hot vector is on and the i\u2019th column of w context can be would be \nselected so then can what can you tell me about the i\u2019th column of w context you see \nthere is this one  to one correspondence between words in your vocabulary and columns \nof the w context matrix how many columns has w context have v columns how many \nwords are there in your vocabulary  v any one word is on only one column will get \nselected and that is a unique column it is not going to change right  so there is a one to \none mapping between the columns of  w context and the words in your vocabulary  that \nmeans the columns of w context are the are the vector representations \ndo you know these vector representations no these are parameters of your network so \nthey will they will be learned how we will see  ok so you see the intuition for  w \ncontext setting it up this way  so now  i have set up the problem  in a way that by \nparameter matrix directly gives me the word representations  ok b ut any kind of \nlearning has to be driven by some objective  so what is that objective it is already clear \nto a lot of you but we will just do that in a bit more detail so this is exactly what i have \njust said \nrefer slide time eleveneighteen \n \nnow how do you obtain p on given sat no no so for a given training instance so when \nyou so you could so i will so for a given training instance you said that your corpus \nhas been divided into those training windows  right  so it is possible that engineer \nsometimes the word does not and is not the next word  but for this training instance what \nis it so that is what you have to predict right is that fine \nrefer slide time elevenfortyeight \n \nand at test time so you are saying that what you are saying is more practical that when  i \nhave typed sat in the w hatsapp message i do not want on as the always the answer  so \nwe get these  five options right three to five options so what could be that you have this \nprobability distribution pick the top  five from there and show it as options so is that fine  \nso we are done with this now how do you compute p on given sat  what is the actual \noperation happening there what is the appropriate output function this is a multi  class \nclassification problem softmax \nthis is what softmax  looks like so the property if suppose on is the  i th word in your \nvocabulary then i am saying that the probability of on given sat is going to be this \nquantity how many of you agree with that  i mean those who agree is fine  i am asking \nwhy the others do not agree what is not clear about this  i do not know how to explain \nthis i mean it is just  so plain obvious what is the softmax function first of all you will \ndo this aggregation so you will do this w word into h that is fine right  so for you will \ncompute this vector consisting of w word into h fine what is the dimension of that what \nis the dimension of that  mod this is k dimensional this is  v cross k or k  cross v \ndepending on how you multiply it so what is the output going to be v so you have v \nentries \nthese are dash entries the options are normalized unnormalized unnormalized now what \ndoes softmax do \nstudent normalization \nnormalization that is exactly what this formula is doing  right  you want for the i th \nword you see what was the end this product right this gave you a  v dimensional vector \nyou look at the  i\u2019th entry there right that is what you are doing here raise it to an \nexponent and divided by the summation of all these entries  come on guys this is highly \ndisappointing i cannot teach softmax i had in the tenth lecture eleventh lecture of the \ncourse right what is wrong how many if you get this now just have to ask of it tangle \nyou \nso you see this right this is what is happening here  so you get this  v dimensional \nvector and you just con converting into a probability distribution using the softmax \nfunction so now this value how did he compute this value actually you computed this \nproduct which is  w word into h and then you took the  i\u2019th entry of that and then this \nsome transformation on that the softmax transformation you see that \nrefer slide time fourteentwentyone \n \nso now i can say that p on given sat is actually proportional to the dot p roduct between \nthe j\u2019th column of w context and i\u2019th column of w word why am i saying that \nso remember that this was the i\u2019th word in your vocabulary and on was the j\u2019th word in \nyour vocabulary so can you explain the meaning of this sentence to me  f irst let us \nlook at the first part what is h  it is a j\u2019th column of w context oh sorry this should be i \nthis should be j so this you already saw that h is the jet column of  w context because i \nam multiplying a one hot  vector with the matrix  is that fine and what is the i\u2019th column \nof w word so why what is this product actually equal to if  i say w word into h w \nword into h that is a vector and then i am taking the i\u2019th entry of that  so i am saying \nthat is the same as taking the  i\u2019th column of w word and multiplying it by h  how many \nif you get this is basically in your algebra right  \nnow these four different ways of multiplying matrices  i am just using one of those  right  \nso if i multiply a matrix with a vector and then take the  i\u2019th entry of that  that is the \nsame as multiplying the  i\u2019th column of the matrix with the vector  ok just go back and \nverify this just take my word for it for now  so now what is happening is that it is \nproportional to the product between the j \u2019th column of w context and the i\u2019th column of \nw word is that clear now everyone gets this \nrefer slide time fifteenfiftysix \n \nso p word equal to i given sat does depends on the i\u2019th column of w word ok \nso now what can you say  so earlier we saw that the  i\u2019th column of  w context \ncorresponds to a particular word now what can you say about the  i\u2019th column of  w \nword it also corresponds to a particular word  so now why these two correspondences i \nalready had a correspondence between w context and every word in my vocabulary now \ni am sayin g that there is also correspondence between  w word and every word in my \nvocabulary how many of you first of all are comfortable with the sentence  that every \ncolumn of w word has a correspondence with some word in the vocabulary \nthe second sentences every column of w context has a correspondence with some word \nin the vocabulary do you all of you agree with both these statements  okay that is what \nwe have try to prove  so far so now for every word  that means  i have two columns \nwaiting for it how do i deal with this situation  have you ever dealt with it before  the \nsame thing happened in svd also right  svd also gave you this u sigma which was w \nword and then  v which was w context so you can always learn  two different \nrepresentations for the words one  is when the word appears as a context word and the \nother is when the word appears as the target would you get that you see why we have  two \ndifferent representations fine \nand as i said hope you see the analogy with  svd right you already saw there that ther e \nwere these two representation now given all this set up and please do not disappoint me \ncan you learn these parameters with some tweaks to the code that you have written for \nmnist can you use the same code to learn these parameters how many if you say yes \nso what is the tweaks  what are the tweaks  t he input changes instead of the image \ninput you have this v dimensional input what else changes  \nstudent output \nthe output changes instead of a ten dimensional output you have a v dimensional output \nall of you are absolutely clear about this and what is the training algorithm  \nstudent refer time seventeenfiftynine  \nback propagation what is the loss function cross entropy good fine \nrefer slide time eighteenfive \n \nso for some  i will do some more stuff on this be cause there is some in interesting \ninterpretations of the gradient descent update rule here so i will refer to the word sat by \nthe index c  and the word on by the index w  ok and you already saw that the \nappropriate loss  output function is softmax the appropriate loss function is cross \nentropy so let me just look at this  right  so w was the index of the output word  so \nmy cross entropy formula would just boil down to this everyone is fine with this  ok i \nwill just try to maximize the w th entry in my y hat how many of you are fine with this  \nokay and that is nothing but the probability of the word given the context \nnow remember that h is equal to w context into x c i am going to call that as u c so u \nc is the dash of the word sat title of t he lecture  it is a vectorial representation of the \nword sat everyone is fine with that  because that is exactly what this product is going to \ndo and now my y hat w is equal to this because  i already said it is the product of the c\u2019th \ncolumn of w context and the w\u2019th column of w word \nrefer slide time nineteenthirtytwo \n \nso now i have a formula for y hat w  what is the training algorithm that you will use  \ngradient descent with back propagation  now let us consider one such input  output pair \nand see the update rule for v w \nrefer slide time nineteenfortyfour \n \nso my loss function is this  this is actually this quantity  ok now i can just rewrite it as \nthis i have just expanded the log so the log of a by b is log a minus b ok now i want this \nquantity because this is the p arameter of the network right v w is one of the columns of  \nw context or is it w word w word v w is one of the columns of  w word and i want to \nlearn i want to learn it what are the what are these column entries so  that means  i am \ninterested in this particular gradient \nso i will start taking this so what is it going to be so only u c will remain here of all \nthese summation terms only one of them would remain and then you can derive de \nderivative right  so this is what it is going to look like wha t is this quantity  the \nsoftmax function so this is what i get \nrefer slide time twentyfortytwo \n \nso now my gradient update rule is going to look like  everyone is fine with this  i have \nderived this formula and  i have just substituted that here  and this negative and this \nnegative ok so now let us look at this update rule \nrefer slide time twentyfiftyseven \n \nso this update rule has a very nice interpretation  which allows us to understand  what \ndoes the continuous bag of words model actually learn  now suppose y ha t w tends to one \nwhat would that mean your prediction is very correct right you are almost predicting it \nhas probability as oneok what would happen to the update in that case  there will be no \nupdated if it is one there will be no update if it is close to one there is going to b e very \nminimalistic update that means you have already learned the v w well enough ok \non the other hand if  i am very bad if y hat w is close to zero what would happen just tell \nme the case when y hat w is actually zero what is the update rule have you seen something \nsimilar ever before  h ave you seen something similar before where did you see this \nupdate rule perceptron what happened when you did this  w and x came closer to each \nother the angle between them actually decreased so the same thing is happening here  \nso what you are trying to do is  you are trying to make your word representation closer \nto the context representation is that clear how many if you get this  i t straight away \nfollows from the update rule right because you are  adding a fraction of your context \nvector to your word vector and we know that when we add  two vectors they come close to \neach other the cosine between them decreases  that is what we proved in the word to it \nlecture in the perceptron lecture right  \nrefer slide time twentytwothirty \n \nso you can go back and refer to this slide on lecture  two now so the training objective is \nessentially ensuring that the cosine similarity between  v w and context word is \nmaximized between the word and the context word is maximized \nrefer slide time twentytwofortyseven \n \nnow what is the result of this now i want you to think go back with a starting example \nwhere we said that we want to learn representations such that cat and dog are close to \neach other but cat and truck are not close to each other \ni want you to think whatever you see on this slide  the conclusions that you drew from \nthis slide how do they help you to relate back to that initial goal  ok so now let us let \nme give you the intuition right  so what happens to the representations of two words w \nand w prime which tend to appear in the same context c so say dog eats cat eats right  \nso dog and cat are  two words which appear with the same context eats  so what will \nhappen to the representation of dog  it will come close to eats what will happen to the \nrepresentation of cat  c ome close to eats  n ot only that dog will also go close to pet \nanimal sleeps right and  so on and cat will also go close to these  so transitively what \nwill happen dog is going close to a certain po int or certain sets of points  cat is also \ncoming close to the same set of points  so transitively dog and cat will come close  to \neach other you get this intuition \nanyone sees a problem with this  no so known objective and i said that dog comes \nclose to eats is that what he wanted  i mean why should dog be close to eats  that means \nif i find the nearest neighbors of dog i will get words like eats sleeps barks and so on is \nthat what i wanted so that is exactly what is happening and based on that i convinced \nyou that dog and cat will come close to each other  but there is a subtle gap here  i want \nyou to close that gap how many matrices do we have  two that is enough hint we are \ngoing to either take columns of this matrix as the representations or the columns of this \nmatrix as the representation not mixed \nso now can you tell me so dog here will come close to eats  sleeps barks here word \nwill come close to context word right cat here will come close to eat sleeps and so on \nright  so transitively dog and cat will come close to here and this is the representation \nthat you care about not representations across these  two matrices ah so what i said is \nthat the training rule  ensures that the words representation comes close to the context \nwords representation ok that is what we saw with the training update rule \nso that means dog will come close to any kind of context word that it appears with  so \ndog i would expect it to appear with context words like eats pet animals  dog barks \ndrinks and so on right  so dog is coming closer to these words and i expect cat also to \ncome up here with these words and of course i do not expect truck to appear with these \nwords right  so then cat will also come close to these set of words  dog will also come \nclose to these set of words  so transitively dog and cat will come close to each other \nright all of them are coming close to each other  ok which is fine which was my original \ngoal \nbut my original goal was not that dog and eats should come close to each other because \neats and do g are neither synonyms when they do not have any semantics  i mean they \nhave a semantic relation  but that is not what i wanted i wanted similar words to come \nclose to each other  but now i have the side effect that dog is co ming close to eats  but \nthat is bad was how can i live with that \nso the i mean the key thing that you should notice is that you have one matrix of words \nthe other matrix is of context words so the representation of dog in the word matrix is \ncoming close to the representation of eats  sleeps etc in the context representation on the \ncontext matrix  t he representation of cat is also coming close to these words in the \ncontext representation and transitively because of this dog and cat in the word matrix a re \ncoming close to each other and this is the matrix that we care about \nin this matrix dog and eats  dog and sleeps are not close to each other right is that fine \neveryone gets this now  ok so this is only an intuition  and this becomes very tricky \nwhen i will blow this up what do i mean by blow this up right now what am i trying to \ndo what is the size of n two right i am taking one word and outputting the other word  \nhence you get all these neat interpretations that you are moving close to that vector  and \nso on the moment i add more words to n these interpretations become  more and more \nhard right but this again i mean this is good to understand that this is what happens at \nleast in the best case  so this is only an intuition which is reasonable in m y opinion i \nhave not come across a formal proof  which says that this is what actually happens and \nthat is one criticism for wordtwovec it works very well but there is no formal proof which \ntells you why exactly it works \nas opposed to  svd right there we know there is a principle behind it  here that is not \nvery clear right but it works very well based on this intuition  ok so everyone gets the \nwhole set up how we started with a classification problem of predicting the n th word \ngiven the n minus one words which had nothing to do with word representations that is a \nsimple language modeling problem which has existed forever  we smartly modeled it or \nsomeone smartly modeled it using a neural network such that the  parameters of the \nneural network end up giv ing you the word representations  and this network is end  to \nend trainable using an objective function the training data comes for free  for popular \nlanguages you have like  tons of training data the entire wikipedia entire web whatever \nyou can scrape  that is why with more and more training data you can learn even better \nand better representations so for popular languages the representations are really good \nand then we saw an intuitive explanation for why this works because of this movement \nof things clo ser to each other and the key thing to notice there are  two different \nrepresentation matrices one for the words one for the context  and this is not surprising \nthe same thing happened for  svd also u sigma was  w word and v was w context  \nright  so it is all in the same spirit right  \nrefer slide time twentyeightthirtyeight \n \nnow in practice instead of window size of one it is common to use a window size of d  \neither d could be four or seven i have i have even say and seen eleven actually but not beyond that \nok now let us see what happens if you have two and here itself it should become clear that \nnow those interpretations are not very neat  so what i will use suppose i want to take a \ncontext of two words then i have he sat and now i want to predict the next word \nso what is my input now he and sat right  so i will take the one hot representations \nof he and sat i will just concatenate them sorry  i just concatenate them is that fine and \nmy input now belongs to r raise to two v in general it will belong to r raise to d v ok and \nnow what is the next step  do you see something funny here  i have just created  two \ncopies of this  ok i am telling you an inefficient way of doing this  later on it will be a \nvery simple thing to do a very efficient way of doing this right  but first just to get the \nmath around i will just do inefficient way of doing it \nwhy have i staged it twice two words right  so now my h is actually going to be the sum \nof all the columns of w which correspond to my input words is that fine i have to earlier \ni had just one word as the input so my h was just equal to that column of w now my h \nis going to be equal to the sum of all the columns of w corresponding to the words that  i \nhave and i will tell you why so i have taken w contexts comma w context which is just \nthe w context matrix staged twice back to back  so this was my w matrix this is my two \nhot vector because i have two inputs now right  so my vocabulary size is three so the first \none hot vector followed by the next one hot vector and now  i am going to repeat w w \nnow what is the product of this  is the sum of the  two columns that you see highlighted \nright and exactly that is what i have written here \nso if i do it this way then  i can just do this very expensive matrix multiplication and to \ndo a something very trivial which is just taking the sum of  two columns right  but at least \nyou get the operation and  i will just on this next slide or something  i will tell you an \nobvious simple way of doing that  so i just get the sum of the  two columns so that is the \ninput to my network  if i had k words as input if  i had my window size four what would it \nbe i would have these  four copies of w context i will have these four one hot vectors and it \nwill just give me the sum of those  four columns  ok that is going to be the input  ok and the \nrest of the story remains the same  right  once you have this h the rest of it from there \nremains the same and this is the formula for h in general \nin the special case it was just the i\u2019th column in the general case is the sum of all the \ncolumns that are there in your input \nrefer slide time thirtyonethirtyfive \n \nnow in practice of course  this is a very mate expensive matrix multiplication  it is \nstupid to do it that way  what you will do is you will just slice of those columns from  w \ncontext right and then just add them up so you do not really need to do that stupid mate \nmatrix multiplication because you know that the matrix multiplication is essentially just \nselecting these columns and adding them so just select those columns and add them up \nso you do not do that bad matrix multiplication operation is that fine \nrefer slide time thirtytwoone \n \nnow what happens during back propagation in this case in the generic case the \nordering does not matter is what you have seen yes it does not matter yeah there is some \nassumption of the model  so it is that is why the name of bag  of words you are not \nrelying on the sequence so this comes from nlp that if you rely the sequence you call \nit sequence if you just going to take the words in the sequence  you just call it a bag of \nwords because once you put them in a bag there is no ordering there right that is why \nthe word name bag of words \nso and again p on given sat is given by this softmax formula ok now tell me during \nback propagation and if you give me a righ t answer to this  i really feel happy that you \nhave understood everything right f rom the beginning of the course  so no pressure so \nwhich are the parameters which are going to get updated during back propagation  which \nare the two large matrices w word and w context so obviously the answer is not w word \nand w context otherwise i would not have asked you the answer is some dash of these \ntwo some subset of these two which subset let us start with w context which is the input \ndo we are we going to update the entire w context did it participate the entire w context \nparticipate in the computation  only those columns corresponding to the words  so only \nthose parameters will get updated  \nso how many columns will get updated d columns right  w word till all the columns \nof w word participate in the computation how many of you say yes  how many if you \nsay no the others do not care can you just focus on this circle did all the columns of  w \nword participate in the computation  you see the summation at the bott om it is over all \nthe columns of  w word all of them participated  so the parameters which will get \nupdated are w word and all the columns of the input words  and same back propagation \nwill work again is that fine \nso remember that and this is i cannot emphasize it enough whatever i have explained is \nonly for an intuitive explanation  you will never ever do this matrix multiplication right \nand that is why what you are going to do is you are just going to select those columns \nadd them up and feed them  and the network will take care or rather you will take care \nthat you update those parameters only and you do not update the entire w context matrix \nbecause anyways there is no gradients flowing to the other components  so remember \nthat in the practical imple mentation of w of word  two vec do not search for this matrix \nmultiplication at the input  or if you are writing the code on your own which is highly \nunlikely do not do it that way \nso if you whatever code that you look at did not have this complex matrix multiplication \ntypically they will just pick up the columns and add them and feed them right and  i \nthink the tensor flow way of doing is you have this word embeddings matrix and you can \nslice columns from there and  so so everyone understands this  so far now what are \nthese problems with this why is this not as simple in some sense as the mnist data set \nagain focus on the circle  this softmax computation is a very expensive operation right \nyou have a v cross k sized matrix somewhere there  and unlike at the input here you will \nhave to do this matrix multiplication right  \nso we have a  v cross k matrix multiplied by a k cross one vector and there is no \nsimplification of this you have to do this multiplication what are the sizes of  v that we \nsaw in practice fifty k one hundred k and if you had googled thirteen million or something right  so \nthis is not feasible we cannot do this expensive matrix multiplication \nrefer slide time thirtyfivefiftysix \n \nso although all of this works very fine  we need to think of ways  to simplify this  \nsoftmax computation where the denominator requires the summation over all  the words \nin the vocabulary so you have to do that many matrix multiplications"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.4 Continuous bag of words model.wav", "duration": 2155.05, "text": "deep learningprof mitesh m khapradepartment of computer science engineeringindian institute of technology madras\nmodule \u2013 tenfivelecture \u2013 tenskipgram model\nsowiththatwewillmoveontothenextmodeli amstillnottellingyouhowtosolvethis problem we will come to that later\nrefer slide time zerofifteen\ni amjustgoingtothenextmodelwhichistheskipgrammodelokandthisisthefamous wordtwovec which you are trying to implement\n\nrefer slide time zerotwentythree\nthemodelthatwejustsawwasknownasa continuousbagofwordsit predictstheoutput given the context skip gram model does the reverse of that\nrefer slide time zerothirtyfour\nyouaregivenawordyouwanttopredictallthecontextwordssonow iamgiventhewordoni amtryingtopredictthewordswhichappearontheleftandrightsideofitisthatfinesohowmanypredictionproblemsami solvinghowmanypredictionsamigivingyoufour inthiscaserightsoyouseethatthisisacasewhereyouryactually\n\nbelongstorfourrightofcourseitisnotrfouritisfourintovandbecauseyouarepredictingtheentiredistributionbutwhati meantisthatyouwantthesefourdifferentoutputsyoujust do not want one single output\napartfromthatdoeseverythingelseremainsameyouhaveaninputwordyoucomputea hiddenrepresentationfromthathiddenrepresentationyoutrytopredicttheoutputsyou geta probabilitydistributionwhatis yourlossfunctionit is a dashofcrossentropiessumofcrossentropieshowmanycrossentropiesdoyouhavefourinthiscaseandalsonoticethati havei hopei haveyeahi havechangedwwordandwcontextthey are flipped now is that fine the role of context and the word has changed\nrefer slide time onefortysix\ninthesimplecasewhenyouaretryingtotakeonewordastheinputandonlypredictonewordaroundit it justbecomesthesameas thefirstcasethatwesawin thecontinuousbagofwordsthereisnodifferencetherebecausetherealsoyoutakeoneword and predicts the other word\nsotheentiremath\u2019s remainsthesamehowmanyofyougetthatandevenwhenwehavemultiplecontextwordsourlossfunctionisjustgoingtobethesumofthecrossentropiesforallthosedpredictionsthati needtomakeordminusonepredictionsthatineedtomakeandthenonceiseealossfunctionwhichisasumofsomethingsiamnot\n\nworriedbecausei knowhowtodealwitheachofthesecomponentsandgradientsareadditivesoifyouhavethegradientofsomeitisjustthesumofthegradientssoasilongasiknowhowtodealwithoneoftheseicandealwiththesumsothatiswhyidonot really worry all of you are at that level\nrefer slide time twoforty\nwhereyoudonotworrywiththesumasalossfunctionwhataretheproblemswiththisalready written there same as the bag of words right\nnow we are doingthesefour expensivecomputationsat theendsothesoftmaxcomputationisexpensivetherearethreedifferentsolutionsandtherearethreedifferentwaysthatwecandealwithitoneissomethingknownasusenegativesamplingtheotheristousecontrastofestimationandthethirdistouseahierarchicalsoftmaxsowearegoingtoseealloftheseandi willshamelesslycontinueforafewmoreminutessofirstwewill see use negative sampling because that is very easy\n\nrefer slide time threethirteen\nsoletdbethesatsetofallcorrectwcommacpairsinthecorpuswhatdowemeanbythatallwordswhichactuallyappearedinthewordcommacontextpair soyoucanlookatthevector whichi haveconstructedsosatonsatorsatchairyoucanimaginethatalloftheseappearedinthecontextofeachother sothisismycorrectcorpusasfrom what i got from my data\nnow letdprimewithathesetofallincorrectwcommarpairsinthecorpusandrherestandsforrandomsomehowami goingtoconstructthiscorpussoi takeawordiknowallthewordswhichappearedwithitandiknowalltheseotherwordswhichhavenotappearedwithitsoi willrandomlysampleawordfromthereandputitasr isthatfinesoi cancomputedprimeagaindprimecomesforfreedwasalwaysforfreenow d prime is also obviously for free\nsoi havewcommacandwcommar andi havethesecorporadanddprimeandasbeforeletv wbetherepresentationofthewordandu c betherepresentationofthecontextwordoksov w willtryto thesetwo andyouseewilltryto thisandu rsomething else that we will use for this hopefully is that fine okay\n\nrefer slide time fourthirtyone\nnow foragivenwcommacwhichbelongstodwhichisthetruecorpuswhatareweinterestedinmaximizingsoletusthinkofzisarandomvariableweatherwhichtellsuswhetherthisisatruepairornotsogivenwcommaciwanttomaximizethatpofzisequaltooneoknow thisiswhatiwanttomaximizenowitdependsonmehowdoimodelthisprobability socanyouguesshowami goingtomodelthistheansweristhereinthefigurecanyoutellmewhatisthemodelthatihavechosencanyoutellmewhatistheformulaforzequaltoonegivenwcommacthatihavechosenthisstandsfordot product this stands for the sigmoid function\n\nrefer slide time fivesixteen\ni knowthisis someucrepresentationthisissomevwrepresentationnotethattheserepresentationsarenotlearnedyetineedtolearnthemusingthetrainingobjectivethatiset\nbutatthebeginningtheyareinitializedtosomerandomvaluesandthewayiamgoingtomodelprobabilityofzequaltoonegivenwcisthatiamgoingtosaythatitisjustthesigmoidfunctionofthedotproductbetweenthemhowmanyofyougetthisareyoucomfortablewiththisthisis themodelingchoicewhichi havemadeorrathertheauthorsofskipgramrightnowhowami goingtonowwhatdoiwanttodoforallwcommac belongingtodi wanttomaximizethisprobabilityisthatfineforallthewcommac pairswhichbelongto mytruecorpuswhichis thed corpusi wantthisprobabilitytobehighhowmanysuchpairsdoi havemanymanyrightsoletuscallthem as i have n such w comma c pairs\nsocanyoutellmewhatmylossfunctionisgoingtolooklikemaximizethisforthefirstpair and for the second pair and for the third pair all the way up to the end pairs\n\nrefer slide time sixtwentyfive\nsowhatis it goingto looklikeforeveryw commac whichbelongstomycorrectcorpusi wanttomaximizethatprobabilityofzequaltoonegiventhatwcommacpairrightandsinceit isanandi willhavethisproducthowmanyofyouarecomfortablewith this\nsothisassuchandthisissomethingthatyoudoregularlyyoushouldhavedonethisinmachinelearningorpatternrecognitionorsomewhererightthatyouwanttobasicallymaximizetheloglikelihoodofthedatawhichissayingthatyouwanttomaximizetheprobabilityofeverytraininginstancewhichissayingthatyouwanttomaximizetheandof all these probabilities right be you take the and of all of them is that fine\n\nrefer slide time seventwo\nnow fortheothercasewrbelongingtodprimewhatisit thati wanttomaximizethisprobabilityrightbecausei knowthisisanincorrectpair soi wantmyrandomvariable to output zero ok\nnow whatisthisgoingtobetheprobabilityoneminustheprobability thatitwascorrectandthatactuallyifyoujustsimplifyabititturnsouttobethisnowforalltheelementswhichbelongtodprimewhatistheobjectivefunctionthatihaveiwanttomaximizethisforthefirstwcommarrandompairforthesecondwcommaarerandompairandsoonforalltherandompairsinmycorpussoitisjustgoingtobeaproductofalltheseprobabilities is that fine so now what is my total objective function\nforeverypairindmaximizethatforeverypairindandforeverypairindprimemaximize the zero probability so what is the total going to be is this fine\n\nrefer slide time eightone\nhowmanyofyouagreewiththissoforeverythingbelongingtodihadthisandruleforeverythingbelongingdprimei hadanotherandruleandiaminterestedinboththeactsrightmaximizefordandmaximizefordprimeofcoursedifferentquantitiesfordanddprimeokfinesoyougetthisonceyoubasicallytakethelogandsoonsothisis a simple set of math operations that i do you will end up with this neat formula ok\nthatforallthewcommac pairsbelongingtodyouwanttomaximizethisquantitywhichmeansyouwanttomaximizewhatyouwanttomaximizethewhenwillthisquantitybemaximizedwhenthedotproductbetweenthetwoishighthatmeansagainwhatareyoudoingwearetryingtobringthecontextvectorsclosetothewordvectorsagaintransitivelywhatwillhappenthewordswhichappearinthesamecontextwillgoclosetoeachother whatistheadditionalthingthatyouareensuringherethewordswhichdo notappearin thesamecontextyouaretryingto pushthemapartwhybecause of second loss function\nyouseethedifferencebetweenthetwonow inthefirstcaseyouareonlyoptimeanyouareobsessedwithbringingthingsclosetogether hereyouarealsofocusingonthecasethatwhereyoudonotwantcertainthingstobeclosetogetherbecausetheyneverappear\n\ninthesamecontextisthatfinesoyouseethatthisisamorepowerfullossfunctioninthe earlier one so that is what the skip gram model does\nrefer slide time ninetwentyseven\nandintheoriginalpapermikolovetalsamplekrandompairsforeverypositivepairright\nsothatmeansifyoursizeofdwasnwhatwasthesizeofdprimebkintonsothattheyhadthatmanypositiveexamplesandktimesthatthenumberofnegativeexamplesandthiswasahyperparameterwhichwastunedandtheyusedavalueofksuchthatitgavethemthebestresultsalsorememberthatwehavethisproblemofconstructingwcommar nowi saidthatconsiderallthewordswhichdonotappearwithyourwordandsamplefromthereandputsomethingtheresotheyusedaslightlythatmeanshowdoi samplethatoneisi assignallthewordsauniformdistributionthateverywordisequallylikely whatisa betterwayofdoingthatokayi thinki justfinishedthisnexttime"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.5 Skip-gram model.wav", "duration": 618.94, "text": "deep learningprof mitesh m khapradepartment of computer science engineeringindian institute of technology madras\nlecture \u2013 tenskipgram model contd\nrefer slide time zeroeleven\nsowhati willdoisi willquickly gooverwhatweweredoingyesterday andthenbythetimepeoplecomeinwecanstartwiththenewstuffrightsowewerelookingatsothatissothisneedstobecorrectedsomeonewhopointedoutyesterday sameasbagofwordsitshouldbesameproblemsasthebagofwordsmodelsowearetryingtofixthisproblemwherewehavethislargesoftmaxcomputationwhichisveryinefficientandyouwantedwaysofgettingridofthatsothefirstthingthatwewerelookingatisusing negative sampling\n\nrefer slide time zerofortyseven\nandherethekeyideawastoconconstructthisdanddprimewheredprimewastherandom corpus and d was a true corpus\nandhowdoyoucreatethisrandomcorpusissomethingthatwasleftattheendandwhich i need to go over today\nrefer slide time onefive\n\nsoi willgooverthatandthenwerealizethatthisactuallycouldbemodeledusingsucha network where you take the dot product between the word representations\nrefer slide time onetwentythree\nandtrytomaximizethistodotproductforallthecorrectpairsbysettingupyourlossfunction accordingly\nrefer slide time onetwentysix\n\nandtrytomaximizeorratherminimizethisdotproductminimizethisdotproductforall the incorrect pairs by again setting the objective function appropriately\nsowehadthisobjectivefunctionwherewewanttomaximizetheprobabilitythatthepairiscorrectforthecorrectpairsandmaximizetheprobabilitythatthepairisincorrectfortheincorrectpairsandboththeseprobabilitieswehadmodeledusinga sigmoidfunctionand insidethe sigmoidfunctionwe hadthe dot productbetweenthecorresponding representations\nsotheneteffectisyoueithermaximizethedotproductofthecorrectpairsorminimizethedotproductoftheorratherandinminimizethedotproductoftheincorrectpairsfine\nrefer slide time twoone\nandthensonowtodaythepartwhichwasremainingaboutthecomparisonbetweendanddprimesowhati wassayinglasttimeisthatdprimeisactuallyktimesdthatmeansinsamplemorenegativeexamplesthanpositiveexamplessoifyouthinkaboutitactuallythenumberofnegativeexamplesinthelanguageismuchmuchmorethananumberofpositiveexamplesletussayifyouhavefiftykwordsinyourvocabularymostofthemdonotappeartogether rightsothatnumberisactuallyvery verylargeascompared to the number of words which can occur together\n\nsohowdoyouaccountforthisnaturalimbalancesotheysaidthatif youkeepitsamethenwearesayingthatthesizeofdprimeanddisgoingtobesamethatmeansthewordswhichappeartogetherandnotto appeartogetherwearekeepingthosetwocorporaasthesamesothatdoesnotsoundreasonablesotheydecidedthatwewillkeepit k timesoknowthisk wasa hyperparameterwhichwastunedbasedonthedatathattheyhadandcanyouguesshowtheywouldhavetuneditnowhatdoyoutuneyourparametersonwhatdidhowdidyoutuneyourparametersforthebackpropagation of the word no using what\nstudent validation set\navalidationsetisittooearlyinthemorningitfinevalidationsetsotheymighthavehadsomevalidationsetandif youlookat theoriginalwordto wordcodewhichsomeonehadpostedyesterday whichallowsyoutocomputethedistancematrixrightsoyoucouldwhatyoucoulddoisyoucouldlearntheserepresentationstakea fewpairsofwordsandtakea fewpairsofgoodwordsrightsaycatanddogorcatandfelineandsoonandalsobadwordslikecatandtruckbadcombinationsrather andseeif thedistancebetweencatandtruckismuchhigherthanthedistancebetweencatandfeline or cat and dog\nsoyouselectthatkwhichgivesyouthebestperformanceonyourvalidationsetandthevalidationsetherewouldessentiallybetofindifyougetgoodrepresentationsforwordpairsthatyoucareaboutandforwordpairsthatyoudonotcareaboutoknowtheotherthingwashowdoyoucreatethisrsoyouhavevwordsinthevocabularyyouare looking at one of those w\nyouknowthatsomeofthosehaveappearedwithwinsomecontextbutthereisthislargesetwhichhasnotappearedwithwinanycontextrightsoyouaregoingtodrawr fromthisset andthesimplestthingto do wouldbe to justdrawtheuniformdistributionthatmeansallwordsandletuscallthissupposetherearecapitalrwordshereallofthesewordscouldbedrawnfromusingtheprobabilityonebyrwhererisless than v\nisthatfinethatisonewayofdoingitjustrandomlypickanywordfromtheremainingwordsandputitapairitwithw butyouwouldalsowanttoaccountfortheindividualfrequenciesofthosewordsrightifthewordisactuallyveryfrequentpairitupmorewithw ifitisnotfrequentdonotpairitupenoughdoesthatmakesensesoicouldactuallyusethefrequenciesof eachof thesewordsandsampleaccordingto thatfrequency right instead of using a unigram distribution\nrefer slide time fivefifteen\nsotheydidsomethingsimilar buttheyhadthishyperparameteragainsobasicallyiwassamplingusingtheprobabilityofr whichis equalto countofr dividedbythenumberoftimesnumberofallthewordsinthecorpusthatisactuallythefrequencyofr dividedbythetotalnumberofwordsinthecorpussoinsteadofjusttakingthattheyhadthisweariedfactorofthreebyfourdoyouwerealizethatifyoutakethisthreebyfouryougetthe best performance\nsoletmejustmakea fewcommentsonthatsotheoriginalcodeoforrathertheoriginalskipgramorthebagofwordsmodelactuallyworkedverywellanditkindofhardalotofseminaleffectoralotofrevolutionaryeffectonthefieldofnlprightsonow everyonestartedtalkingaboutwordvectorsandhowyoucanusethismeaningfulrepresentations of words as features for various down steep nlp thus right\n\nsoattheendinnlpwhatyouaredoingisyouarecollectingofabunchofwordsadocumentora sentenceorsomethingandtryingtodosomeprocessingonthatnowearlierusedtoconstructfeaturesoutofthesesentencesusingsomehandcraftedfeaturesbutnowsomeonesaidthatthereis thisautomaticwayofconstructingwordfeaturesrightwhichisusingthismethodsopeoplereallyboughtontothatideaanda lotofworkstartedhappeningandthenlateronattheendofthecoursewewillseesomethingthat what it eventually led to\nbutlateronwhenpeoplestartedanalyzingthismorecarefullyrighttheyrealizedthattheoriginalwordtwovec implementationhada lotof theseheuristicsor lotoftheseparameterswhichneedtobereallytunedtothecoreforittobeabletocompetewithsvdrightsothatis whatwelookat theendsosvdwasalreadyonewayofcomputingwordrepresentationsahwhichwhilepopularwasnotsopopularitwasusedfor variousreasonsbut it was not like everynplapplicationis usingsvdrepresentationsrightbutnowit is almostlikeeverynplapplicationis usingwordrepresentations\nsolateronwewillseethatsomeofthesethingslikethree byfourorkthevalueofkthevalueoflearningrateandsomeotherhyperparametersifyoureallytunedthemveryverywellit is onlythenthat as thiswordtwovec algorithmcanbeatthe worldrepresentationslearnedbysvdorrathertheotherthingthatif youintroducesomeparametersinsvdandtunethembecauserememberforsvdtherewasnotuningrightwe just got a solution we just had the closed form solution which is the eigen vectors\nbutyoucoulddosomethingsforcreatingthecooccurrencematrixifyouintroducesomefactortherewhichisalsolookslikethisthree byfourorsomethinglikethatorifyoualsointroducedsomethingwhichlookslikea kthenyouwillbeabletogetthesamekindofrepresentationsorequallypowerfulrepresentationsfromsvdaswhatyougetfromwordtwovecsothatiswhyi amstressingonthesehyperparametersthereissomesignificance of those"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.5 (Part-2) Skip-gram model (Contd.).wav", "duration": 474.46, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 tensix \nlecture \u2013 ten  \ncontrastive estimation \nso we will move on t o the next way of dealing with the expe nsive softmax so \nremember that so this is known as contrastive estimation  \nrefer slide time zerotwentyfour \nso remember that this is where we are in the story that we saw the bag of words model \nwe saw the skip gram model and we saw that both of them have this expensive softmax \ncomputation at the end and that is the problem we are trying to deal with so we saw \none way of dealing with which was negative sampling so you i hope you saw that there \nwas no expensive computation there \nthe only computation there was the dot product  between the two words which appear \ntogether or which do not appear together now let us see what happens in contrastive \nestimation \nrefer slide time zerofiftyone \n \nso here again you use a same idea so you have a positive sentence or a positive \nexample he sat on a chair you create a negative sentence which you replace the word by \nsome random word  now you construct a feed forward network like this which takes \nthese two one hot representations basically uses your word context matrix to give you the \nsummation of these two representations right that is exactly what we have done in the \nskip gram model now you have this hidden representation which is the sum of the two \nword representations \nnow from here on instead of doing this s oftmax computation which we had ea rlier we \njust predict a single score ok we just predict the score for this word pair being of correct \nword pair  we do the same thing with the random pair so we take sat we take  \nabracadabra and the add up there word representations you get this hidden  \nrepresentations and you get a score sr fine so what is the output computation right \nnow what is the  is it a matrix operation is it a scalar operation is it a vector \noperation what is this h is equal to  we need to change this to k on the slide plea se \nnote so what is this product w into h just a dot product between two vector \nw is just k cross one  that means it is a vector  so as compared to k cross v earlier we \njust have k cross one you get that how many of you get this we have a very simple \ncomputation at the end  ok but now how we set up by loss function  earlier i could set \nup the loss function as maximizing the log like it of the correct word but now  i just \npredicting two scores so what is the loss function what should i try to intuitively  do \nand today we are going to see a new loss function which we have not seen earlier so \ntry to think about this what would you do forget about the math forget about the \nmachine learning all that what would you actually want what is your wish list t hat \nshould be easy to characterize \nscore s score sr do you want this or this first one right you want s to be greater than sr  \ncan you think of making an objective function out of this you want to maximize \nstudent refer time threeeighteen  \n s minus sr  fine that is a good starting point  so would you be happy with this what \nwould you want this or this both cases s is greater than sr right what would you \nwant \nstudent a big margin \na big margin fine \nrefer slide time threethirtynine \n \nso we would like sr to b e greater than s and not just so we could try to maximize s \nminus srok but we would also like this difference to be a certain margin that means  i \nwould want s to be greater than sr by at least a margin of m and that m is something i \nwill decide so i could say that it should be at least ten points greater than sr or one point \ngreater than sr depending on the scores that  i have so all my scores are between zero to one \nthen probably a margin of zerothree or zerofour is  ok sounds reasonable  right that means s could \nbe zerosix and sr could be zerotwo does that make sense \nso what i am saying is what  i am trying to say is that this is my sr  i want s to be \ngreater than sr i am not just happy with that i am saying that even if i add a margin to sr \neven then this condition should hold right \nand that is the same as saying that s sr and there should be at least a margin of m \nbetween that that is the difference that i accept i am not  if you tell me that s is zeroninetynine \nand sr is zeroninetyeight where then you are not really distinguishing muc h i want at least s to be \nzeronine and sr to be at least less than zerofive or somewhere \nrefer slide time fourfiftyfive \n \nso there should at least some gap between that and that gap is m  so instead of \nmaximizing s minus sr  i am going to maximize s minus sr plus m is that fine  ok \nnow suppose you are at some point of training i will have some need some parameter \nconfiguration that means you have learned some values for vc and vw and you do \nthis forward propagation compute s and sr and we actually find that thi s condition \nholds right so right now my loss function is this at some point you are doing this and \nyou observe that this condition holds that means s is actually greater than sr plus  m in \nthat case what do you want a loss to be how many of you get the question \ni want that s and sr should be separated from a margin of m in the favor of s i am \ndoing my training i am at certain configuration for uc s and vw s and so on i pass it \nthrough the feed forward network and  i get s and sr and i observe tha t this condition \nalready holds \nis my network doing anything wrong at this point it is doing it is job properly what \nshould be the loss that  i back propagate zero a gain gets that there is nothing to correct \nhere i do not need to back propagate any loss \nrefer slide time sixfourteen \n \nso then can you give me the full objective function maximize this but at this condition \nalready holds then do not do anything is that fine  so that is about this so and again \nobserve that we have gotten rid of the expensive softmax computation"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.6 Contrastive estimation.wav", "duration": 384.36, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nmodule \u2013 tenseven \nlecture \u2013 ten \nhierarchical softmax \n refer slide time zerosixteen \nthe next one is a bit tricky  so the third solution is  to use something known as \nhierarchical softmax this is a bit  counterintuitive in the sense it is a very smart trick \nbut it is not something which is very obvious so just pay a bit attention on this it is a \nneat way of handling this large vocabulary th ing and this  i think used in various nlp \napplications where speed is important not often but wherever speed is important \nrefer slide time zerothirtyfive \n \nso this is what our original network was this was the either you take it as a skip gram \nmodel or you take it as a continuous bag of words model  right let us take it as a \ncontinuous bag of words model \nyou had a word as the input and then you had this larg e prediction and you had this \nsoftmax computation which gives you the probability and you are try ing to maximize \nthis probability for the correct word right where v w is the correct word  \nrefer slide time onefour \n \nnow instead of this the hierarchical s oftmax says that you construct a binary tree such \nthat your tree has how many nodes v nodes  it  has one node corresponding to every \nword ok and there exist a unique path from the root node to every leaf node every leaf \nnode corresponds to a word and there is a unique path from the root node to leaf node of \ncourse there will be overlapping thing s for example for this word the path is these \nnodes and for this word also the path is like there is some overlap in the path \nbut for every word there is a unique pat h how many if you get that set up now let lw one \nlw two up to lw p be the nodes on this pa th so i am calling this as lw one lwtwo lwthree sorry \nsorry sorry sorry yeah actually it is so actually this is l on one l on two l on three that means \nthe third node on the path of on the second node on the path of on and so on right that is \nhow it is going to be and let pi w be a binary vector \nrefer slide time twofifteen \n \n so what is the size of pi w actually binary tree log of v right so the size of  pi w \nvector is going to be log of v so if there are eight leaf nodes you will have three nodes as the \nsize of the vector so for each of these things this vector takes on a value one so here the \nvalue would be one because the path branches to the left if the path branches to right \nthen the value is going to be zero right so for every node or every word i have this way of \nuniquely defining it is path i can say that the path is one zero zero is that fine for the word on \nthe path is one zero zero if i consider some other word the path would be different is that fine \nand of course i have assumed there are only eight words here right that is why this holds if \nthere are either otherwise i would have a vector whose size is log  v right now my v is eight \nso it is just three \nrefer slide time threetwentyfive \n \nfinally each of these internal nodes is associated with a vector ok so i have u one u two u three \nso how many of these would i have if there are v nodes at the leaf how many nonleaf \nnodes do you have in the binary tree v you all know this right \nso if you have v nodes at the leaf then you will have v nodes internally so for each \ninternal node i have a vector associated with it so how many vectors do i have in all \nu v and my input side is still the same right i have this w word or w context depending \non whether it is a skip gram or by or continuous bag of words model \nso how many parameters does this model have is it same as the bag of words model or \nless than the bag of words model or more than the bag of words model this is how you \nwill think you will see how many input parameters do the poo l two models have how \nmany output para meters to the two models are input parameters same  output parameters \nhow many vectors do you have u one to uv each of size k same as the original model \nright it is just as an original model i had put everything inside as w  context which was k \ncross v right so it is the same number of parameters \nrefer slide time fourfiftyseven \n \nso the total number of parameters in the network is the same \nrefer slide time fiveone \n \nnow for a given pair w comma c which is the  correct pair we are interested in the \nprobability p of w given vc nothing great about this it is the same as i have been saying \nalways that we want the pa probability of w given c what we are going to model as w  \ngiven vc because vc is the representation of c and we model this probability now as the \nfollowing thing why does this make sense you just assume  this is on and these are on \nk\u2019s right so on one on two on three why does this make sense \ni will get the word on at the output only if the first element on the path was pi on one and \nthe second element on the path was pi on two up to the k\u2019 th element on the path was pi on \nk how many forget that please raise your hands  ok right so that is how we are \nmodeling it is it but what about pi on one pi on two pi on k how do you model that at \nleast this f orm is clear to everyone right if it is not let me know because then you not \nunderstand the rest of the stuff yeah ok \nso now see that modeling part is always in your hands right you know that you want \nyou are interested in a certain probability it depends on you how to model it so now \nwhat you have done is you have con constructed a binary tree now i am interested in p \nof on given some word vc right or some word vector vc now i can say that but the way \ni am thinking about this is that i get the word on only if the first if i started from the root \nnode the first vector took on the value one or the first  branch took on the value one the \nsecond branch took on the value zero and the third branch took on the value zero so that is \nexactly what i am saying here \nit is a probability that the first turn that i took was a left turn then a right turn then a \nright turn yeah the path is you have constructed the binary tree and the path is fixed \nnow for all the words how to construct the binary tree is a sepa rate thing but the binary \ntree has been constructed and every word has a unique path associated with that so that \nword will occur only if that path is executed  right so i am just trying to find the \nprobability of that path being executed \nnow i need to tell you what does each so how many  terms are there in this product k \nterms right how do i estimate each of these k terms is what i need to tell you  ok can \nyou think of it how would i model each of these probabilities remember that every \nnode has a vector associated with it how many if you can think of an answer i hope i \nare you saying what i think you are saying \nrefer slide time sevenfortyeight \n \nso this is what i will do so as i said for the on example this is what you want this is \nthe path that you want to be executed \nrefer slide time sevenfiftynine \n \nand i am going to model it as this \nso getting a left turn i model it using this that dot product between the original word \nvector which was the input word vector which was vc and the node represen tation of the \nnode associated with that particular node does this make sense so i will tell you what \nwe are trying to do so this path was clear that the probability is going to be a product of \nthese probabilities  \nnow i want how do i get each of these probabilities so that is again in my hand right i \nam going to say that i am going to train my parameters vc and ui where ui is the \nparameter corresponding to every node i am going to train it in a such a way that \nwhenever i want this to take on the  value one this should be close to one  ok because i will \nset up my loss function accordingly we will see the loss function \nbut i am saying that whenever i want the probability to be equal to one i am going to use \nthis to computed and alternately when i wa nt the probability to be zero i am going to take \none minus that which is just this is that fine okay let us go ahead a bit and then we will \ncome back if you are still lost \nso what does this actually ensure this ensures that the representation of a context word \nvc will have a very high similarity with the node ui if the path takes a left turn there and \nit will have a very low similarity with the node ui if the path takes a right turn their how \nmany if you get this part based on if you assume that this i s how we are going to model \nit when is this going to be high when the dot product between vc and ui is high when \nis this going to be low  \nwhen the dot product between these two is  low right there is a negative yeah so we  ok \nsorry i or rather when is  this going to be low right so you get that so it is coming so \nthe word representation which is vc which is this guy would come to the  come close to  \nall these representations or move away from them depending on whether you want to \ntake a left turn there or a right turn there \nnow what would happen to words which appear in similar context the same thing that \nwe have been discussing so far right they will come close to the node representations \nwhich are along the path right is that fine so this is the context representation right \nthis is actually you are representing every context word by these three representations now \nif a word appears in the same context it is representation is going to either come close or \nmove away from these representations  right so words appear in the same context if \nyou have cat and you had sleep here then cat has to come close to this it has to move \naway from this and it has to move away from this is that clear that is how we have set \nup the probabilities \nnow i nstead if i had dog and again you had the context word as sleep now the \nrepresentation of dog also has to go close to this it has to move away from this and it \nhas to move away from this so in effect again the same thing is happening that the \nrepresentation of cat and dog are moving in the same directions so they will eventually \ncome close to each other how many if you get this intuition \nrefer slide time eleventwentyfive \n \nand how many computations do you need now to compute the probability of this so \nearlier you acquired that complex softmax computation how many computations do you \nneed now you definitely need these many computations and each of these \ncomputations requires a sigmoid over or dot product  right so that is much much lesser \nthan so you just need these many dot products  as compared to your expensive s oftmax \ncomputation earlier \nso you see how you get the savings using the hierarchical s oftmax so this is as i said \nthis is not very intuitive it is like a really smart trick and it takes  time to get your head \naround it but i am sure if you go back and look at the slides you will get it right if it is  \nif you have just got fifty percent of the idea here that is typically how it happens every \ntime but and i probably not figured out a better  way of teaching this but once you go \nback i am pretty sure that you will get to understand what is happening \nso now the question is how do we construct a binary tree  anyone has any thoughts on \nthat do we need to ensure certain things while constructing the binary tree okay i will \nask this as a quiz question  just note that there is some subtlety here ah in practice this \nis what is done you just randomly arrange the nodes on the leaf nodes and then you just \nconstruct a binary tree from there  righ t so you have distributed all your leaf nodes \nrandomly and on top of that you have constructed a binary tree my question is there a \nproblem in doing that which i will ask you on"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.7 Hierarchical softmax.wav", "duration": 769.82, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 teneight \nlecture \u2013 ten \nglove representations \nso now from here we will move on to yet another way of learning word representations \nwhich is known as the glove representations \nrefer slide time zerotwenty \nso the count based methods rely on global co  occurrence counts from the corpus for \ncomputing word representations  that is what we saw in  svd t hey look at these co  \noccurrence counts and from there they build the word representations \nthe predict based models  set up a learning problem where you have this feed forward \nnet network and it tries to predict certain things from the given words and then you learn \nthe parameters of that network and you set up the task in such a way that the parameters \nactually correspond to word representations  so this was the difference between count \nand predict based methods  n ow what is the obvious next thing to do  like hear the \nanswer from a few of you but i want to hear it from everyone \nwhat is the obvious next thing to do  you have count  based methods you have predict \nbased methods combine the two right so come up with some kind of a hybrid so that \nis exactly what glove does which is known as global vectors \nrefer slide time oneten \n \nso i will go back to the co occurrence matrix so remember x ij encodes the important \nglobal information about the word  i and j and whether you replace it by pmi or ppmi or \njust keep the counts it just gives you some information about how many times these two \nwords actually appeared together \nso x ij encodes this global information and  i call it global because it is computed from \nthe entire corpus fine why not learn word vectors which are faithful to this i nformation \nso what do i mean by that suppose v i is the representation of the i\u2019th word and v j as a \nrepresentative the j\u2019th word which i want to learn  i do not have these representations  i \nwanted to learn now this gives me the dot product between th em which gives me the \nsimilarity between them  why not i set up my task in such a way that this similarity  is \nactually proportional to this probability \nso what does a similarly tell us how well these two go together what does p of j given i \ntell us how likely j is given  i right so does that make sense to have this analogy that  \nthe dot product tells me the similarity the other notion of similarity is that how likely j is \nto appear in context of i which is given by p of j given i so why not set up my task such \nthat whatever vector as  i learn are actually faithful to this global similarity that  i have \ncomputed from the entire corpus how many if you get this intuition \nhow many if you see the difference between this and the predict based models  in the \nprediction based models you are operating at one word pair at a time here you are \nlooking at these global counts  ok we are trying to directly learn vectors which are \nfaithful to your global similarity as given by your co  occurrence counts  you get the  \nmerger between the two methods you should not get it yet because we still have to do \nsomething or at least you get the intuition now what is p of j given  i it is actually this \nok \nso i can write it as this  ok and similarly i can write the other guy v  j transpose v  i and \nthat is going to be different because that is going to have p i given j instead of p j given i \nso i will have log x ij is fine but instead of x i i will have x j here \nrefer slide time threethirtyfour \n \nnow if i add these two equations so i am going to add this equation and this equation  \nso the left hand side i just get two times v i transpose v j because v i transpose v j is the \nsame as v  j transpose v  i and on the right hand side  i get certain quantities  so this is \nwhat i would actually want my word vectors to look at look like  i would want my word \nvectors to be such that  when i take their dot product they give me the quantity on the \nright hand side and this quantity has come based on counts learned from the corpus \nso i have counts on the right hand side and i have learnable parameters on the left hand \nside so you see how we are merging these two  but how do you learn this problem  \nnow it is ok to say what i like what i have said now is that this is what  i desire i desire \nthat my word vectors should be learned in such a way that they are faithful to the global \ncounts through the following equation  this is what  i desire desiring something is one \nthing but now how do i set this up as a learning problem \nso when i ask you what is  the learning problem what do you need to think about  \nobjective function good that is a good start  so what is the objective function for this  \nwhat are the parameters of the optimization you are optimizing with respect to what the \nv i is and the v  j\u2019s right all the word representations how many of those do you have  v \neach of size k  so those are your parameters for optimization  now what is the loss \nfunction if i give you the loss function it will look very very obvious  but i do not want \nto do that \nso just continue thinking about that  while i will make some more simplifications to \nwhat we have here now  what is this count  this is the co  occurrence count how many \ntimes these two occur together  what is this count  t he number of times the word  i \nappear so this depends only on  i what is this count  the number of times the word g  \nappears so to make the model more flexible  that means give it some more freedom \nwhat i am going to do is instead of log x  i and log x j i am going to introduce parameters \nb i and b j ok \nrefer slide time fivethirtytwo \n \ni am saying that these parameters can also be learned so effectively using all these three \ni should be able to get this this is what i desire now set up the loss function using these \ntwo things come on  that should not be  so hard what is this this is what you are trying \nto predict what is this this is what you know is true because you have computed from \nthe corpus now can you come say the loss function the difference between these two \nright so you could have this as the loss function  this is the predicted value using \nmodels parameters this is the actual value computed from the corpus \nso think of this that you are trying to learn the parameters in such a way that you end up \npredicting this and if you predicted this  you know you have done the right thing  ok and \nthis you know already because you have computed it from the corpus  so this is the true \nvalue and this is the predicted value  so as in any loss function predicted minus true the \nwhole square does that make sense how many if you are fine with this \nrefer slide time sixfortyfour \n  \nso now how will you train in this network  g radient descent so  i will use gradient \ndescent and you will get these parameters \nso there is a bit more on this wh ich i will not cover actually  so i will just skip this \nslide you can go back and take a look at  it it is a some slight modifications to this yes  \nso again the same idea that cat will go close to all the so here again you will have the v \ni and the uc  s right  so you will have cat will come close to all the words that it co  \noccurs with feline will also come close to the same words  so maybe i have not used to \nright notation here  if you need to change it again  so we should have v  i\u2018s and u  j\u2019s \nright so again you have one word matrix  word representations and the other is the \ncontext representation then it is fine right that is the problem here how many if you get \nthat right again we have to have these two things let us change that everywhere"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.8 GloVe representations.wav", "duration": 438.24, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 tennine \nlecture \u2013 ten \nevaluating word representations \nnow we come to this important part about how do you evaluate word representations \nrefer slide time zerosixteen \nso there are different tasks that are set up i hope some of you have read that paper  and i \ncan see that none of you have read that paper so semantic relatedness is one way of \nevaluating word representations \nrefer slide time zerothirty \n \nso ask humans to judge the relatedness between a pair of words  so i construct some \npairs of words and i show them to a human  and ask them how related do you think \nthey are on a scale of one zero to one so it is likely for cat and dog someone would say zeroeight or \nat least you would expect values greater than zerosix \nnow you have learned the representations using your model it could be any of the \nmodels that we have seen so far continuous bag of words skip gram or glove vectors \nso these are  the three things right continuous bag of words skip  gram which is known \nas wordtwo vec and the glove representations and within them yo u could have this \nhierarchical softmax and other things and so on \nso you could if i asked you what is the similarity be tween cat and dog according to \nyour word representations  you could just use the cosine similarity and tell me that this \nis the representation right so now i will have many search words w one w two for which i \nhave the human judgment and i have the model judgment right so i will have w one one w two \none then w two one sorry one two and so on i will have many such word pairs for each of these \nword pairs i would have the human judgments and i would have the model judgments \nright how close do the humans think they are and how close do the think they are \nnow i can compute the correlation between these two  decisions or these two random \nvariables and i would want that for a good model this correlation should be high so \nwhenever humans said that the two words are  actually simi lar the models word vectors \nshould also predict a high cosine similarity and whenever humans said that the two words \nare not similar the models word vector should also result in a low cosine similarity how \nmany forget this \nrefer slide time twoseven \n \nso that is one way of evaluating how good your word representations are right so as i \nwas saying earlier how do you tune those parameters so you could have such a set \nonce you have learned some word representations and you want to see whether \nparameter k one was better than  sorry rather hyper parameter k one was better than hyper \nparameter k two you could just take those two word representations learned by these two \ndifferent hyper parameter settings evaluate them on this corpus and whichever gives a \nhigher correlation you can keep that hyper parameter how many of you get that \nrefer slide time twothirtyeight \n \nother task is synonym detection so from a resource known as word net or from other \ndictionaries you could get all the synonyms of a word  so then peop le create a corpus \nwhere you give us in  sin a word and give four candidates or some k candidates out of \nwhich one of these is the correct synonym the others are just distraction words right and \ndistracting words now  what would you expect your word represe ntations to do you \nhave word representations for all of these what would you want how would you pick \nup the synonym based on word representations \nstudents refer time threethirteen \nthe one which has the highest cosine similarity  so again you will compute  the cosine \nsimilarity you will rank these and you  will pick up the synonym right  and now again i \ngave you one hundred such instances i gave you a word for candidates and i gave y ou one hundred such \ndifferent word comma  candidate pairs and you pick the synonym for ever yone and see \nfor sixty of them you got it right then your accuracy sixty percent so that tells you how good \nyour word representations are \nagain if you are given two different hyper parameter settings one gives you sixty percent \naccurate the other gives you seventy p ercent accurate you will probably go with the one \nwhich gives you seventy percent accurate they are find how you can use this  \nrefer slide time threefortynine \n \nthe third is analogy task \nrefer slide time threefifty \n \nwhere you find the nearest neighbor of this opera tion what should it be  granddaughter \nthis is this analogy teller brother is to sister as grandson is to  something right so now \nthe idea here is that if i mea n it is like pretty weird right  so if i take brother minus \nsister i get something \nnow if i a dd grandson to that then i should get granddaughter it is intuitive in a way \nright i mean this is what you expec t your word vectors to do right  so that is how the \nanalogy task works so you could set up an analogy task you could have and you could \nget several such an analogy tasks from online tests and so on and you would want your \nword representations to exhibit this kind of a behavior right \nso again you have these one hundred analogy tasks for each of these you know the true answer \nand from each of these you predict the answer from your word representations and first \nsee for how many of them you get it correct then you could also have a syntactic \nanalogy so you can tell me what this would be right in fact here again it should be the \nother way round  we works minus we work thus we speak would be we speaks right \nso that is the syntactic thing right \nso you are getting a different form of the world so your word representations should \nalso have this kind of properties that is what you desire so just evaluating whether your \nword representations show this kind of a property or not so we have seen three tasks \none is semantic relatedness whether a pair of words how do humans rank it and how do \nthe model how does the model rank it  then the synony mous detection and the analogy \ntasks in each of these you do something with the word representations in the first two \nyou use the dot product and this last one you use some arithmetic operation over the \nword representations \nso you would want v brother minus v sister is equal to v grand son minus v \ngranddaughter right so v granddaughter  right so that is there is a plus minus error \nthere  \nrefer slide time fivefiftyone \n \nso now which algorithm gives the best result right so whenever we see a bunch of \nalgorithms same as we did with adam and refer time fivefiftynine and so on we always \nwant to answer this question which of these gives the best result  \nso there was this study done by boroni et al in two thousand and fourteen that show that the predict based \nmodels right which  are either which are the predict based models actually skip gram \ncontinuous bag of words and even glove for that matter right because it is also a predict \nbased model these continuously or consistently outperform count based models  that is \nwhat they said but a year later there was a separate study done by someone  and in my \nopinion this was a more thorough analysis because  the earlier study right they did not \nreally give svd a chance to win in my opinion this is all on camera \nbut the later the second  set of guy right they gave svd a chance to win so i will tell \nyou one example of how they gave is really a chance to win so remember in wordtwovec \nyou had this weird three by four which you are using to raise the probability right now what \nthey did is they s aid even in the co occurrence matrix actually these counts that you \nhave if here you are using them  by three by four in the case of wordtwovec and  getting better \nresults why not do the same thing in the cooccurrence matrix also \nat the end of the day you are rai sing the count to three by four right so whatever counts you \nhave here based on that you will compute ppmi or pmi or whatever but first why not try \nto adjust these counts so why not have a parameter k such that you can raise the counts \nto this parameter and then do all those computation and that is fair because the \nwordtwovec has a parameter hyper parameter so why not give a similar hyper parameter \nto svd \nsimilarly they did something to take care of the k negative s amples which wordtwovec  \nhas why not giv e sv d also similar chance right  so when they did these kind of \nadjustments they found that after these modifications svd does as well as or even better \nthan wordtwovec models for the similarity tasks but not for the analogy task  but the \nanalogy task was the last task right brothers to sister\u2019s grandsons to grandmother right \nso in most cases we care about similarity and in very few cases we care about analogy \nif you are doing nlp application so that means in most cases svd would just be fine \nso that is what i just said at the beginning"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 10.9 Evaluating word representations.wav", "duration": 490.82, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science engineering \nindian institute of technology madras \nmodule \u2013 tenten \nlecture \u2013 ten \nrelation between svd and wordtwovec \nnow and later on actually the same guy s they a lso came up with this forma l relation \nbetween svd and wordtwovec which is again under some assumptions \nrefer slide time zerotwenty \n\n \n \nrefer slide time zerotwentyone \n \nbut i am not going to do the proof here i am just going to give you the intuition so \nrecall that svd does a matrix factorization of the co occurrence matrix levy et al showed \nthat wordtwovec also does such a implicit matrix factorization so what does this mean \nso recall that wordtwovec gives us w context and w word it gives us these two parameters \nso they say that there exi st a matrix m such that  ok this is wrong just be the product of \ntwo matrices right this is the product of two matrices it should be w context transpose w \nword or just see which way the transpose should be  \nso it is actually a product of these two matrices t hat we have learnt ok and what is m m is \nactually nothing but the pmi matrix minus this log k where does the k come from \nwhat was k the negative samples that you have taken  so they actually showed that \nwhatever representations wordtwovec runs  it is ac tually doing a factorization of this \nmatrix where this matrix has a strong connection to the pmi matrix and svd also \nworks with the pmi matrix \nif you take svd matrix and do these modifications to it that means you take every \nvalue which is the pmi and  then subtract this log k from that and then just do an svd \nof that you will essentially get back the same word representations as wordtwovec \nthere was some certain assumptions made in the paper but that is  i mean i do not want \nto go into those but the key idea here is that you can actually show that svd and \nwordtwovec are actually connected and if you think about it at an intuitive level though \n \n \nthese methods are relying on the same underlying principle that words appear together \nbased on that the word representations get updated or in svd based on there the counts \nget updated and you then eventually end up with certain representation \nnext the underlying principle is the same so there has to be a connection right it is not \nthat they are doing fundam entally something different both of them are relying on the \nidea of co occurrence or  the idea of distribution right  so they have to at some leve l be \nsimilar in some ways right so that is what they finally showed and so now but still in \nmost applications wordtwovec is preferred  \nso one reason for that is that this is an iterative training procedure right as compared to \nsvd and i come back to your question right how do you do that  how do you \ncompute the eigenvectors of x transpose x and the answer is there is no simple way of \ndoing that and you have to do that expensive matrix multiplication \nand then rely on various very smart libraries for computing the eigenvectors  which are \nstill order n raise to two point something or something like that they not order n cube but \nthey are still order n raise to two point something means they are still expensive and then \nof course you have this memory issue that if you have a very large vocabulary your \npmi matrix is going to be very high dimensional and then  you need to do the \nfactorization of that high dimensional vectors  so that runs into these computational \nefficiency issues \non the other hand wordtwovec by design is an iterative algorithm because you are going \nto grade gradient descent which is that ev ery times that you are going to update some \nparameters of the model you are not learning all the parameters together you are only \ndealing with some parameters at every time set right so that is more computationally \nefficient especially if you do the contrastive divergents or the negative sampling or the \nhierarchal sample so that is why perhaps it is still more popular than svd"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.1 The convolution operation.wav", "duration": 1068.54, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 eleven \nthe convolution operation \nso why  let us start  so far in the course  we have  looked at  feed  forward neural \nnetworks we have seen how to train  them and  we have  seen  two special  cases of feed \nforward neural  networks one  was  the auto  encoders for  learning  representations  or \nlearning latent representations of inputs and the other thing that we had seen was how to \nuse a feed forward neural network to learn word representations where we saw this word \nto wake algorithm and it is different variants it was continuous bag of words skip gram \nmodel graph a nd so on so those are all since some since applicati ons of the feed \nforward neural network \nand now we  will move on from  there though we will look at  different type of  neural \nnetwork today  which is convolutional neural networks  and we  look at  some specific \narchitectures which have become popular over the past few years ok \nrefer slide time oneseven \nso with that i will start this lecture on convolutional neural networks so in the first \nmodule we will look at the convolution operation ok \nrefer slide time oneeleven \n \nso let us see so suppose we are tracking the position  of an aeroplane using a laser \nsensor at discrete time intervals right so you have this ok so you have this aeroplane \nsuppose it is going from say chennai to delhi and at discrete time intervals you are \nseeing the tracking the position of the aeropl ane right how far it is from chennai at this \npoint right may be it is fifty kilometers one hundred kilometers and so on  \nand now your laser you think that it might be noisy it might not be giving you very \naccurate measurements so you would be taking these measu rements and say intervals \nof course it is not in practice you would not do that but just indulge me for the purpose \nof illustration that say you are taking these measurements every five seconds or ten seconds \nor something like that  \nnow since your sensor is noisy instead of relying on a single measurement you would \nprobably want to take the average of the past few measurements that you have taken so \nthat would give you a more accurate representation of what the current position is does \nthat make sense like you are taking multiple measurements and taking averages of those \nright and of course more recent measurements are more important as compared to the \nprevious measurements right so this is suppose at time step t say this was t minus five \nseconds and this was t minus five minutes suppose  \nso obviously you would not want to take give a very large weightage to the \nmeasurement that you are taken t  five minutes back right because the plane would have \nmoved by a lot by that time so it rely more on the rece nt measurements and less on the \nprevious measurements right so now the mathematical way of lighting this is that you \nknow the positions or the readings that you have taken at time steps one two three up to time \nstep t you are interested in the revise estima tion of this measurement right so you have \ntaken some measurement at time step t and you want a revised measurement of that and \nthe way you are going to compute that is you are going to take a weighted average so \nw is the weight of all the previous measurements that you have taken right \n refer slide time threeseventeen \n \nso the measurement that you take a t  one t  two t  three all the way up to t minus infinity and \nfor each of them would have a weight associated with this \nso this operation right this thing  you can write it as the following operation that you \nhave a vector of measurements or an array of measurements which is x and you have an \narray of weights associated with these measurements the farther the measurement from \nthe current time step hopefully smaller is the weight assigned to that and this operation is \nknown as the convolution operation right \nrefer slide time threethirtyseven \n \nso you have x which is the input w is known as the filter and the operation that is \ndefined as this equation is known as a convolution operation right \nrefer slide time threefortynine \n \nlook but of course in practice you would not do this from infinity right you would \nprobably keep a window you will say i will rely on the previous six measurements that \nmeans whatever i took at t  one second t  two second up to t  six seconds right beyond that \nit does not really make sense so let us see how this computation happens so this is \nweight array so now what would be the dimension of this weight array how many \nentries would it have \nstudent seven \nseven right zero to six so seven entries ok and this is what my situation looks like right so this is \nthe x the measurements which i have taken using the laser ok so i have taken some \nmeasurements now i am at a particular time step and i want to make a revised estimate \nso i have this xt and from that i want to compute st and the way i am going to do that is \nby taking a weighted average of all these previous measurements is the setup clear to \neveryone ok  \nand now this is what my formula is going to be so the revised estimate of ssix is going to \nbe whatever was x six into wzero that means the weight assigned to the current time step x five \ninto w minus one that means weight assigned to the time step  one xfour into  two and so on \nso you get this ok so  i have these seven weights and i will multiply with them with the seven \nprevious readings one is to one multiplication and i will get the weighted average and using \nthat i get a revised estimate \n refer slide time fivetwentynine \n \nnow i want to get a revised estimate for the next entry how will i get it i will just slide \nthis weight matrix right \nso i will just slide it by one i will again do the same computation and get the revised \nmeasurements again for the next entry i will slide it by one slide it by one slide it b y one and \ni will keep getting these entries ok so everyone gets the setup how do you do the \nconvolution operation it is basically a weighted average of the previous entries fine \nso here the input as well as the kernel is kind of one dimensional right you so you have it \nis so you do not have a twod input here you just have a single dimensional input here \nrefer slide time sixone \n \ncan you use a convolution operation on a twod input also do you know of any twod inputs \nimages right so we can think of images as twod inputs now again i am trying to do the \nsame thing the setup is the same the story just changes from laser to a camera now so \ni have taken an image maybe the image was captured and i am not very confident about \nall the pixels that i have captured ok \nso now for any given pixel i want to reestimate it using it is neighborhood that is what \ni want to do ok so this is the pixel i am going to look at some neighborhood around it \nright so every cell here is one pixel just assume that every  cell here is one pixel so now i \nam going to re estimate this pixel by taking a weighted average of all its neighborhoods \nright so now can you tell me what is my filter going to look like in this particular case \nmy f ilter would be just three\n\uf0b4 three right so whatever neighbors i want to average on for \nevery neighbor i want a weight so if i am going to average on a neighborhood of three \n\uf0b4 three \nthen for each of these i will want a weight so my filter would also be of size three \n\uf0b4 three how \nmany of you get this ok \nso we now like to use a twod filter which would be m cross n ok and in general it \nwould be m \n\uf0b4 m so it would always be a square filter but i am just takin g the case \nnow what this nasty looking formula is doing right so i have a particular pixel so \nthis is an image so i will refer to this pixel as i ij right so it is the i th ijth entry in the \nimage i want a revised estimate for that i want an sij for that  \nso the way i am going to do that is i am going to look at m rows and n columns before it \nright so i am going to look at this neighborhood of m cross n ok and for each of \nthese i would have a weight associated with it so if i am looking a t say for example \nthis was four  four this pixel was four four then i will look at four \u2013 one four  one so that would be i three three \nso i will look at that neighbor and with that neighbor i would have some weight \nassociated do you get that how this formula expands \nso this formula would have m cross n terms for every term you would have a have a \nweight and that weight you can just represented as this filter matrix so you get this \nwhat this formula is doing it looks a bit nasty but it is just the weighted average of all \nthe neighborhood that you have and the neighborhood is a two dimensional neighborhood \nin this case how many if you get this properly ok  \nnow this in this formula actually i am looking at minus a and minus b that means i am \nlooking at previous neighbors right now you should have these questions right why \nprevious neighbors why not future neighbors so why am i not looking at this \nneighborhood \nrefer slide time eightfiftyone \n \nso there is no correct answer here different convolution operations i  mean different \npackages use different convolution operations but the most standard one i believe is \nwhen you look at the next neighborhood right that means you at this pixel and you will \nlook at this neighborhood the neighborhood after it right not the before it ok \nand in fact so this is the formula that i am going to look at plus j and plus p that \nmeans i am looking at pixels in the rows after this and in the columns after this pixel all \nof you get this instead of before now what is even more natural to do the names \nsurrounding thing right so i will have this pixel and i will look at it is such a way that \nthis pixel is the center of the neighborhood right so that is what i am going to go \ntowards after a couple of slides and that is what i  will use for all my convolution \noperations but in terms of textbook definitions these are the definitions that you will \nfind in textbooks ok  \nrefer slide time ninethirtyfive \n \nso let us let us apply this to a toy example so i have this input which is two d imensional \ninput i have a kernel which is a two\n\uf0b4 two kernel so my m is equal to n is equal to two so i \nam going to place this kernel at this location ok and then what will i get as the output \nrefer slide time tenone \n \na into w plus b into x plus e into y plus f into z right and i will keep sliding this to get the \nother entries do you observe something about the input and the output \nstudent refer time tenseven \nsize the output size has reduced why we will get back to this \nrefer slide time tennineteen \n \nso right now i just you to notice it is obvious nothing great about it but i will just get \nback to it more formally later on \nso for the rest of the discussion we will use the following formula for convolution \nwhich is the centered formula right so \ntwo\nm  to \ntwo\nn  that means i will be looking at a \nneighborhood which is centered on the pixel of interest that is why this  \ntwo\nm to \ntwo\nm  is \nthat fine ok  \nrefer slide time tenthirtyfive \n \nso this is how i am going to look at it so this is how i will place if this is the pixel of \ninterest which i want to re estimate i will replace the kernel such that it this pixel lies at \nthe center of the kernel ok \nrefer slide time tenfortynine \n \nso we will be looking at both preceding and succeeding neighbors ok \nrefer slide time tenfiftyone \n \nso let us see some examples of twod convolutions applied to images \nrefer slide time tenfiftyseven \n \nso this is an image i decide to apply the following convolution operation to edge ok tell \nme what the resulting image would be \nstudent blurred \nblurred why blurred \nstudent we are taking average \nwe are taking the average right so it would be blurred you get the i ntuition so this \nkernel basically i have fitted at every pixel and i have computed the average around it \nand place at pixel by that average value and when we are going to take average things are \ngoing to get blurred right because all the sharpness is gone ok  \nrefer slide time eleventwentythree \n \nnow let us look at this kernel what will this do sharpen why because one was blurred \nthe other has to be sharpened what is happening here \nstudent refer time eleventwentynine \nit is subtracting the neighbor\u2019s right so you are taking five times the current pixel and \nsubtracting the neighbors from it right so if the neighbors are similar those would get \nsubtracted and this would stand out really right does that make sense  \nthis will result in a but this in my on my laptop  this looks like a sharpened image i do \nnot know why it is looking like this here ok it is a sharpened image just trust me you \ncan so actually are common right so people who have used adobe or any of these photo \nshopping software\u2019s right so you have  this click button and where you say take an \nimage sharpen and blur it so this is exactly what the tool is doing in the background it \nis applying this convolution operation throughout the image \nso when you say blur it is basically placing that convol ution operation throughout the \nimage and computing the blurred image and same for sharpening and all these other \nspatial effects that you have most of them come out of some convolution operation ok \nrefer slide time twelvetwentythree \n \nso for example the next one what would this do  \nstudent refer time twelvetwentysix \nso i will give you a hint when will this result in a zero output \nstudent refer time twelvethirtytwo \nwhen all the neighbors are the same as this right so then when will it result in a nonzero \noutput \nstudent refer time twelvethirtyseven \nwhen there is a difference when there is a difference so looking at this image tell me \none place where you know that it will result in nonzero output \nstudent refer time twelvefortyfive \nall the boundaries right so this is basically an edge detector in the slides it appears \nproperly ok so this is basically an edge detector and you get the intuition that these \nboundaries whether neighbors are not the same as the current pixel you will not get a zero \nvalue in this case when all the neighbors are the same as the current pixel so you are \ntaking the sum of the eight neighbors and subtracting the current value eight times so that \nwould be zero right ok \nrefer slide time thirteeneleven \n \nso enough of examples so now we will see a working example of a twod convolution so \ni just want to drill this idea of what happens when you do a twod convolution \nrefer slide time thirteennineteen \n \nso what we are going to do is we have this three\n\uf0b4 three kernel and assume that everything here \nis a pixel ok everything here is a pixel so i am going to slide this three cross three kernel \nacross this filter now when i place the filter once on the image how many outputs do i \nget \nstudent one \none output so if i keep sliding it across the image i will keep getting one one pixel in the \noutput ok so what the resulting thing that i get is known as a feature map ok because it \nis the original input that we have taken for every pixel you have tried to approximate it \nor whatever filter weights you have applied and it necessar ily does not mean that you \nare taking an average it could be some weird average of your neighborhood right so \nyou have extracted some features from there  \nso for example in the edge detector case you could think of it that you have extracted \nthe feature that this pixel does not lie at a boundary right that is why you get the black \npixel do you get that you see this way of interpreting a convolution operation that you \nare trying to extract some features from that neighborhood \nso in this earlier example whenever you got a black you are basically extracting the \nfeature that this pixel does not l ay at a boundary is that ok fine so now you could get \none such feature map by using a single three cross three convolution operation ok if i use \nmultiple such convolution operations what would happen i will get multiple feature \nmaps ok so let us try to understand this what is the dimension of my original image m \ncross n into three why is it into three \nstudent rgb channels \nrgb channels ok rgb is what we will h ave right so we will have this three\n\uf0b4 m\n\uf0b4 n so \nwe will return back to this idea and from now this one image by using a single kernel so \nthis in fact in for this figure right i am assuming that the input i s one cross m cross and i \nam not assuming there are three channels although it is a colored image but just bear with \nme so it is a one cross m cross an image and when i apply a filter i get a one feature map if \ni apply k such filters i will get k feature maps  so one feature map could be for the \nblurring one one could be the sharpening one one could be the edge detector and so on \nright there are various such filters that you could apply \nrefer slide time fifteenthirtyfive \n \nnow in the oned case we slide a one dimensio nal filter over a one dimensional input in the \ntwod case we slide a two dimensional filter on a two dimensional input what would happen in \nthe threed case \nrefer slide time fifteenfortyfive \n \nso now we are going to this rgb images right so we will have three cross m cross n as \nthe input what would happen in the threeg threed case not threeg \nrefer slide time fifteenfiftyfive \n \nso what would a threed filter look like  \nstudent box \nlook like a cuboid a box basically and we will call it a volume why volume because it \nhas a width it has a hei ght and it will have a depth so this is what a three d filter would \nlook like i will assume that it is depth is the same as the depth of your input what is the \ndepth of your input in this case \nstudent three \nthree so i will assume that the depth of the filter is the same as a depth of the input and t he \nwidth and height could be three \n\uf0b4 three five \n\uf0b4 five seven \n\uf0b4 seven anything right so we will get into that in \nmore details later on so once again we slid e this volume across the entire image ok \nwhat is the output going to be twod or threed \nstudent twod \nwhy so when i was oned i was getting oned output when i was twod i was getting twod \noutput threed again twod output why because i have assumed that no not width  \nstudent refer time sixteenfortynine \nthe depth of the filter is the same as the depth of the input so now you just imagine this \nif you can suppose the filter was of depth two instead of three then i would slide it \nhorizontally first vertically and then across the depth  also so then what would be \noutput be in that case \nstudent three dimensional \nthree dimensional and it would have depth of two \nstudent two \neveryone gets that right but for this lecture i am always going to assume that the depth \nof the filter is equal to the depth of the input always right and that is how it is for all the \nconvolution neural networks that we will see the depth of the input is going to be equal \nto the depth of the filter the rather the depth  of the filter is going to be equal to the depth \nof t he input so whenever i apply a threed filter i am actually doing a twod convolution \nbecause i am moving only along the width and the height i am not moving along the \ndepth so the output is going to be twod so now can i have multiple such filters yes \neach filter will give me a twod output if i have k such filters i will have a \nstudent refer time seventeenfiftyfour \nk \n\uf0b4 twod output right k twod outputs fine"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.2 Relation between input size, output size and filter size.wav", "duration": 706.22, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 eleven \nrelation between input size output size and filter size \nok so now we will g o to the ne xt module where we will try t o learn the relationship \nbetween the input size the output size and the filter size ok  \nrefer slide time zerotwentyone \nso so far we have not said anything about the dimensions of the inputs i have just been \nvery vague that its m \n\uf0b4  n and also for the filter i have just said three \n\uf0b4  three five \n\uf0b4  five and so on \nand in fact i am not even told you what the dimensions of the outputs are except that i \nbe got some intuition that it seems that the output dimension is smaller than the input \ndimension right \nso let us look at these in more detail and see what do these different outputs look like \nwhy do i care about these things what do the inputs and the output sizes look like \nbecause these are your matrices that you will be dealing with this tell you these tell you \nhow many parameters you are going to have \nthese tell you what is the size of the memory that you need to load this entire network \ninto your memory and so on it so that is why this co mputation is very very important \nand i will have some more things to say about it towards the end of this lecture or some \nlecture ok  \nrefer slide time onesix \n \nso if you first define the following quantities so we have the input which has a width \nwone height hone and depth done so if you are looking at the original image then the depth \nis going to be three in most cases it is going to be rgb ok there is something known as \nthe stride s i will come back to it later on \nbut i am just defining it here or  others just introducing it here and then you have \nnumber of filters so i said that every filter that you apply you are going to get one \nfeature map which is two dimensional if you have k such filters you will get k feature \nmaps each of them is twod right  \nso we will have something known as number of filters k and then you will have \nsomething known as the spatial extent of these filters so that is the number three \n\uf0b4  three five \n\uf0b4  five \nwhich i have been saying so  that is known as the spatial extent i am going i am going \nto refer to it as f ok and we are going to always assume square filters \nso it is always going to be f\n\uf0b4  f is that fine ok and the depth of the filter is one more \nthing which i need to worry about but i have already said that i am going to assume that \nthe depth of the filter is the same as the depth of the input ok  \nnow the output is again a volume which is of size wtwo htwo and dtwo and my quest is to \nfind out how do i compute these wtwos htwos and dtwos that is what i want to figure out it is \nnot very difficult but i just want to do it properly so that is what the setup is right so \nwe have defined the sizes of everything on the input and the filters now you want to l ook \nat how do we get the output sizes ok \nrefer slide time twothirtynine \n \nso let us compute this for one example so this is my original image so i am looking at \na two dimensional input which i believe is seven \n\uf0b4  seven and i am applying a  three \n\uf0b4  three filter to it \nso every time i slide the filter i will get one pixel in the output and i got the entire \nfeature map \nnow it is obvious that the size of the output is less than the size of the input why is it so \nbecause there are certain pixels at which i cannot place the filter why you will go out \nof the boundary right so i cannot if this is my pixel of interest i cannot place my filter \nthere because then the filter will go outside the image and i do not know what the \naverage to come to how to compute the average right those values are undefined do you \nget that ok \nrefer slide time threetwentysix \n \nso in general for let me see for the three cross in fact this is true for all these pixels which \nhave been shaded or any of these  pixels i cannot place the filter because you will go \noutside the boundary so now for a three \n\uf0b4  three filter what is the reduction in the size of the \noutput compared to the input the width decreases by two and the height decreases by \nstudent two \ntwo right so can i be bold enough and say that the width and height decreases by not yet \nok  \nso let us see if we had a five \n\uf0b4  five kernels ok then which are the places at which i will not \nbe able to place the filter these  two shaded boxes and both these boxes i cannot place \nthe filter because the filter will go outside the image  \nrefer slide time fourthirteen \n \nso now can you tell me how many so in this case the size reduce is by how much \nthe width reduces by \nstudent refer time fourtwenty \nno the width reduces by four sorry and the height reduces by \nstudent four \nfour so now can i say that the width and height actually reduce by f minus one where f is \nthe size of the filter is that ok how many of you get this so you did not ge t this no \nyou did not get this ok so in the three \n\uf0b4  three case you see that that is one row and one \ncolumn on each side left and right which i cannot apply ok \nso let us focus on the columns so columns give me the width right so  when i have a \nthree \n\uf0b4  three filter there are two columns which get subtracted because these are the boundary \ncolumns when i have a five \n\uf0b4  five filter how many columns get subtracted two on the left \nhand side two on the right side is a total of four \nif i have a seven cross seven filter three on the left hand side three on the right hand side so total of six \nso you see the relation it is always f  one right so your new width and height is always \ngoing to be wone  f  one which is w one  f  one is that ok everyone gets this and same for \nthe height the width and height are going to be symmetric because the filter we have \nchosen to be symmetric it is f \n\uf0b4  f ok  \nrefer slide time fivetwentyeight \n \nnow but what if we want the output to be the same as the input what do we do \npadding ok you can use you know something known as padding so that means now i \nwill have a boundary of zeros so i am saying that this is my original image and outside it \nthere is a black border or a  white border i do not know whether zero stands for black or \nwhite it is embarrassing but \nstudent black \nblack ok so it is a black border outside the image ok and now i am going to take an \naverage that way \nnow this was the pixel earlier on which i co uld not place the filter but now i can \nartificially place on it assuming that it there is a black boundary around it so now what \nwould be the output size \nstudent refer time sixeight \nsame as the input so now can you tell me so i have wone i have f and now i have \nsomething known as p also ok so i know that w output rather wtwo is the output is one \nnow when i add the padding what would the formula be two p is that fine everyone gets \nthat ok  \nrefer slide time sixthirtythree \n \nso now if i have a five cross five f ilter and if i want the output size to be the same as the \ninput size what is the padding that i should use \nstudent p \np is equal to two right that is clear from the example that we saw that there were two \nshaded columns and rows which were problematic so  i need to do a padding of two and \nthen if i substitute in this formula you can just see right so five  four  one right so you will \nget back wone is that fine is that ok how many if you get this how the formula works \nwith the padding how many of you do not ge t this please raise your hands you do not \nget this ok  \nso remember in the three \n\uf0b4  three case there was one column on the left hand right which was a \nproblem so when i say a padding of one i add one column to the left one column to t he \nright one column to the bottom and one column to the top \nand that is exactly the problematic region in the three \n\uf0b4  three case right so that means this was \nmy original formula ok now the new width is going to be plus two times the pa dding \nwhich i am going to use because i have used one padding here and one padding here \nright now in the three \n\uf0b4  three case that is ok \nnow in the five \n\uf0b4  five case how many columns are problematic two so that means  i have use \na padding of two when i say a padding of two i add two on all the sides so now again if you \nsubstitute in this formula so you would have wone  five  one  four so that will give you back \nwone right so that is how it is right  \nrefer slide time eighteight \n \nnow the question is you have if you have taken care of filter size and padding now the \nother thing that we need to look at is stride ok so the stride defines the interval at which \nthe filter is applied now what do i mean by that \nso remember that stride is basically a step right the same definition as it is applies to \nwalking rate so right now what we were doing is we placed the filter at as this pixel at \nthe center then this pixel has the center and then this pixel has the center instead of t hat \ni could take a that means my stride is one i am taking one step at a time so if i do a \nstride of two what would happen \nstudent refer time eightfortyeight \ni will apply two alternate pixels right so this is how i will move so then what would \nhappen to my output size if my stride is two \nstudent refer time eightfiftyseven \nwhat would happen \nstudent refer time eightfiftynine \nit will become half ok so what would the formula be then so so far my formula was \nnow if i have a stride of two what would the formula be \nstudent refer time ninetwelve \nthey divide the whole thing by two \nstudent refer time ninesixteen \nby s right if it was s was three then this would have become one third roughly right if s \nwas four this would have become onefourth so what should i divide by \nstudent s \ns so i should divide the whole thing by s \nstudent refer time ninethirtytwo \ns ok  \nrefer slide time ninethirtysix \n \nso it turns out that is not exactly that but you get the intuition and you can work out the \nformula so you do not divide this by s and you w ill figure it out it is easy to see \nbecause of some ceiling and flooring and things like that so you can go back and check \nthat out and basically you could just think of this that this was your original weight in \nthe absence of stride or rather than the stride was one \nso now if you are going to take longer strides you have two account for that if you take \na stride of two stride of two your width is going to become half you should take a stride of four \nyour width is going to become onefourth is that fine do not worry about this additional \nplus point you can go home and figure it out \nrefer slide time tensix \n \nfinally coming to the depth of the output what would the depth of the output be so let \nme just see right now all our formulas were wtwo htwo in the  presence of filter padding and \nsize a stride sorry but we never had a formula for  dtwo so that is what i am asking \nnow what is the depth of the output same as the every filter is going to give you one \ntwo dimensional output if you have k filters \nstudent k \nyou will get k two dimensional outputs that means the depth of your output is k right  \nrefer slide time tenthirtyeight \n \nso the depth is very simple it is just equal to k the number of filters that you have so i \nwant you guys to note down these three formulas \nrefer slide time tenfortyeight \n \nnow let us do some exercises so this is my original input which is two hundred and twentyseven \n\uf0b4  two hundred and twentyseven \n\uf0b4  three i \nhave decided to apply eleven cross eleven filters and i am not going to tell you th e depth of the \nfilter because it is going to be the same as the input ok \nand i have ninetysix such filters i have decided to use a stride of four there is no padding can you \ntell me what is the output volume going to look like what are the dimensions of the \noutput volume ok so dtwo is simple what is dtwo ninetysix what is wtwo \nstudent fiftyfive \nfiftyfive htwo ok you guys have the last class fine so similarly you can do it for so actually \nthe computation which i had that this was just not some random computation this is \nactually the first convolution layer from alexnet right so one of the popular \narchitectures that we are going to cover later on right \nso this is what aalexnet does at its input it takes the rgb input and gives you a volume \nof this slice this side and then ther e is something else with this volume right so we will \nsee that later on there is one more exercise you can do it later on i do not want to do it \nnow ok"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.3 Convolutional Neural Networks.wav", "duration": 977.17, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 eleven \nconvolutional neural networks \nok s o now we will go to the nex t module  thi s is for the camera a nd thi s is on \nconvolutional neural networks \nrefer slide time zeroseventeen \nso far we have done what we have just talked about a convolution operation you just \ntaken some input boxes and converted them to output boxes what does this anything of \nthis have to do with neural netw orks i keep saying that is a course on neural networks \nright so everything has to link to that so what is the connection \nso we will try to understand this by taking the example of image classification and i will \nuse the same trick to get everyone\u2019s attention so the next few slides are going to tell \nyou the difference between machine learning and deep learning ok so now everyone \nwill pay attention  \nrefer slide time zerofortynine \n \nso this is the task you have give given an image and you want to class ify it into one of k \ncategories and i am considering four categories here car bus monument flower ok what \nis the simplest thing that you can do suppose this is a twenty cross twenty image you know the \nsimplest thing  \nstudent sir \ngiven on the slide you will j ust take this as a four hundred dimensional input feature vector right \nand you will treat it as a four class classification problem train some multiclass svm or \nanything on that right so you have a simple input so you are given some one million \nimages all of these are four hundred dimensional and they come from one two three or four these are the four \nclasses which is car bus monument and so on  \n refer slide time onefortythree \n \nso you can just treat this as an input feature vector and do your classification right that \nis the simplest thing that you do or else what you could do is you could do some kind of \nfeature engineering right you could say that actually this entire blue sky is not really \nhelping me in deciding anything these entire green lawns and all this is not helping if \nmonument car bus and flower are the classes what i care about is the shapes i do not \ncare about the details inside the shapes i am not trying to decide whether the car is of a \nblue color or what model the car is and so on right all i want to see is tha t this particular \nshape of a car is present or not \nnow what kind of filter gives you the shape of the image \nstudent refer time twosixteen \nedge detector right so i could use edge detector so now this is something that i have \nused based on my domain kn owledge that for these four classes actually just detecting the \nshape is important so i will ignore everything else so there is a lot of details there \nright so i have actually sparcified my entire input i have just looking at the edges in \nthe input and now this is a better refined feature as compared to the earlier feature how \nmany of you agree that this is a more refined feature representation right  \nbut this was handcrafted i actually hand coded the edge detector kernel because i knew \nthat it is eight at the center and minus one everywhere else right that is how i thought of it that \nthat is what an edge detector is or at least i read about it somewhere right so that is how \nyou would do it so this is feature engineering  \nso what is this this is how you do machine learning right you take an input you do \nsome feature engineering and then you train a classifier on top of that but now you could \nbecome even more creative with the feature engineering and that is what the computer \nvision community was  doing largely before two thousand and twelve come up with different ways of \ncapturing better and better features from images so too popular in features from that era \nand that is i am just talking about two thousand and twelve not some like five hundred years back but from that era \nwas sift and hog features which actually tell you how do the gradients of these pixels \nchange across the image right  \nso this is again try to capture something like how what is the variation in the image \nfrom pixel to pixel right so that is the essence that how is y ou do not care about these \nentire blue patterns because they are just telling you sky it is redundant right if you have \nseen some ten pixels or twenty pixels which has sky you know that a large part of it is going \nto be sky \nso these try to capture some abs tractions from the image and these are better than the \nedge detectors and these features were extremely popular so what you would do is you \ntake your original input this is a deterministic algorithm you apply the hog algorithm \nor the sift algorithm and  it gives you a transformed representation for the image and \nyou can use this transform representation to do classification and a lot of work prior to \ntwo thousand and twelve two thousand and eleven show that these features work extremely well across a wide variety of across \na wide variety of image tasks ok  \nso again what was happening here this was feature engineering because someone \nrealized that what i care about is this gradation in the input images and i can capture this \nby this nice algorithm called sift or hog of course someone ca me up with that \nalgorithm but it is still kind of feature engineering right  \nso this is how the learning is to happen is you are given some input you do a static \nfeature extraction no learning so feature extraction is deterministic you take the input  \npass it through one of these algorithms either the edge detector or the blur detector or \nsift or hog and you get some representations for the input and the only learning that \nhappens is on top of this transformed input so you now have a transformed in put and \non top of that you are going to train a classifier and you are going to learn the weights of \nthe classifier so the only thing that you learn is the weights of the classifier \nrefer slide time fivetwentyfive \n \nso that is equivalent to learning only the  soft max layer in case of a feed forward \nnetwork that is the output layer right \nnow instead of using these so this is the question instead of using these handcrafted \nkernels or features such as edge detectors or blur detectors or what not can we lear n \nmeaningful kernels in addition to learning the weights of the classifier do you get this \nquestion at least whether the answer or not but you get the question so what i am \nasking is that why should i hand code this edge detector ok \nrefer slide time fivefiftyfive \n \nwhy not have after what is the edge detector it is like a three cross three matrix right and i have \nseen tons of such matrices in my feed forward neural networks i have dealt with very \nlarge matrices which were called parameters of the network  \nso why not have a three cross three or a five cross five or whatever dimensional matrix and try to \nlearn what should be the right values in this matrix instead of hand coding the edge \ndetector matrix do you get the idea how to do that as still far but at least do you get \nthe idea that is what i am we are trying to do ok \nso now instead of just learning the weights of the classifier we also want to learn the \nweights of the kernels that means instead of using handcrafted features i am now going \nto \nstudent refer time sixthirtyseven \nlearn the features so that is the difference between deep learning and machine \nlearning so you had handcrafted features there and now you are going to learn the \nrepresentations also by treating them as additional parameters in your network how you \nwill do that we will see and it is very simple given that you understand how to train \nfeed forward networks \nrefer slide time sevenone \n \nbut then why the stop there why just have one feature representation for the input can \ni learn multiple such kernels right i could have one three cross three matrix which learned this \nfirst representation another three cross matrix which learned this another representation and \nyet another three \n\uf0b4  three or five \n\uf0b4  five or seven \n\uf0b4  seven matrix which learns this different representation so \ninstead of learning one static representation from the input i could learn multiple \nrepresentations from the input \n refer slide time seventwentynine \n \nin fact why not why just stop t here what is the next thing that i am going to try to do \nmultiple layers of features right so that means at the first layer i learned this \nrepresentation now i could take this and try to learn an even more abstract representation  \nand then keep doing this to make it deeper and deeper do you get this ok \nso at every stage now i have these parameters which are helping me learn the \nrepresentation of the input i am learning multiple representations at every layer and then \nhaving multiple layers of the se representations right and everything is learnable end to \nend ok so you get the difference between deep learning machine learning now there is \nno handcrafting of features you are learning the feature representation i know i \nunderstand there is some confusion about how you would do this \nrefer slide time eightfifteen \n \nbut we will get to that just trust me on that that you will be able to figure out how to do \nthis ok \nand all of this we have some weight matrices here we have some weight matrices here \nthese are the layer one weight matrices the other layer two weight matrices and these are the \noutput layer matrices and you see this layer wise arrangement of these weight matrices \nand they are very comfortable with this because we have done feed forward neu ral \nnetworks where we had these multiple layers and we knew how to back propagate from \nthe last layer to the first layer  \nnow what i am trying to say is that i would like to adjust these weights of filters in such \na way that my classification loss is m inimized and what is the loss function that i am \ngoing to use here \nstudent refer time eightfiftyone \ncross entropy because this is a multi class classification problem ok  \n refer slide time eightfiftynine \n \nso just hang on with this intuition and we will make it more clear fine \nso such a network which has these multiple convolution learned convolution operations \nat every layer and then multiple such layers is known as convolutional neural network \nrefer slide time ninenine \n \n ok fine so get this idea that we  need to learn kernel filters by just treating them as \nparameters of the classification model ok but how is this different from a regular feed \nforward neural network you could have taken a regular feed forward neural network \nand i will show it to you on the next slide and what is the difference between that and a \nconvolution operation \nrefer slide time ninetwentynine \n \nso if you understand that then you would be done for this lecture yeah \nso we have an input so let us say now i will take back the eminis t case where you are \ngiven an input as an image and these are digit inputs and you want to classify them into one \nof ten inputs and i am going to assume that my input is four cross four that means i have sixteen \npixels ok  \nso the simplest thing that i could have done or the feed forward neural network way of \ndoing this is that i would just flatten out this image i will get sixteen inputs i need ten \noutputs at the output layer so i have an output layer which will have one of these ten \nclasses and then i add as many l ayers that i want in between ok this is what a feed \nforward neural network would look like and if i consider any one neuron in the first \nlayer it takes inputs from all the sixteen inputs right that is how a feed forward neural \nnetwork is you have these extr emely dense connections where every output depends on \nevery input at every layer ok  \n refer slide time tenthirtyone \n \nnow so this is the same story which i have said now let us look at what a \nconvolutional neural network looks like so again you have these sixteen inputs i am using a \ntwo cross two convolution ok now if i use a two cross two convolution if i place it here then i \nam using pixels one two five and six and computing one value so you see the difference \nbetween this and a feed forward neural network in a feed forward neural network h eleven \nwould have depended on \nstudent refer time eleventhree \nsixteen values sixteen inputs and a convolutional neural network it is depending on \nstudent refer time elevennine \nonly four only four neighbors ok and similarly htwelve i am using a stride of two by the way right \nso i am not placing the filter here i am just skipping one step h twelve would depend on \npixels three four seven eight  ok and so on right \nso one thing is clear that as opposed to a feed forward neural network you have sparser \nconnections here is that clear and why do we have sparse connections because we are \nexploiting our knowledge about images that in an image you do not really care about the \ninteractions between on between a pixel at the leftmost left top most corner and the right \nbottom corner right so there is sky here there is ocean here or there are trees here you \nwould want to capture the neighborhood around that pixel not really capture it with the \nentire image that is why you do not want all of these sixteen inputs to contribute you only \nwant a small neighborhood to contribute do you get that intuition ok  \n refer slide time twelvethirteen \n \nso this is the first property of a convolutional neural network that it has sparse \nconnectivity ok but its sparse connectivity really good i ju st made a case for that now \ni am going to counter argue right is it really good that you have these sparse \nconnections because you are losing out information right you are using out interactions \nbetween certain pixels so why can why is that good \nstudent refer time twelvetwentynine \ni am hearing a lot of interesting answers but remember that you are always going to \nhave multiple layers ok so consider these two pixels in the first layer these two pixels did \nnot interact because h two only dependent on these three and hfour only dependent on these three \nthere is no a there is no unit here which depends on both xone and xfive is that obvious \nbecause i am just using a window of size three  \nbut now once i go to the next layer once i go to g three gthree depends on htwo hthree hfour here which \nin turn depends on x one xtwo xthree xfour xfive right so even though at this layer x one and xfive are not \ninteracting with each other as you go deeper these interactions become obvious do you \nget that right so that is why you will always use a deep convolutional neural  network \nwhere all the pixels get to interact at a deeper layer but at the more image it layers you \njust want to capture the interactions between a neighborhood  \nso it is like you take this neighborhood find out something then take neighborhoods of \nneighborhoods and then try to find out something at the next layer and keep continuing \nin this layer how many of you get this right ok so this is what sparse connectivity \nlooks like \nrefer slide time thirteenfortythree \n \n refer slide time thirteenfortyseven \n \nanother chara cteristic of cnns is something known as weight sharing so let us see \nwhat it is so remember i had considered this two cross two kernel and i was placing it at \nthese four pixels which is pixels one two five and six and i was pacing another kernel at these four \npixels which is pixels eleven twelve fifteen and sixteen right these four pixels and i have used different \ncolors for them indicating that these filters are different so they are both two cross two \nfilters but i am assuming at the values inside them are different does this have t o be the \ncase just think what a filter is trying to do \nstudent refer time fourteentwentyseven \nit is striding across the entire image at every locatio n i want to do the same operation \nremember when we are doing blurring or edge detection or sharpening i had the  same \nfilter which i was applying at every location so i want to see what is the effect of this \nfilter throughout the image so i do not really want to change this filter that means \nthese four weights would be the same as \nstudent pink weights \nthe pin k weights how many of you get this so this is a question do we want the \nkernel weights to be different for different portions of the image so imagine that we are \ntrying to learn a kernel that detects edges so the same kernel configuration is requir ed \nthroughout the image because that is the kernel configuration which will detect an edge \nrefer slide time fifteenseventeen \n \nso you want the same kernel to be there everywhere so we are going to share these \nweights they should not be these pink and orange  weights we will just have the same \nweights everywhere ok so this is known as weight sharing  \nso now this is something ridiculous if you think about it now how many weights do i \nhave in layer one \nstudent refer time fifteenthirtythree \nfour weights that is all that looks too less right it would lead to \nstudent refer time fifteenthirtynine \ndash fitting \nstudent refer time fifteenfortytwo \nunder fitting because we have very few parameters so how do i deal with the situation \nstudent refer time fifteenfortyfive \nyou will have multiple kernels right so you will have another kernel which takes \nsomething else you will have one more kernel which takes something else and you can \nhave as many such kernels right so the more the number of kernels will have you will \nhave that many into four as the number of parameters and that many outputs at layer one \nhow many if you get this ok good  \nso these are the two important characteristics of convolutional neural networks one is \nsparse connections and the other is weight sharing ok \nrefer slide time sixteenseventeen \n \n so so far we have focused only on the convolution operation now let us see what does \na full convolutional neural network look like or maybe i will do this next time i think \nthis is"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.3 (Part-2) Convolutional Neural Networks (Contd.).wav", "duration": 1018.69, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 eleven \nconvolutional neural networks contd \n refer slide time zeroeleven \n refer slide time zeroseventeen \nso we will start fro m where we left yesterday so this is what we had seen yesterday \nwe saw what\u2019s the difference between a convolutional neural network and a feed forward \nneural network and we focused on two main properties one is sparse connectivity and \nthe other was weight sharing that is about it and then we saw that this representation of \ni mean we saw this diagram about how to you could have multiple kernels and each \nkernel would apply across the entire image and the weights would be shared for that \nkernel  \nrefer slide time zerofortythree \n \nso far we have only focused on the convolution operation and even when you have seen \nthe neural network or the convolutional neural network we have only seen the \nconvolution layers  right so there is something more in a typical co nvolutional neural \nnetwork and that is what i was about to start yesterday so we just continue from there  \nrefer slide time zerofiftynine \n \nso this is what a full convolutional neural network looks like so ignore these things for \nnow all these parameters etcetera as a just ignore for them i will just walk you through \nwhat is important in this diagram right  \nso your input is an image and the tasks that you are dealing with here is digit \nrecognition or handwritten digit recognition right and what you see here is that you have \ntaken an input which is a two dimensional input and then the next layer you actually see one \ntwo three four five six outputs so what does that mean \nstudent refer time onethirtythree \nyou have use six filters apply them throughout the filter throughout the image each filter \ngave you one feature map and so in this layer you have six such feature maps so the \noriginal two dimensional input has now become six two dimensional outputs ok after that \nthere is something known as a pooling layer we will see what a pooling layer is in detail  \nnow what i want you to understand is lets assume that what the pooling layer does is it \ndoes some kind of a shrinking it takes the original output and shrinks it how it shrinks \nit we will see in a while but let us s ee what happens after that so now you have one two \nthree four five six as your input so now you have this is your input this volume is now your \ninput i call it a volume because it has depth height and width and now you are again \ngoing to apply convolutions to that and what do you see how many outputs do we have \nnow we have sixteen outputs  right so what does that mean \nstudent refer time twotwentyseven \ni took this threed input applied sixteen threed filters on it each threed filter give me one feature map one \ntwod feature map why one twod feature map \nstudent refer time twothirtynine \nbecause we are doing a twod convolution we are taking a threed filter but we are doing a twod \nconvolution  right and then after that again this becomes my input and then do a max \npooling on top of that and then so mething else happens after that so there as which we \nwill come to later \nso right now i just want to say that there is this input then you come out will come with \nsome output after applying convolutions now this becomes the input for your next stage \nwhere you have done pooling now the output of pooling becomes the input for the next \nstage so you will take this as the input apply some convolution on that get some output \nand continue in this way right and we will come back to what these are  \n refer slide time threenineteen \n \n refer slide time threetwentythree \n \nso now let us dive deeper into what is pooling what does the pooling layer actually \ndo so here is your input again it is a volume and now when i say input you should not \njust think of the input as t his remember that all of these can be inputs right so now at \nevery stage once you have got an output for the next stage that becomes input that is \ntypically how it was even in the feed forward neural network right once you compute a \nhidden representation that is the input to the next layer  \nso i have some input at one of these layers either the input layer or any of the \nintermediate layers and i apply a filter on that and that filter gives me some twod output it \ngives me one feature map and let us say this is what the feature map looks like  \nnow what does the pooling operation do so i would apply two \n\uf0b4  two pooling with a stride \noff two so let us see what that will do that means i look at this two \n\uf0b4  two region i will pick \nup the max value from there that is why this is max pooling ok so the max value is eight \ni will just keep that then i am going to do a stride of two that means i am not going to \nplace it on this block i am just going to shift to the next block again i will take the max \nfrom there which is four right and i continue this and i get this is the output so you see \nwhy the shrinkage happens because i am taking a two \n\uf0b4  two area and i am shrinking it by a \npicking up only one value from there right  \nso it actually kind of half the width and half the breadth so total of one by four reduction is \nwhat you get but you could also use a filter with stride off one so this is what it would \nlook like so you will place it here take the max value then the max value then the max \nvalue and so on right so in that case you will get a lesser reduction and instead of max \npooling you could also do average pooling that means instead of taking the max of \nthese four you could take the  average of these four is it fine so is the pooling operation \nclear and how it results in the reduction of the size of your input ok \nrefer slide time fivefifteen \n \nso now what we will do is now that we have some idea of what a full convolutional \nneural ne twork looks like so it looks like alternating convolution and max pooling \noperations we know what a convolution operation looks like in particular we know that \na threed filter applied to a threed input results in a twod output because we are not applying the \nconvolution along the depth we just applying the convolution along the width and the \nheight right so that is what we know so far \nrefer slide time fivefortyseven \n \nand based on this knowledge now we are going to see some success stories of \nconvolutional neural network right \nso we will start with the first one which is lenet five so this was already the fifth \nversion this was around ninetyseven or ninetyeight or something and i had mentioned this when we were \ndoing the history lecture  \nso this is the input now you have decided to apply six filters you have said that the stride \nis going to be one that means you are going to place at every location the spatial extent is \ngoing to be five and the padding is going to be zero  now the question that i have for you is \nhow many parameters does this convolution layer have what are the parameters here \nstudent the weights \nthe \nstudent refer time sixtwentyfive \nthe filters the weights in the filters how many filters do have \nstudent six \nsix how many ways does each filter have \nstudent twentyfive \ntwenty \nstudent five \ntwentyfive right five cross five is a twentyfive so the total number of parameters is one hundred and fifty now i want you to \nappreciate something here so the input was actually thirtytwo cross thirtytwo which i believe one thousand and twentyfour \nand the output was twentyeight cross twentyeight right that is what the outpu t you got that is i guess seven hundred and eightyfour \nyeah so in a feed forward neural network if you had x belonging to r one thousand and twentyfour and your h \nbelonging to r seven hundred and eightyfour how many parameters would you need one thousand and twentyfour \n\uf0b4  seven hundred and eightyfour how many \nparameters do you have here  \nstudent one hundred and fifty \none hundred and fifty much much smaller right this is because of \nstudent refer time seventwentyone \nboth sparse connectivity as well as weight sharing right so now you appreciate the \ndifference between the twook and thats one of the reasons that even before this i s for \nexample from ninetyseven ninetyeight one thousand nine hundred and ninetyseven one thousand nine hundred and ninetyeight right so even before the deep learning wave of two thousand and six \nor the revival after two thousand and six right convolutional neural networks must still being trained for \neven deep networks four to five layers because they had much fewer parameter s and that is \nwhy it was relatively easier to train them as compared to a very dense feed forward \nneural network \nrefer slide time sevenfiftyfive \n \nso i want you to appreciate that fact now after this i have the pooling layer when i am \ngoing to use a stride of one and f equal to two that means i am going to pick up ok so \nnow i am going to use a max pooling layer where i am decided to use a stride of one and \nthe max pooling will happen in the region of two \n\uf0b4  two and again k six because there are just \nsix filters so max pooling happens on every feature map independently it does not \nhappen across the depth that means i am not going to pick the max along these six layers \ni am going to pick the max along each of these feature maps so it is  a per feature map \noperation ok \nand this since it is a two \n\uf0b4  two filter it will result in a size reduction and from twentyeight \n\uf0b4  twentyeight i will \nget to fourteen \n\uf0b4  fourteen how many parameters is the max pooling layer have \nstudent refer time eightfortytwo \nwow good there is no parameters in the max pooling layer because you are not having \nany weight matrices just taking the input and applying a simple max operation on that \nthere is no w transpose x or any  kind of a transformation happening there now this \nbecomes your input what\u2019s the size of this volume \nstudent thirtytwo \nfourteen cross fourteen cross \nstudent six \nsix so all the filters that i am going to use from now on what is the depth of those filters \ngoing to be \nstudent refer time nineten \nwhat is the depth going to be  \nstudent six  \ni want to everyone to answer \nstudent six \nsix right because we are always going to assume that the depth is equal depth of the filter \nis equal to the depth of the input  \nrefer slide time ninenineteen \n \nnow here they decided to use sixteen filters and by the way you did hopefully notice that \nthis twentyeight how did you get it from which formula \nstudent w n minus refer time ninethirtyfour \nwn minus f plus twop plus one right so that is the formula  \nso now we have a fourteen cross fourteen input and you have sixteen filters so what is the depth of the \noutput volume going to be  \nstudent refer time ninefortysix \nwhat is the depth of the output volume going to be \nstudent sixteen \nsixteen right and a ok so you have sixteen and you have a spatial extent of five \n\uf0b4  five just a minute \nspatial extent of five \n\uf0b4  five how many parameters does this layer have i want everyone to \nsay it \nstudent four hundred \n refer slide time teneleven \n \nfour hundred ok fine so that is \nstudent refer time tenten \nthere are sixteen of these each is five \n\uf0b4  five \nstudent refer time tenthirteen \noh into six into d ok ok good then we made a mistake here also no there the depth was one \nare you do you get it how you got two thousand four hundred ri ght we forgot about the depth so each of \nthese filters is five \n\uf0b4  five \n\uf0b4  depth right what is the depth six the same as the input right so \neach of these filters is twentyfive \n\uf0b4  six which is one hundred and fifty \n\uf0b4  sixteen yeah fine is that ok did i confuse you \nor everyone back on track pooling layer edge should have been two because of that half \nreduction using \nstudent refer time tenfortynine \nyeah maybe can we just check this i think it should be two ok yeah there was a \nquestion \nstudent refer time elevenzero  \nstudent refer time eleventwo \ngo from the pooling layer to the next layer from here to the next layer ok \nso now what is the depth of your input volume six and what is the width  and height fourteen \ncross fourteen so now every filter that i going to apply at the next layer is going to have a \ndepth of six so i have decided to apply sixteen such filters so what is the depth of this layer \ngoing to be the depth of the output is equal to the numb er of filters so the depth is \ngoing to be sixteen and all my filters are five \n\uf0b4  five but what we forgot is that when i say the \nfilter is five \n\uf0b4  five it is actually five \n\uf0b4  five \n\uf0b4  six because the depth is also there right so it is \nwidth into height into depth so that is this number of parameters in each of my filters \nright and that is one hundred and fifty and i have sixteen such filters so that gives me a total of two thousand four hundred \nparameters is it fine ok ok \nso now we have a volume of size sixteen cross ten cross ten now i am going to do max \npooling on that maybe again this should be two there was the same doubt you had fine \nso it will result in a reduction in the output and now what is the volume what  is the \nsize of this volume five \n\uf0b4  five \n\uf0b4  sixteen and the parameters is zero max pooling layer does not have \nany pooling  \nnow after this what we have is something known as the fully connected layer ok so \nnow as i said the size of this volume is sixteen \n\uf0b4  five \n\uf0b4  five its arranged in these feature maps \nbut i can always flatten it to get one single vector do you get that so from these sixteen \nfeature maps each of five \n\uf0b4  five size i can flatten it out and get the single vector of size four hundred \ndo you get that ok so thats what i do in the fully connected layer \nso now i am going to flatten this treated as a single vector and then fully connect it to \nthe next layer what do i mean by fully connected dense connections no more sparse \nconnection so now we have a feed forward network from this point of view so you \nhave four hundred and that connects to a layer of size one hundred and twenty so what are the number of parameters  \nstudent four hundred into one hundred and twenty \n refer slide time thirteenthirteen \n \nfour hundred into one hundred and twenty plus we will have one hundred and twenty biases so that is what this number is fine so this \nis one fully connected layer of size one hundred and twenty after that i have another fully connected layer of \nsize eightyfour so the numbe r of parameters would be one hundred and twenty into eightyfour plus eightyfour and after that i have \nthe output layer so the output layer this was twentysix or whats the output \nstudent refer time thirteenthirtyone \noh but the this is digit this is alphabet recognition right ok so probably they ha ve \ndone the computation using ten but it should have been using twentysix as the output layer \nbecause you want to predict one of the twentysix alphabets so you can assume this is twentysix so it \nwould be eightyfour \n\uf0b4  twentysix  twentysix right that is the size of the output layer right  \nnow do you observe something immediately is a something very striking immediately \nin terms of the number of parameters  \nstudent refer time fourteenzero \nthe fully connected layers clearly dominated right here we were dealing of orde r two thousand four hundred \nand max and here we just start with fortyeight thousand itself right so just keep this in mind that the \nfully connected layers have the largest number of parameters that you have and we will \ntry to come back to this and see if we can solve this problem ok \nso now when you see a convolutional neural network you should be able to reason \nabout the following things at each layer what is the size of the input volume what is \nthe size of the output volume what are the number of filters being used and what are  \nthe number of parameters in that layer right if you can reason these things and you \nhave really understood what is actually happened and unless you can reason these \nthings i do not see how you can efficiently code it up right so you should be able t o \nknow that this is the size of the input this is the size of the output and so on and i guess \nall of us are comfortable with others  \nrefer slide time fourteenfiftyone \n \nso now how do we train a convolutional neural network what is the answer your \nnodding t hat means you do not know or you know it is too trivial to even ask this \nquestion how will you train it \nstudent refer time fifteenzero \nbut how do you back propagate through a convolution operation that is a very nasty \nlooking operation  \nrefer slide time fifteenseven \n \na cnn can be implemented as a feed forward neural network with what being the \ndifference \nstudent refer time fifteenfifteen \nonly some of these weights would be active all the gray weights will not exist only the \ncolored weights will exist right  now can you back propagate to this network have you \nseen something similar before \nstudent dropout \ndropout right so if you could do that you can do this also so now if you take this view \nof a convolutional neural network where so this is just to giv e you an intuition or make \nyou feel confident that once you know the backpropagation algorithm there is no \nnothing much different from training a convolutional neural network operation ok \nso everyone is fine with that how many of you agree with that  that you can actually \ntrain using the same algorithm with some smart coding required to make sure that these \nweights are not active and so on but in practice of course you will not do this in \npractice you will define the convolution operation you wil l also define the derivative of \nthe convolution operation and then use that right because that would be much more \nefficient this is very inefficient right because you are assuming that there is a fully \nconnected network and then some things do not exist there so that is not thats the whole \npoint i mean you wanted to avoid such dense connections right  \nbut in principle you could have just used this and trained the convolutional neural \nnetwork in practice you do not need to worry because you just ne ed to define your \nforward convolution operations and people like google and all who release tensor flow \ntorch and all will do the hard work of doing the back propagation for you right so you \nnever have to write back propagation in your life apart from what you have already \nwritten in the assignment right that is why i make you go through that torture once  \nso now afterwards afterwards whenever you use any of these platforms or pytorch or \ntorch or tensor flow the back propagation comes for free you ju st need to write the \nforward progression that means you just need to write convolution operations and you \ndont need to worry about how the derivatives will be computed but i what i want you to \nunderstand is that conceptually it is the same you can still use or in fact you still use the \nsame back propagation algorithm to train a convolutional neural network also everyone \nis fine with this pk ok so now we know how to trained a convolutional neural \nnetwork also"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.4 (Par-2) CNNs (success stories on ImageNet) (Contd.).wav", "duration": 136.74, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 eleven \ncnns success stories on imagenet \nso now we will g o to the nex t module  we will talk about so me succe ss stories on \nimagenet rig ht s o thi s is the challenge whic h actually made convolutiona l neural \nnetworks very famous back in two thousand and twelve  \nrefer slide time zerotwentythree \nso they are going to look at some algorithms in fact two more hopefully today \nrefer slide time zerotwentynine \n \nso this is the story right so there is this challenge or competition called imagenet \nlarge scale visual recognition competition right that is what ilsvrc stands for and \nthis was a data set created which had one thousand categories it actually has ten thousand categor ies \nbut in the competition we use only thousand of those categories yes  \nso one thousand plus one thousand i think the roughly the data set size is one million and so that is what \nwas used for training a classifier and i am talking about two thousand and ten the pre deep era right i \nmean so of course deep era networks existed at that time but the participants and these \nchallenges and that time were relying on the classical machine learning approaches so \nwhat was that approach take the image  \nstudent refer time onesixteen feature   \nextract features which features \nstudent refer time onetwenty \npredominantly \nstudent sift and hawk \nsift and hawk features were the predominant features at that time and then you train a \nclassifier on top of that and then you use things like on symbols or better handcrafted \nfeatures and things like that certain more tricks on top of that so that with that on this \ndata in two thousand and ten the error was twentyeighttwo percent that means if i give you a test set of one thousand \nimages you will make two hundred and eightytwo errors on that right  that is what this means then in eleven there \nwas still some progress this was again pre deep era and there was this error came down \nto twentyfiveeight and then in two thousand and twelve there was this alexnet which was a deep convolutional \nneural network applied to the task of imag e classification and it gave a dramatic \nreduction right from twentyfiveeight to sixteenfour  \nand was i think absolute in absolute terms eight to nine eight to nine percentage better than the best \nsystem in that competition that year ok so that was in two thousand and twelve in two thousand and thirteen there was further  \nimprovement on a different architecture for doing this and that give a further error \nreduction of elevenseven then in two thousand and fourteen there was vgg net so these are all three that we are going \nto see today which give a further error reduction of seventhree then google decided t o join \nthe party and they make it sixseven and as i have said before then afterwards microsoft got \ncrazy and they brought it out in threefiftyseven and this is when we started making claims that a \nconvolutional neural network has become better at this task than humans right  \nbecause if you show these one thousand images to a human even a human is bound to make a \nthreefive percent more than threefive percent error that means because some of these images would \nbe blurred so i would not be very sure whether this is a bulldog or a differe nt type of \ndog or something like that right so i even a human cannot really recognize it correctly \nand that is the whole hype around how convolutional neural networks have beating \nhuman level performance on this particular task right  \nand let us see so  this was all the shallow pre deep era the first architecture was eight \nlayers and i think this was called a varied no this probably not this yeah the second \narchitecture was also eight layers then we had nineteen then twentytwo and then one hundred and fiftytwo right thats how the \nprogress has happened so these are all the architectures that we are going to look at \ntoday or at least some of them today and the rest maybe tomorrow \n ok so we will start with alexnet and i am going to tell you the exact architecture of \nalexnet what was refused what did it actually use  \nrefer slide time fourtwo \n \nso the input was an rgb image so it had a depth of three and it was two hundred and twentyseven \n\uf0b4  two hundred and twentyseven that is \nwhat the data set input was all the images in the data set were to two hundred and twentyseven \n\uf0b4  two hundred and twentyseven cross three so \nthe first thing that they did was they decided to use ninetysix filters can you read that \nanyways i will say it outright  \nso they resided to use ninetysix filters with a spatial extent of eleven cross eleven a size of four and \npadding of zero ok so the moment you see size a stride of four what do you know is going \nto happen there is going to be some shrinkage roughly by how much one fourth \nright so now can you compute these three things what was wtwo what was htwo and what \nwas the number of p arameters in this layer we will do it for a few of these layers and \nthen i will just rush through that so whats wtwo going to be you have already done this \ncomputation right the exercise that we did was exactly this computation so there was \nfiftyfive cross fiftyfive and what is the depth going to be i want everyone to say that \nstudent refer time fivefifteen \nok and what is the number of parameters \nstudent refer time fivetwenty \nninetysix into \nstudent eleven \neleven into \nstudent eleven \neleven into three don\u2019t forget the depth the depth is three here  \nrefer slide time fivethirtyone \n \nso that is the number of parameters that they had in this layer ok eleven into eleven into three into \nninetysix now what is the next layer going to be a max pooling layer  \nrefer slide time fivefortytwo \n \nok so they had a max pooli ng layer where they used a three cross three max pooling that \nmeans you are going to pick up max from a three cross three grid and the stride was two that \nmeans we are going to get half the output and now can you tell me what wtwo htwo would \nbe roughly half of fiftyfive fiftyfive right so twentyseven twentyseven  \nrefer slide time sixtwo \n \nand what is the number of parameter is going to be don\u2019t be lazy everyone be say it  \nstudent zero \nzero right so that is the max pooling layer  \nnow what is the size of your input volume at this point \nstudent twentyseven \ntwentyseven cross twentyseven cross \nstudent ninetysix \nninetysix as opposed to the original input which was two hundred and twentyseven cross two hundred and twentyseven cross three so as you keep \nprogressing your width and height is decreasing but your depth is increasing because \nyou are using more and more filters to capture more and more patterns in the images  \nrefer slide time sixthirtyseven \n \nnow so you have twentyseven \n\uf0b4  twentyseven \n\uf0b4  ninetysix then they decided to use two hundred and fiftysix filters each of size five \n\uf0b4  five \nwith a stride of one and pad ding of zero ok is it right so how many parameters do you have \nnow \nstudent refer time sixfiftythree \ntwo hundred and fiftysix into  \nstudent five into five \nfive into five into \nstudent ninetysix \nninetysix \nrefer slide time sevenzero \n \nso thats the number of parameters that you will have and the size s ince would decrease \nonly by one right because you have a stride of it will decrease by two because a filter size is \nfive and you have a stride of five ok \nrefer slide time seventwelve \n \nso these are the number of parameters we had zerosix million parameters in this l ayer \nwhat is the next layer going to be pooling  \nrefer slide time seventwenty \n \nso you do a max pooling again you do a three \n\uf0b4  three you do a stride of two so your width in \nheight is going to decrease the depth does not change remembe r in max pooling the \ndepth does not change because the max pooling operation is per feature map it is not \nacross the depth fine then use a three \n\uf0b4  three filter and three hundred and eightyfour of those  \nrefer slide time sevenforty \n \nso how many parameters would you have \nstudent refer time sevenfortythree \nthree hundred and eightyfour into three cross three into two hundred and fiftysix the depth  \nso now you guys get it so i will not bore you anymore  \nrefer slide time sevenfiftyone \n \n and then you have a convolution operation again which is a three hundred and eightyfour convolutions each of \nsize three \n\uf0b4  three and so many parameters followed by a convolution operation again followed \nby a max pooling operation then followed by a fully connected layer \nrefer slide time eightfive \n \nso what would i do to this two hundred and fiftysix \n\uf0b4  two \n\uf0b4  two i will fatten it so i will get what dimensional \noutput \nstudent one thousand and twentyfour \none thousand and twentyfour two hundred and fiftysix into two into two so this one thousand and twentyfour dimensional vector i am going to fully connect it to \na four thousand and ninetysix dimensional vector how many paramet ers four million right four into ten raise to six \nright so roughly four million  \nrefer slide time eightthirty \n \n then you have another four million another four thousand and ninetysix vector fully connected how many \nparameters  \nstudent sixteen million \nsixteen million then you have the one thousand classe s that you are interested in right so again \nfully connected so you get the full architecture anyone has any questions no one \nwants to know why this particular configuration among all the possible configurations \nwhy not ten layers why not first eight cro ss eight filters why not nine cross nine filters  \nunfortunately no one knows laughing \nstudent refer time eightfiftynine \nso this i mean see this what this is what would happen right now we get into something \nknown as hyper parameter tuning right so what are the h yper parameters in this \nnetwork the kernel size is and the number of filters right so you would have tried a \nlot of these things evaluated on the validation set seen which one gives the best \naccuracy and then chosen right so that is probably what wo uld have happened but \nthere is not enough insight into how this particular architecture came up  \napart from some things right that three curves three neighborhood sounds reasonable initially \nwhen you have the full image you use larger filter sizes because yo u want to capture a \nlot of things there but once the image has shrunken you use smaller filter sizes so those \nare some rational decisions which look reasonable but why these three convolutional filter \nlayers back to back instead of convolution max pooling convolution max pooling and so \non right \nso the some of those things are not clear so just in case you are wondering do not \nwonder this is just the architecture this is known as modestly named as alexnet so \nthat is laughing yeah and so i said that  this has eight layers but you clearly see more than \neight layers here so why did i say that has eight layers which are the layers we are not \ncounting \nstudent refer time tenten \nwhy  \nstudent refer time tentwelve \nbecause they have no parameter right so when you count the number of layers you only \ncount those layers which have parameters so you have five convolutions and three fully \nconnected layers  \nrefer slide time tentwentyone \n \nthen so the total number of parameters in this network is twentysevenfiftyfive million parameters an d \nok at this point i will and obviously you notice that most of these parameters were there \nin the fully connected layer so you had four million here then sixteen million here and then \nagain four million here right  \nso roughly twentyfour million of the twentyseven million par ameters were there in the fully connected \nlayer you see that skew in the number of parameters ok \nrefer slide time tenfifty \n \nand i will just look at the fully connected layer again so the last max pooling layer \nactually gave you a two hundred and fiftysix cross two cross two output you just flatten it to get a one thousand and twentyfour \ndimensional vector and then you connected fully to the four thousand and ninetysix vector right so that is what \ni mean by a fully connected layer why do you move max fully  \nso the reason for that is basically to shrink the size of the image right because after that \nif if you keep working with this size right then the number of parameters is going to \nreally blow up a by using a larger stripe yeah both of them are feasible right so now \nsee from here remember that we had the ori ginal image sizes two hundred and twentyseven cross two hundred and twentyseven and by the \nend we were just left with two cross two \nand then adding a fully connected layer on that makes sense right if i had not done this \nshrinkage throughout either by increasing the stride of the convolution layer or by d oing \nmax pooling right then you would have left with something of the order of two hundred cross \ntwo hundred here and then you have to do a fully connected on top of that is just infeasible right \nit just throws away all the hard work that you have done by doing weight sharing and \nsparse connectivity right so that is not feasible  \nthere are also papers with say which i think it is titled fully convolutional neural \nnetwork which does not have any max pooling layers and they show that that also works \nfine in fact whe n we see vgg net we will see that it has back to back convolution \nlayers and very few max fully layer right so these are all things which people have \ntrained  \nnot so many years two years the challenge came out in two thousand and ten and in two thousand and twelve this was used \nright so it is like not really a large gap right and if you read the original paper they \nhad to do a lot of tricks to actually make this work it was not as simple as i am showing \nit of course now with all the stability which comes from these platforms tenso r flow \npytorch you can probably just go and implement this as it is and you should be able to \nreproduce the results but six years back that was not the case there was a lot of hard \nwork involved in getting this too work and they this was also the paper  which \nintroduced the relu non linearity in the context of convolutional neural networks right \nso they had to change from sigmoid or tan edge to relu \n a lot of these small small things which they had done and at that time it is also not \npossible with t he existing hardware to train this on the given gpus that you had at that \ntime so they had to do some splitting across gpus and so on so it was not as simple \nas it is today with all the hardware as well as api developments or platform \ndevelopments around this right so probably that is why it took two years to yeah sure  \nso each of these things so after you do the convolution operation you pass it through \nthe relu non linearity ok so what does that mean is that the convolution operation \ngave you a  feature map every entry here was just a weighted average of the neighbors \nright you take this entry or rather you take this feature map and create a new feature map \nwhere every entry here is the sigmoid of every entry here do you get that or not sorry \nsigmoid some nonlinearity and they use the relu has the non linearity so you do get \neveryone gets this so all the convolution layers are followed by a relu non linearity \nlayer \nso you get this volume pass it through the relu and get a new volume b ut i have just \nshown that as a single operation it is before pulling so this was the fully connected layer \nso now we look at the next architecture which is zfnet \nrefer slide time fourteenfive \n \nnow i am going to compare zfnet with alexnet so on the to p you will see alexnet \non the bottom you will see zfnet ok so again the input was the same two hundred and twentyseven \n\uf0b4  two hundred and twentyseven \n\uf0b4  three \nnow instead of eleven cross eleven filters zfnet decided to use seven \n\uf0b4  seven filters and their rationale \nwas that you do not need such large neighborhoods you do not need as small as three \n\uf0b4  three \nbut probably you need at least as much as seven \n\uf0b4  seven you do not need eleven \n\uf0b4  eleven so that is the \nfirst change that they did and that would also result in some parameter pruning right \nbecause the number of parameters now would be seven cross seven into three so the difference in \nthe number of parameters at this layer for zfnet which is at  the bottom and alexnet \nwhich is at the top would be this how many of you get this ok so thats in the \ndifference in the number of parameters so now the output volume still remains the \nsame its fiftyfive \n\uf0b4  fiftyfive \n\uf0b4  ninetysix  \nrefer slide time fifteenthree \n \nthen again they had the same max pooling operation this layer there was no difference \nbetween zfnet and alexnet and then after that you had again layer three which was \nexactly the same as alexnet  \nrefer slide time fifteentwenty \n \nthen layer four again the same as zfnet afterwards layer five instead of three hundred and eightyfour filters they \ndecided to use five hundred and twelve filters the rest of the thing remains the same that means the size or \nthe spatial extent of the filter remains the same that again results i n some difference in \nthe parameters so thats the number of parameters that got added in zfnet as opposed to \nalex net  \nand of course the  oh sorry sorry oh sorry the bottom one is a zfnet yeah that is \ncorrect sorry so in zfnet you had five hundred and twelve filters as opposed to three hundred and eightyfour filters in alex net ok \nis it fine  \nrefer slide time fifteenfiftytwo \n \nand then the next layer again instead of three hundred and eightyfour filters they had one thousand and twentyfour filters  \nrefer slide time fifteenfiftyseven \n \nthen again instead of two hundred and fiftysix they had five hundred and twelve filters and then a max poo ling layer then the \nsame dense fully connected layers ok \nrefer slide time sixteentwo \n \nso everyone gets this this is the difference between the two architectures and this led \nto that difference in the error of around three to four percent is that we have seen earlier \nrefer slide time sixteenfifteen \n \nso difference the total number of parameters was onefortyfive million and of course zfnet had \nmore parameters because is that it has these more filters in the deeper layers ok so we \ngo to the last point which is may be more in that vgg net  \nrefer slide time sixteentwentysix \n \nso again in the case of vgg net the input was ok so i just want to i will not see it \nrefer time sixteenfortyone in so the input was again the same it was rgb cross two hundred and twentyseven cross \ntwo hundred and twentyseven  \nso this is what the vgg arc hitecture looks like they have so in vgg network \nthroughout ok wait so how many layers this zfnet have eight so you only count the \npink boxes because the those has ones which have two parameters now vgg net has \nslightly more number of layers but in all the convolution layers they use three cross three filters \nright from the beginning they use three cross three filters ok so you have the first \nconvolution layer then another convolution layer another convolution another max \npooling layer followed by two convolut ion layers then a max pooling layer followed by three \nconvolution layers max pooling just keep adding box is writing just because you can \nand then you have the fully connected layers  \nso again there is not much intuition for why sixteen in fact later on so meone came if this is \nthe vgg sixteen architecture because it says sixteen layers later on some of someone came up \nwith the vgg nineteen architecture which has nineteen layers right so a lot of this is data center \neven right so you try your best to get the best possible a ccuracy on the imagenet data \nand that is the architecture you came up with right  \nbut as long as how many of few feel comfortable with what is happening right and i \nmean when i say comfortable i mean that you really understand the gory details of \nwhat is happening at each layer in terms of input volumes output volumes number of \nparameters how are you going to train this network end to end can you see how are you \ngoing to train this so you will get some loss here that is going to propagate all the way \nback to the first layer right and this propagation is going to happen over some sparse \nconnections that fine now this is one very important point that i have skipped and \nwhich none of you is questioning is everything that is happening here differentiable \nstudent refer time eighteenthirtyeight \nwhat happens to max pooling is max pooling or differentiable operation so i am \ngoing to ask you this how are you just note this down how are you going to back \npropagate to the max pooling layer because you need to see whether the max pooling \nlayer is actually a differentiable layer or not  so here i just some statistics about vgg \nnet everyone is writing that down laughing this perhaps means i will not ask it  \nthe kernel size is three cross three throughout the tot al number of parameters in non fully \nconnected layers is sixteen million the total number of parameters in fully connected layers \nis one hundred and twentytwo million so you see that this fully connected layer is really a problem it like really \nhogs all the lime that it has the m aximum number of parameters there right and so and \nthe most number of parameters are there in the first fully connected layer because you \nhave this five hundred and twelve \n\uf0b4  seven \n\uf0b4  seven you remember then alex net and zfnet the last layer was two hundred and fiftysix \n\uf0b4  \ntwo \n\uf0b4  two which has definitely more manageable than this layer which has grown almost eight \ntimes in size but not even eight actually four into four into two right sixteen thirtytwo times in size right \nso that is really blown up the number of parameters in the first fully connected layer \nso you just imagine the i mean you have such a deep layer and then you realize that all \nthe main number of parameters are there in this one particular lay er everything else  is \nmuch fewer parameters or orders of parameters less number of parameter is less then \nthis one fully connected layer"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.4 CNNs (success stories on ImageNet).wav", "duration": 1208.33, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  ninetyone \ncnns success stories on imagenet contd \nthis is where  we left off in the last class  so  we  look at  three  networks  for image \nclassification starting with alex  net then zf  net and then vgg  net  vgg net in \nparticular had  sixteen layers  including convolutions and fully connected layers  and one \nthing that we saw that a large number of parameters are there in the first fully  connected \nlayer \nrefer slide time zerothirtysix \nbecause you are connecting a five hundred and twelve \n\uf0b4  seven  seven volume to a four thousand and ninetysix dimensional vector right so \nthat is one thing the other thing that i would like to kind of mention right now so that it \nbecomes useful for the later part of the lecture is that if i look at any of these pink boxes \nhere right or even these things which are known as the fully connected layers and if i \njust flatten them out and view them as a vector what does that vector actua lly capture \nit captures a it captures an abstract representation of the image right \nso now imagine what would happen is suppose you have trained one of these networks \nalex net vgg net or any of your favorite networks and by what i mean by training is \nthat you have been tracking the cross entropy laws and you have run it for several epochs \nwith some patience and so on and i was satisfied with whatever training error you are \ngetting and you have stopped training now right  \nnow after this if i pass images through these net through this network and i take the \nrepresentation from any of these boxes or from the fully connected layer what is it that i \nhave essentially got now i have got an abstract representation of the image that i have \nbeen feeding it right so just remember this and this is something that we will use \nso this is very common to do so you have a trained image net many people have \nreleased different models for image net the ones which we have covered being included \nthem and now for  him any image task if you want to do some processing then it\u2019s \ncommon to take the strain network pass your image through that \nso you can train any you can use any image trained image net and pass that image \nthrough it or sorry any trained convolutiona l network trained on image net and pass the \nimage through that and you can get a representation for that image and these are known \nas the fc representations and these are as the convolution representations ok any of the \nconvolution layers fine"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 11.5 Image Classification continued (GoogLeNet and ResNet).wav", "duration": 1329.98, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  eleven \nimage classification continued \ngooglenet and resnet \nso we will g o to the nex t module where y ou wanted to look a t two more architectures \nfor image classification these are googlenet and resnet  \nrefer slide time zerotwentytwo \nso here is a question right so consider the output at a certain layer of a convolutional \nneural network so you have this layer after layer of c onvolutions and max pooling and \nso on and you are at somewhere in the middle and this is what your volume looks like \nthis is what your output volume looks like now after that you could apply a max \npooling layer you could apply a one \n\uf0b4  one convolution you could apply a three \n\uf0b4  three convolution \nor you could apply a five \n\uf0b4  five convolution right  \nand so far we saw that all these architectures they do one of these things they either do \na max pooling or they do a three \n\uf0b4  three convolution or they do a five \n\uf0b4  five convolution or a seven \n\uf0b4  seven \neleven \n\uf0b4  eleven right any convolution but they are all uniform they are all either three \n\uf0b4  three all \neither five\n\uf0b4  five or either seven \n\uf0b4  seven right so why choose between these options why not do all \nof this at every layer do you get the question that i am trying to ask right \nso far what we were doing is that you have this volume this volume at a certain layer of \nthe convolutional neural network and after that you are either using all three cross three filters \nso you are using two hundred and fiftysix three \n\uf0b4  three filters or five into three \n\uf0b4  three filters or using seven \n\uf0b4  seven or using five \n\uf0b4  five \nyou are never using a mix of all these right so why not use a mix of all these why the \nwhy take a decision on that i only wan t three \n\uf0b4  three because it is possible that you want to \ncapture interactions at different levels so you should have varied size filters at every \nlayer  \nso how many of you get the question and the intuition that i am trying to ask o k  so \nthe idea here in googlenet or in the inception net is that why not apply all of them at the \nsame time and then concatenate the feature maps right so i will also do max pooling i \nwill also do three \n\uf0b4  three feature maps i will a lso create five \n\uf0b4  five eleven \n\uf0b4  eleven and then just \nconcatenate all of these together so let us see how to do that right  \nrefer slide time twofourteen \n \nnow one problem with this naive idea is that it could result in a large number of \ncomputations so let us see what i mean by that so suppose the padding is zero the stride \nis one then if you have a w \n\uf0b4  h \n\uf0b4  d input as the volume and if you have an f \n\uf0b4  f \n\uf0b4  d \nfilter then the output would be of this size we all agree with this this is the formula that \nwe have been looking at so this is the size of the output volume now every entry in \nthis output volume requires how many computations to get a single entry in this output \nvolume how many computations do i have to do  \n student f \n\uf0b4  f \n\uf0b4  d \nf \n\uf0b4  f \n\uf0b4  d so how do i get a single value i apply a convolution at that value and then i \ndo those many computations and here the number of computations is that i am going \nover this block of f \n\uf0b4  f \n\uf0b4  d doing a weighted multiplicati on and then adding them up \nright so you need that many computations everyone is clear with this ok  fine  \nso each element of the output requires these many computations and we have so many \nelements in the output right so you are doing really those  many number of \ncomputations right so can we do something to reduce the number of computations right \nso that is the key idea that we need to focus on so all of us buy the idea that doing this \nmulti granular or multi sized filters is a good idea because you are capturing interactions \nare different layer but i showed you that this is a problem with this you guys just apply \nmultiple filters so let us see what we do \nrefer slide time threefiftythree \n \nso we what we do is one \n\uf0b4  one convolutions what is the one \n\uf0b4  one convolution do what does \nit make sense \nstudent refer time fourtwo \nhow does the one \n\uf0b4  one convolution make sense you have a pixel i fit a one \n\uf0b4  one convolution \non that what will i get i will get back the pixel  \nstudent refer time fourtwelve \nalong the depth right so remember it is not one \n\uf0b4  one it is one \n\uf0b4  one \n\uf0b4  depth \nstudent depth \nright so what does a one \n\uf0b4  one convolution do it actually aggregates along the depth so \nthis is what my one \n\uf0b4  one convolution looks like it is one \n\uf0b4  one \n\uf0b4  d so i just fit that block on \nthat pixel and do everything along the depth and get a single value right so from a threed \noutput using a one \n\uf0b4  one convolution i can go to a two \n\uf0b4  twod feature map everyone gets this \nok  \nnow i could use several of these one cross one operations one cross one convolutions in fact i \ncould use done of these such that done is less than d so what effectively happens the \ndepth of the output reduces so i take a certain output volume whose origin al depth was \nd now i take done one \n\uf0b4  one convolutions right so i get an output whose depth is smaller \nthan the depth of the original output is that fine everyone gets this ok  \nand you see how this will save computations right bec ause remember that this was f \n\uf0b4  \nf \n\uf0b4  d and now i have reduced d to done ok so it is going to reduce the number of \ncomputations so that is what the idea is you reduce it from f \n\uf0b4  f \n\uf0b4  d to f cross of \ncross done right so thats this particular network or this paper introduced the idea of this one \ncross one actually it did not introduce it used it but it made it popular probably  \nrefer slide time fivefortyone \n \nnow once you have done this so this is how i am going to proceed now i have a certain \nvolume i have applied one \n\uf0b4  one convolutions to it using that i have reduced the number of \ndimensions now i am going to apply three \n\uf0b4  three convolutions as well as apply five \n\uf0b4  five \nconvolutions on top of that right because that is the motivation that i had started with \nthat i want to apply kernels of multiple granularity  \nnow can you think of some refinem ent to this you see this branching over here why \nuse the same one \n\uf0b4  one convolutions before feeding to three \n\uf0b4  three as well as five \n\uf0b4  five i could use a \ndifferent set of one \n\uf0b4  one convolutions and feed it to a five \n\uf0b4  five and use a different set of one cross \none convolutions and feed it to three \n\uf0b4  three is that fine what is the problem with this \nstudent again increasing the number of computations \nagain increasing the number of computations right but they found out that the tradeoff \nbetween this is fine even if you are doing more one cross one operations it still is ok the \nnumber of computations are still manageable ok and then you c ould also do a max \npooling because we were choosing between these things right five \n\uf0b4  five three \n\uf0b4  three seven\n\uf0b4  seven and \nmax pooling so we will do all of these in parallel and we also do some one \n\uf0b4  one \nconvolutions so how many different types of operations we have done we have done \none \n\uf0b4  one three \n\uf0b4  three five \n\uf0b4  five and max pooling followed by one \n\uf0b4  one convolutions  \nnow all of these outputs that we have got we have got a bunch of feature maps now \nthis is one set of feature maps this is another feature maps this is another and this is \nanother all of these four we are going to concat together to get a single output volume do \nyou see what is happening right it is not very mechanical there is nothing really \nprofound about what is being done the only two profound ideas are one is apply \nmultiple kernels of different sizes and the other  is to use one \n\uf0b4  one convolutions to make the \nwhole computation manageable that these are the only to main ideas the rest of it is not \nvery different from what we have been doing how many if you get this operation \ncompletely \nso this block is called the inception module ok this entire thing is called an inception \nmodule so in subsequent slides when i put an inception module then you know that \nthese parallel operations are happening right so far whenever we had seen a \nconvolutional neural network it was all serial right so you started with one operation \nthen another operation then another operation and so on now you have an output or an \ninput volume you apply multiple operations in parallel and get one single output right \nso it is a parallel serial combination ok \nso you will now see the full googlenet architecture so his question was basically three \ncross three would result in a different sized feature map right because of an five cross five would \nresult in a different sized feature map so i will use appropriate padding so that all of \nthis becomes equal ok  \nrefer slide time eighttwentysix \n \nso this is how googlenet looks like so you have the input again rgb and same two hundred and twentyseven \n\uf0b4  \ntwo hundred and twentyseven or two hundred and twentynine \n\uf0b4  two hundred and twentynine then you apply a convolution layer followed by a max pooling layer \nconvolution max pooling then you have this inception module with a very specific \nconfiguration so they have ninetysix one \n\uf0b4  one convolutions before feeding to one hundred and twentyeight three \n\uf0b4  three \nconvolutions sixteen one cross one convolutions before feeding to thirtytwo  five\n\uf0b4 five convolutions and so \non and i do not really see much point in going into the details of these numbers there \nis hardly any intuition behind them  \n i again guess that it\u2019s you try a bunch of things and this is the one which probably gave \nthe best output so the key idea is that of course you have this inception module which \nis a parallel module which does a lot of operations in para llel this is again followed by \nanother inception module which has a different configuration followed by max pooling \nthen again a few inception modules in fact five of them again max pooling then inception \nand this is the other interesting idea that they came up with  \nso at this point remember in vgg net at the final layer you had an output of size five hundred and twelve \n\uf0b4  \nseven \n\uf0b4  seven right and we said that this was a problem how many of you remember this why \nwas this a problem \nstudent refer time ninefiftyfour \nbecause i need to connect this to a \nstudent fully connected \nfully connected layer right and that fully connected layer was of size four thousand and ninetysix right so what \nthey said is that what you could do is instead of taking five hundred and twelve cross  seven cross seven for each of \nthese five hundred and twelve feature maps that you have take the seven cross seven and just do an average pooling \nfrom there what does what do i mean by that \nstudent average \ntake these seven cross seven values take an average of that so now instead of five hundred and twelve cros s seven cross \nseven how many values will you end up with \nstudent five hundred and twelve \njust five hundred and twelve and in their case instead of one thousand and twentyfour cross seven cross seven you will just end up with one thousand and twentyfour \nvalues right so instead of looking at these dense connections with every pixel in the \noutput volume you just take the average of those pixels and then do a dense connection \nfrom there so from this volume you just go to a vector of size one thousand and twentyfour which is exactly \nthis vector shown here and from there on life becomes easier right because you have \ndone a fifty  percent sorry fifty times reduction in the volume so this was one thousand and twentyfour cross fortynine \nnow we just have one thousand and twentyfour cross one so you have a fortynine times reduction in the size and that is a \nhuge parameter reduction  \nand that actually worked very well in practice they of course  add these dropouts and \nother things and then you have your fully connected layers and finally the soft max \nlayer at the output to predict one of the thousand classes right so this is the full \nstructure of googlenet or inception net or with multiple i nception modules right so \njust remember that key takeaways are three one is half filters at multiple granularity \napplied in parallel the other is use one cross one convolutions to reduce the number of \ncomputations and the final one is to use this average  pooling to make sure that you do \nnot have this blow up of parameters at the output ok so these are the three main ideas \nthat you need to do right ok \nrefer slide time elevenfortyfive \n \nso this is exactly what i explained so instead of having this nasty looki ng connection \nwhich would have been fifty thousand one hundred and seventysix cross one thousand you just take the average from this grid and \njust get a one thousand and twentyfour dimensional vector which results in a much smaller weight matrix at the \noutput everyone gets this so ok yeah so this is fine \nrefer slide time twelveeight \n \nso this has twelve times less parameters than alexnet it has two times more computations \nright so that is what i meant by the tradeoff so the number of parameters has reduced \nsignificantly of course a large amount of this savings happen in the fully connected \nlayer its not the ingenuity in the inception module which led to the fewer number of \nparameters that actually leads to more number of parameters right  \nbut the reason they could afford more number of parameters in the convolution layers is \nbecause they reduced a lot of parameters in the fully connected layer do you get that \nso they did this tradeoff and it has two times more computations then alexnet but it is \nstill acceptable because you see that there are many many layers as compared to alexnet \nright so let us actually count the number of layers that we had here so one two three four five six seven eight nine \nten eleven twelve thirteen fourteen right so it has fourteen layers and each of these inception modules is again \nlike split layer right it has this parallel components there  \nso having two times more computations was still an acceptable tradeoff and it of course \nled to much better accuracy as compared to alexnet or zf net or vgg net right that we \nhad seen in the original trend graph ok so now we will go on to the last architecture that \nwe will discuss for image classification which is resnet \nrefer slide time thirteentwentyfour \n \nso here is the idea behind resnet or here is the motivation right now suppose we have \nbeen able to train a shallow neural network well now again m y definitions of shallow \nare relative this is by no means shallow there are many layers here right so you have \nsome eight layers here  \nnow if i have been able to do this properly that means what i mean by that is that using \nthis network at least i was a ble to reduce the training error to zero or close to zero some \nacceptable value and i was able to get some reasonable generalization performance \nthat means on the test that i was able to get some reasonable accuracy that is what i \nmean by i was able to make this network work well  \nnow suppose i add a few layers to this network and i have carefully added some layers \nin between here and in between here or over there right now intuitively i could argue \nthat if the shallow network was working well right then for the deep network this is exist \nat least one solution which can directly come from the shallow network can you tell me \nwhat that solution is \nstudent refer time fourteentwentynine \ni want all of you to kind of digest that idea what the deep network could have  done is i \nknow that this shallow network works why not i just behave like that and i learn these \nparameters in such a way that i just end up copying from here to here how many if you \nget this right so there is a case for the deep network to do at leas t as well as the \nshallow network and it could do the same thing at this point all of you get this idea \nright \nso in other words the solution space of the deep network or rather the solution space of \nthe shallow network is actually a subset of the solutio n space of the deep network there \nwas one solution for the shallow network which could have been used as it is for the \ndeep network of course for the deep network there are several other solutions because \ninstead of the identity here you could learn di fferent things there but at least that one \nsolution exists so i should at least if i do use this in practice ex i should expect that this \nwould work as well as the shallow network right is that argument fine with everyone  \nstudent refer time fifteentwentynine \nor which has only one yeah yeah of course \nstudent yes \ni mean so it those arbitrary things would not work but here what there it is the for the \nexplanation intuition right you are using some reasonable things and you are just trying \nto make it compatible with whatever you have so far so the argument is valid right so i \ncannot expect that i had a volume whose depth was one hundred and twentyeight and then i suddenly decide to \nuse only one filter in the next layer right  \nthat means i have compressed everything and now i expect it to be able to recover from \nthere that is not going to happen right so that is a fair argument but the argument \nwhich i was trying to make or at least for the illustration purpose is that if you do \nreasonable things and that is what people were  trying out right these this is the exact \nnetwork that someone was trying out and this did not work with well i will tell you what \nit is so do you get his doubt and my clarification on that is that fine ok  \nrefer slide time sixteensixteen \n \nso this is wh at was happening in practice right so you have a twenty layer network or thirtytwo \nlayer network or fortyfour layer network and a fiftysix layer network and you see that the training \nerror of deeper networks is much higher than the training error of the shallow networks \nthat means this argument which i was trying to make that the deep network should at \nleast do as well as the shallow network was not working well in practice right and it is \nif you think about it is not very surprising because this argument hinged on the fa ct that \nit should be able to learn this identity mapping but this identity mapping is one of many \nsolutions right  \nso for it to be able to narrow down on that solution it is easy for you and me to think \nabout it but for the network it does not have t his intuition right that i can just copy it \nfrom there to here do you get that the solution space is really really large and like \nfinding that needle in a haystack right you have these many solutions possible and i \nam trying you to arrive at a soluti on where you end up with the identity solution is that \nclear and it is not so easy for the network to do that everyone gets this how many of \nyou get this idea  \nrefer slide time seventeentwentyfive \n \nso why not explicitly try to do something of this sort where the network can actually \nlearn some kind of an identity function so now consider any two layers you know by \nstack layers i mean this is a convolution layer and followed by a convolution layer right \nso these are two convolution layers back to back so from  i what do i actually end up \ndoing here i had a certain input x and i am learning some transformation of x through \nthese convolution layers right i am trying to learn x and then i sorry i was given x i am \npassing it to convolution layer so i will run some transformation of x  \nand my argument was that if it could learn to directly copy x here the deep network \nshould at least work as well as the shallow network so why not i explicitly ensure this \nso why not i do this that in addition to these conn ections i also explicitly connect x to \nthe output do you get this so now what i am trying to do this is hx is equal to f of x \nwhich is the transformation that i learned for x and in addition i also add x so what am \ni doing i am explicitly feeding i t the identity function right how many if you get the \nintuition for this so what i am trying to do is i have a sense that if i could have \ntransferred this x as it is across layers then there is a chance that i should be able to do as \nwell as the shallow network right  \nso now i am going to explicitly ensure that that you learn these transformations but i \nwill also feed you x at every stage at a reasonable time right so these are known as skip \nconnections so after every two layers i will feed back th e x or you could try after every three \nlayers i will feed back the x so i am trying to maintain the original copy of x after every \ninterval ok fine so why would this help so this follows back from our argument and \nit is the same thing which i said before \nrefer slide time nineteenseventeen \n \nso using this idea of using these skip connections these authors were able to train a \nreally deep network of one hundred and fiftytwo layers right and this gave on multiple vision challenges \nright one being imagenet it gave sixteen percent better  results and the best network and \nthis is the one which reads that near human performance then imagenet localization is \nanother challenge where you need to localize the object so there they get twentyseven percent \nbetter than the best results and there are th ese bunch of other vision tasks detection \nsegmentation and all of them and in all of them this significantly outperformed the best \nsystem using a very very deep network of course the downside is that you have a very \nvery deep network it will take its own time to train and so on but of course if you have \na microsoft or google you can afford to do that  \nrefer slide time twentyeight \n \nso that is the current theme right i mean the one with the largest computational \nresources wins everything right so and  they also are there is some other bag of tricks \nwhich is not i mean it is not very difficult to understand so they used batch \nnormalization every after every conversation layer have you heard of batch \nnormalization ever in your life ok good they used  xavier by two initialization ever \nheard of that xavier by two was the same as \nstudent he initialization \nhe initialization right then they use sgd not any of the fancy adam or adagrad or \nanything with a momentum of zeronine learning rate was set to zeroone and  divided by ten \nwhenever the validation error plateaus the mini batch size was two hundred and fiftysix they use a weight \ndecay of one ht what is weight decay weight decay is in the context of which \nregularization \nstudent ltwo \nltwo and what does this mean weight decay of one e raised to minus five \nstudent lambda refer time twentyonefive \nthe lambda was set to one e raised to minus five all of you remember these things right we \ndid it in some previous course in some previous life and no drop out was used right so \nsince i have this here i will just say something more on this so in your reading papers \non deep learning right focus on the experimental section where all these hyper \nparameters are described so these are known as the hyper parameters these are not \nrelated to the parameters  of the model these are related to hyper parameters which is \nwhat the batch size is whether you used l two regularization what was the learning rate \nwhat was the optimization and so on \nso turns out in many cases if you do not stick to this you will not be able to reproduce \nthe results of the paper right so you might be wondering that this network i understand \nthis is just one hundred and fiftytwo layers and i can just keep adding skip connections i can easily code this \nup but i am not getting the same results as the original authors of the paper  \nso this is where you need to dig up them right you need to look at the experimental \nsection where most good authors provide these details of how they have tr ained the \nnetwork how many epoch  that they use what was the patie nts set and all that if you \nfollow those the chances of reproducing are much higher still not guaranteed but \ndefinitely much higher ok so that is where we end the lecture on convolutional neural \nnetworks and imagenet classification"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.10 Fooling Deep Convolutional Neural Networks.wav", "duration": 362.62, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 twelve \nvisualizing convolutional neural networks guided backpropagation deep \ndream deep art fooling convolutional neural networks \nso in thi s lecture we wil l look a t va rious ways of visualizing convolut ional neural \nnetworks and although it is not very obvious a t this point a s we g o along we will see \nwhat i mean by that so let us start this lecture \nrefer slide time zerotwentynine \n \nso i forgot to add the acknowledgments slide so a lot of the material that i am going to \ncover today is based on some content by andrey karpaty in his online course stanford \ncourse we will add the appropriate acknowledgments and a link to the course ok \nso with that i will start module one which is visualizing patches which maximally activate \na neuron ok so what are we trying to do here is we are trying to the quest today largely \nis going to be able to understand what a cnn has actually learned right and what i mean \nby that is we said that there are these filters which try to detect edges which try to detect \nblurs and so on and then there are these neurons which fire for certain things and so on \nso we want to see different ways of finding ou t what a convolution neural network has \nactually learned or what have the filters actually learned or what are the different neurons \nin the convolutional neural network actually capturing what do they fire for what are \nthe kind of images that make them tr igger and so on right so that is the first thing that \nwe are going to look at how do you visualize patches which are causing a neuron to fire \nrefer slide time onethirtytwo \n \nso this is again our vgg network just put it vertically say have passed an image  to \nthat and then at every layer you are applying convolutions and then match pooling and \nso on right up to the last layer right now we consider some neurons in one of these \nlayers so i am considering this neuron and i want to find out what exactly is this neuron \ntrying to do right and which is the same as asking what kind of images does this neuron \nfire for \nso i have thousand different classes i have cats dogs cars trucks and so on i am \ninterested in figuring out what are the different kinds of classes that this neuron fires \nand this is more from say i am already getting some output accuracy and i am either \nhappy with it or not happy with it in either case i just want to see what is it that my \nnetwork is learning is there any scope for improving  is that that there are no neurons in \nthe network which actually fire for the dog class did not should i do something \ndifferently was it that most of the neurons fire for all classes that means they do not \nhave any discriminative power so what exactly is going on right  \nso that is why we are that is why this study is interesting and you will do something of \nthis sort in your cnn assignment ok so and by now we are clear that if i am focusing \non any neuron and any layer i can always go back and tra ce the patch to which it \ncorresponds in the input image everyone is fine with that right so we saw that if i am \nsomewhere here then every neuron here corresponds to some sixteen cross sixteen patch in the \noriginal image and the same is true for every layer righ t i can always this is a \ndeterministic process i can just find out which are the original image pixels which \ncontributed to the computation of this particular neuron in any layer ok  \nso i can do that so now what i am going to do is i will send as m any images as \npossible whatever images are there in my training data test data whatever images i \nhave i will pass these images through the convolutional neural network ok and for the \nneuron of interest i will note down which when does it fire and wher e ever it fires and \nby fire i mean it is a output is close to one or it is a output is high because these are \nrelu neurons i look for high output they do not saturate at one right so this i look \nwhich images for which this neuron had an high output and for those cases i will go back \nand trace the image and see which patch of the image actually caused this to fire \nso i want to see whether my neurons are actually learning things like noise detector or \neye detector or something right \nrefer slide time fourone \n \nso let us look at the results of one such experiment done by a group of researchers so \nthey considered some neurons in the pool five layer and they did this experiment that they \npass a lot of images and whenever this neuron fired they went back and  saw what was \nthe patch in the image which was causing this neuron to fire  \nso that they found that one set of neurons is actually fires for people places so if you \ngo back and trace which is the image which caused is to fire or which is the patch t hen \nit is largely centered around a persons face or which is something which is very clearly a \nperson ok another set of neurons fires for dogs another set of neurons fires for flowers \nall sorts of flowers and different orientations different maybe colors  are same here but \nthey are all different thing right somewhere inside a bouquet somewhere inside a flower \npot some somewhere on a table and so on but expected of that these neurons are firing \nfor any flowers that appear in your input image and the fire  only for that patch nothing \naround it \nso it is very is actually able to localize and fire there are some images which fire for \nthis images the digits and alphabets written in the image so these are some addresses or \ndates or billboard signs or something like that and whenever there are these characters or \nnumerals there and this neurons fire \nand some neurons fire for houses and then some neurons fire for shiny surfaces so \nthere is this different sets of neurons which fire for different sets of thin gs right so also \nthat means your convolutional neural network is trying to learn specific characters of the \ninput characteristics of the input and this is one way of visualizing so this is not like \nanything tricky here it is just that its good you ca n think of this as debugging tools for \nyour convolutional neural network right because in your you i guys are used to \nprogramming where you give different inputs and see what is the output and then try to \ndebug it  \n so this is one way of trying to figur e out whether your network has learned does it \nreally need more training is there a certain class of images for which it is not firing at all \nor is it confusing between two classes and so right so that is one way of visualizing"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.1 Visualizing patches which maximally activate a neuron.wav", "duration": 355.4, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  ninetyfour \nvisualizing filters of a cnn \nso now this was visualizing the neurons inside the convolutional neural network so \nneurons remember are the outputs right these are not these are the feature  maps what \nabout the weights itself what are the weights in a convolutional neural network \nstudent refer time zerotwentyfive \nthe filters the filter s themselves are weig ht have y ou eve r tried to visualize weights \nbefore when \nstudent auto encoders \nauto encoders and what was a trick there how did we \nstudent refer time zerothirtysix \nvisualize what was the optimization problem that we solved \nstudent refer time zeroforty \nhow many of you went and looked at the prerequisites how many of you looked at the \nprerequisites ok \nrefer slide time zerofortysix \n \nso we had done something similar while discussing auto encoders so because that we \nhad done something similar while discussing auto enc oders right so we were interested \nin knowing that there is a particular hidden neuron inside the auto encoder and we \nwanted to see that what does this neuron capture so if i give it emnist digits then what \nkind of patterns does it fire for and if you remember we had solved this optimization \nproblem and realize that this neuron will fire for an input which looks like this where \nwone or all the weights which are connecting to this neuron ok what was the dimension of \nthe input if you are dealing with emnist digits \nstudent refer time onetwentytwo \nseven hundred and eightyfour what is the dimension of this a one thing which i have circled here  \nstudent refer time onetwentyseven \nseven hundred and eightyfour right it is written x equal to so it has to be seven hundred and eightyfour why is it seven hundred and eightyfour because there are seven hundred and eightyfour \nweights connect ing each of the input pixels to that neuron right so that means this \nweight matrix itself we can visualize it as an image and thats exactly what we had done if \nyou remember we had this grid of images that we were analyzing and in some images we \nsaw that some dark element fires here and each we were arguing that this is the curve \nwhich exists in two or nine or eight and that is the one which is capturing  \nand in some cases there was a cusp here which was firing and we were arguing that this \ncould be for the three or f or a nine or for a eight or something like that right so we were trying to \nvisualize these things and the way we had plotted it was just treating this weight matrix \nor weight vector as an image and seeing what causes the neuron to fire right  \nrefer slide time twotwentyone \n \nso we can do something similar for convolutional neural networks i want you to think \nhow would you do that i will give you some hints  the answer is there on the next slide \nbut i just want you to think about it right so remember here you have dense connections \nok that means your weight vector was the same dimension as the input vector what \nabout filters in the case of cnn they are smaller they are three \n\uf0b4  three five \n\uf0b4  five or seven \n\uf0b4  seven much \nsmaller than your original image \nso then what do these filters correspond to just think of the animation that we had seen \nright we had this image and we were taking a filter and applying it at different places so \nwhat does the filter cor respond to what is the filter overlap  with patches in the image \nright so now what kind of analysis can you do \nstudent dense \nwhat kind of patches does this filter fire for or what kind of patches does the neuron \nconnected to this filter fire for does that make sense everyone gets the intuition how \nmany if you get the intuition please raise your hands thank you  \nrefer slide time threetwentythree \n \nso now recall that we can think of a cnn as a feed forward neural network and in \nparticular when you have  a filter it actually interacts only with few pixels right so \ninteracts with say pixel one two five and six so that is the patch that it interacts with \nand now i want to see when does this neuron fire so that is the same as asking what do \ni put in one two five six for this neuron to fire or similarly what do i put in three i do not know this \nwas one two five six i guess so three four seven eight for the same different neuron to fire right but all \nthese neurons fire because they are connected to the same filter  \nso that means i am  interested in these patches which will cause the neuron to fire and \nthose patches can appear anywhere in their image is that fine that is the whole point of \nconvolution neural networks wherever there is a nose whether it is at the top corner of \nthe image or the center or the bottom it should be able to detect right that is the whole \npoint of weight sharing and sparse connectivity ok \n refer slide time fourtwentythree \n \nso we are going to do exactly the same thing we will have a three \n\uf0b4  three filters or five \n\uf0b4  five \nfilters or seven \n\uf0b4  seven filters were just going to visualize as them as images but unlike the \nearlier case where the image a ctually correspond to the full mnist image here these \nimages are just corresponding to those three cross three or five cross five patches and you want to see \nwhat kind of patches causes the neurons to fire ok and the solution is still the same we \nwill have this w by w the normalized weight filter weight which is causing the input to  \nfire how many if you are fine with everything at this point please raise your hands high \nup \nrefer slide time fourfiftyfive \n \nso this is what we get right and we observe certain things which like we had earlier \nmade a case for that these filters try to detec t certain types of patterns or textures or \nedges so you can see that right this is capturing these slanting edges this is trying to \ncapture some horizontal sorry vertical edges then some edges oriented differently and \nalso some colored patterns some tex ture right so you see something like a checkbox \nhere or chess box here and so on \nso these filters are actually firing for different kinds of patches so they are trying to \ndetect different things from the images so you could visualize this and unles s you see a \nlot of variety in this that means something is wrong right because your filters are not \nbeing trained to be discriminative with terms of different patterns that they can detect \nand so on right so you want these variety of patterns to occur  ok and i am going to \nmake a claim that this is only interpretable for the first layers in the convolutional neural \nnetwork why is it so i am seeing some half complete answers so i will ask this as a \nquiz question"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.2 Visualizing filters of a CNN.wav", "duration": 350.48, "text": "\uf0b4\n\uf0b4\n\uf0b4\n\uf0b4\ndeep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 ninetyfive \nocclusion experiments \nok the another thing that you can do is to figure out whether things are \nworking properly or not so you can do something known as an occlusion experiment \nso these \nare all your debugging tools sort of to say if you are working in vision or \ncomputer vision where you are using a convolutional neural network and this is to \ngain more \ninsight said most of you will get away by just taking an off the shelf convolution \nneural network training it on your data getting some accuracy and reporting it \nbut for those of you who want to really understand what is happening and how can \nimprove things further so this could actually tell you for example if you want to \uf0b4\ncompare whether a five  five filter would have been better than a seven  seven filter then \nyou could have observed what these filters are actually learning and in your data does it \nmake \nsense to have a five  five filter versus seven  seven filter because maybe the five  five filters are not \nbeing able to distinguish enough but if you had used a smaller or a larger filter things \nwould have been different right \nso this is for people who really want to get into the know how of how things are \nworking otherwise most of you i do not really expect you to do this is but this  is an \nimportant set of tools to have and i would strongly encourage everyone to experiment \nwith them and some of this you will do in the assignment ok \nrefer slide time onesixteen \n \nso here is the idea of behind occlusion experiments so we are interesti ng knowing that \nwhat patches in the image are actually causing the output to belong to a particular class \nright \nso i have here the figure of a dog and the class being probably predicted is a \npomeranian and i want to know that what patch of the image act ually resulted in this \noutput right so have you tried doing this in any other context if you want to know if \nyou have several features or several things several factors and you want to decide which \nactually influenced the output how would you do it s o what you could do is you could \ndrop one factor right and see whether your output would have drastically changed if it \ngoes from positive negative then that maybe that was the factor which really mattered \nright \n so if for example it is a movie revie w classification right so when you drop certain \nwords from your review so you drop the word amazing great and so on and keep \neverything else the same now it is quite likely that your probability of the review still \nbeing tagged as a positive review will at least drop earlier maybe with these words it was \ngetting tagged as a positive review with zeronine probability it would come down to zerosix but \nnow if you drop words like the and for and so on then you do not expect the output to \nchange much because th ese words are not really important indicators of positive or \nnegative ok \nso the similar thing that you do here is you occlude certain patch patterns a certain \npatches of the image so i have shown one occlusion here so i have replaced that patch \nby a gray patch and i again feed the image to the convolutional neural network and i see \nwhat is the probability of the pomeranian class right now ok and i do it for all such \npatches in the image i can do for as many patches as i want \nrefer slide time threethree \n \nand i create something known as a heat map so the red portions here are the ones \nwhich do not cause a large drop in the output probability if you occlude them and the \nblue portions are the ones which cause a large drop in the output probability if you \nocclude them and it is pretty obvious because what is happening is when i cover the \nface of the dog the probability drops drastically and that is what you would expect right \nso this is also an indicator that your network has actually learned something meaningful \nit is being able to detect this based on the facial features and not just randomly guessing \nthat this is a dog right and see the similar experiment \nso for example if there is a car sometimes these results are not very at least to me  it \ndoes not look very intuitive so i would have expected that the wheel would have been \none of the deciding factors right so if i occlude the wheel the probability should drop \ndrastically but the other way of looking that it is that its really learn ing a lot of \nredundant features so it is not heavily relying on the wheel unlike in the dog case even \nif the wheel is occluded it is relying on certain other features which look like cars and \nhence the probability is not dropping drastically right \nso this allows you to interpret what kind of things it is running so if its heavily for \nexample for face detection if its heavily relying on nose to detect the face to say that this \nis the face the moment you block the nose it will drop its probability of  detecting this as \na face but thats not good right because you want these redundant features remember \nwe had discussed this at some other point where it should try to detect the face not only \nfrom the nose but also from the ears from the hair from the eyes and so on \nso if your occlusion is not drastically reducing your probability that means it has \nlearned some redundant features which are still allowing it to operate well even though \ncertain portions of the image are not there that means it is m ore robust noise in that \ncase right and here it looks like it is not so robust because it is probably heavy this is \nthe rearview mirror of the car so it is probably heavily relying on that feature to detect a \ncar ok \nthen this is another thing where the true label is an afghan hound and for some reason \nif you occlude the face of the woman its probability decreases now let us not comment \non that but if you go back and look at the image you might be able to make some \nobservations right so these are  things so this is an indicator that is probably not really \nlearnt it well maybe all the afghan hound images that it saw maybe a woman was \ncarrying the dog always right \nso its learn this wrong association that when i see a woman with some object it tha t is \nthe portion which is the dog which is bad right so now you can see that your network \nhas not learned something interesting and you would want so if you look at one \nnetwork which is predicting a dog based on this kind of a occlusion and another n etwork \nwhich is predicting a dog based on this occlusion then you would prefer the other one \nand so this is a very interesting experiment to do"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.3 Occlusion experiments.wav", "duration": 340.68, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 ninetysix \nfinding influence of input pixels using backpropagation \nso now so far  what we have done is we  have seen the influence of neurons on the or \nwhat image patches  cause a neuron to fire  then we have visualized the weights so \nneurons have been visualized  weights have been  visualized then we have done some \nocclusion experiments on the input  image now we will  take this further and we are \ninterested in seeing that what  pixels in the image  actually help in the output  or in any \nneuron in the intermediate layers and we will find out some principal way of finding this \ninfluence right and we are going to use bac k propagation that means we are going to \nuse gradients ok \nrefer slide time zerofifty \nso we can think of an image as an m cross n inputs going from x zero xone all the way up to x \nm \n\uf0b4  n nothing great about that and we are interested in finding the influence of each of \nthese inputs on a given neuron ok now what is one way of computing influence that \nyou have learned in this course what is the hero of this course gradients right so \ngradients tell you the influence so now can you tell me if i want to compute what is the \ninfluence of this neuron or this input on this neuron what would you do \nstudent refer time onetwentyseven \n\uf0b6\nhj\n\uf0b6 xi but can you compute that how will you compute that how d o you compute the \ngradient with respect to the input we have always stopped at the last hidden layer and \nthe weights before that so how will we do that how will you do this ok this is a trick \nquestion just a hint is there a restriction on the chain r ule or can you do it you can just \nkeep adding links to the chain right so what is so difficult about that you already \nknow how to compute the gradients till this point and in fact you will also know how to \ncompute the gradients till this point \nand what is stopping you from doing it up to this point what if i just call this h instead \nof x then you would not have a problem right and actually we call it h right we call it \nhzero we can do it right it is straightforward so let us see \nrefer slide time twoeleven \n \nrefer slide time twofifteen \n \nif i want to compute \n\uf0b6 hj\n\uf0b6 xi i can see that if the if \n\uf0b6 hj\n\uf0b6 xi is zero that means this pixel \nhas zero input on the neuron if it is large then it has a high influence if it is small then it \nhas a low influence so this is how i will see whether a pixel has an influence of the \ncertain neurons in the in some of the hidden layers and this is not restricted to \nconvolutional neural networks as you can see i am just actually treating it like a feed \nforward neural network with parse connections ok \nso we could just compute these partial derivatives and visualize this gradient itself as an \nimage so what do i mean by tha t is i am going to compute \n\uf0b6 hi\n\uf0b6 xzero \n\uf0b6 hi\n\uf0b6 xone all the \nway up to \n\uf0b6 hi\n\uf0b6 x mn right so i am going to co mpute this m cross n entities and i can \njust visualize this as an image now what do you expect this image to look like if zero \nrepresents gray colour what do you expect this image to largely look like what would \nyou actually hope for this image should be  largely gray because most of the input \npixels should not be influencing a given pixel in the hidden layer right that pixel should \ninfluence by only a small number of pixels \nso that we can say that this is the patch which causes it to fire and not tha t every pixel \nin the input is causing it to fire because that is meaningless so that does not that is not \nsomething that we care about how many if you get this please raise your hands so i \nwill just repeat it if a pixel fires for every pixel if a ne uron in the hidden layer is \ninfluenced by all the pixels in the input that means it is not really discriminating it is \nnot really specialized right we want neurons which fire only for certain patches in the \ninput so that we know that this neuron is responsible for this kind of a pattern ok \nso if i plot this as a image i would want most of these entries to be close to zero right \nbecause i want the influence to be zero ok \nrefer slide time fourfour \n \nnow the question is how do we compute these gradients so we will just treat them as a \nfree forward neural network we already know how to do back propagation across these \nroots and we just need to add one more term to the chain right so i will just show you \nwhat we will do here so i am interested in \n\uf0b6 hthirtytwo\n\uf0b6 xtwo so i will observe that th ere are four \npaths which go from hthirtytwo to xtwo or rather from xtwo to hthirtytwo so i will just sum up the gradients \nalong these four paths right and if i solve it i will just be left with this ok so that is how i \nwill visualize so this is very simple we have done a lot of gradients in class so you can \njust go back and check this and it should work out well ok \nso you can just see this and this way we can just compute the gradients for  all the input \npixels \nrefer slide time fourfiftythree \n \nand now i am going to plot it as a image and this is what my image looks like do you \nsee what is happening here its all very murky right most of it is great that is fine we \nexpected it but there is not hing really standing out right even in this patch where you \nhave some non gray pixels it is almost like the entire cat region is appearing as non gray \nthe influences are not coming out to be very sharp we would have wanted something \nlike only the eye p ixels cause some neuron to fire or only the ear pixels cause some \nneuron to fire and that is not really happening ok so it does not produce very sharp \ninfluences so someone proposed something known as guided back propagation which \nwe are going to see next and that helps you to better understand the influence of the input \npixels"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.4 Finding influence of input pixels using backpropagation.wav", "duration": 332.02, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  ninetyseven \nguided backpropagation \nso we will see wha t guided backpropagati on i s s o idea here is a bit hacky a bist \nheuristically but it still works very well so let us see what it is right \nrefer slide time zerotwenty \nso suppose you feed an input image to a convolutional neural network that image will \ngo through all the convolution layers and say it one convolution layer thi s is what your \nfeature map looks like i am operating at a very small scale i am just considering a two \n\uf0b4  two \nfeature map ok  \nnow we consider one neuron in some feature map at some layer ok so we will consider \nthis particular neuron and we are finding interested in finding the influence of the input \non this neutron so this is what i will do is i will set all the other neurons in this layer to \nzero because i do not care about them i only care about this particular neurons i just focus \non that  \nrefer slide time onezero \n \nand we now back propagate all the way back to the image right that means i will \ncompute if i call this as htwo then i will compute \n\uf0b6 htwo\n\uf0b4\n\uf0b6 ioneitwoithree and so on ok  \nnow recall that during forward pass what happens is because you have relu neurons any \noutput which was negative that was clamped to zero in the forward pass any output which \nwas negative was clamped to zero so what would happen to th e gradients when they flow \nback through those neurons you already did this if an relu neuron is dead the gradients \ndo not flow back right so the gradients will not flow back through these neurons that \nmeans that only the so only these gradients will actually flow back which correspond to \nnon negative entries in the image before it or in the matrix above it right is that fine  \nso now these guys use this interesting idea that in the forward pass you dont allow \nnegative things to go forward so th e backward pass also do something similar dont \nallow the negative influences to go back that means any gradient which is negative just \nclamp it to zero ok so what i am going to do is all these negative elements in the gradient \ni am going to set them to  zero you see that so this is just taking the same idea which you \napply that forward propagation that relu clamps the output to zero if the influence was \nnegative and the backward pass also do the same any gradients which are negative just \nclammed them to zero  \nso the intuition here was that maybe there was a pixel which is really influencing the \nparticular neuron and it stands out but because there are some positive and negative \ngradients flowing back they seem to cancel each other and all these influence s tend to \nbe zero because thats what we observe that image was largely gray with very few non gray \npixels  \nso this is very heuristically because the reason i call it a heuristic is because you are \nmessing with the math right the math tells you that the correct gradient has to go back \nirrespective of whether its positive or negative but they give this justification that on \nbased on two things and the forward pass you are not passing the negative gradients a \nnegative outputs so in the backward pass also  kill them and this should avoid this \ncanceling of positive and negative output \nso this is known as guided back propagation because you are meddling with the actual \nback propagation you are doing something different  \nrefer slide time threetwentyfour \n \nand s o the idea was to neglect all the negative influences and when they apply this \nguided back propagation this is what the influence looks like so you see that it is much \nsharper now it is actually very nice its focusing completely on the eyes and you can  see \nthe layout of the cat much more clearly as in the earlier picture earlier image right  \nso this is a popular technique to use to for various things it is also among other things \nfor in for understanding what your convolutional neural network is doin g right so this \nlecture is entirely about understanding what are the neurons learning what are the weight \nmatrices learning what are the kernels learning and so on so these are all again tricks \nthat you need to have in your repository to be able to d o something more than just \nreporting accuracy ok i will get seventy percent accuracy on this status refer slide time \nfourfifteen right so this guided back propagation is one algorithm that you will implement \nas a part of the assignment so"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.5 Guided Backpropagation.wav", "duration": 251.52, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlectureninetyeight \noptimization over images \nok the next thing that we are going to do is optimization over images so this is again \ninteresting a nd it eventually led to thi s whole fie ld of adversaria l dee p learning or \nadversarial machine learning in general right so we will see what this is  \nrefer slide time zerotwentysix \nsuppose i have a trained convolutional neural network ok and now i wa nt to figure out \nwhat kind of image should i pass through this so that it gets recognized as a dumbbell \nwhy we want to do i would not want to have such a weird objective can you think of a \nreason why would want such a weird objective i know there is a convolutional neural \nnetwork which can distinguish k classes these classes could be anything  \nnow i want to deliberately create images which get passed as the dumbbell class why \nwould i want to do this ok you are going into your details so i will  give you a \napplication right suppose this network is supposed to do face detection and the k classes \nwhich are there are k people right now you want to see what kind of image should i \nfeed to this so that i get recognized as amitabh bacchan right so  now that could have \ncertain benefits and various high places and so on its i would want to do that right  \nso thats the whole idea behind adversarial learning so now i am asking this question \nthat i want and here its in of course a toy setup there  is no reason i why i would want \nto generate dumbbells but say if i am going to if its an automatic verification whether \nmy product looks like a dumbbell or not i might want to do this right so you could \nthink of all sorts of reasons why you want to d o this so what we will do is the question \nthat we are interested in is that i have a blank slate with me it just contains some pixels \ni want to be able to modify this pixel so that my class dumbbell class gets fired  \nnow we have done enough gradie nts enough back propagation everything in this class \nso i will ask you to give me a solution for this and the hint is treat the image itself as a \nparameter matrix the second hint is assume that all of this is going to remain constant \nyou are not going to change any of this and you have initialized your parameters which \nis the image pixels to zeros that means you are started with a gray image  \nnow i will change the question a bit only a bit and all of you will be able to answer this \nok suppose my network is strained and now i want to change the weights in this layer \nso that my accuracy improves so that when its a dumbbell class it predicts dumbbell \nhow will you do that it will pass the same image what will you do how will you \nchange the we ights in this layer back propagation what is the update rule say the \ngradient descent update rule say that the gradient descent update rule \nstudent refer time threeone \nw is equal to ok you guys actually unanimously said gradient is an update rule ok so w \nis equal to w minus oops oops oops ok minus eta into ok \nstudent refer time threesixteen \nthats what you will do now if i ask you the question for this you can answer it but if i \nask you the same question here why cannot you answer it  \nstudent refer time threetwentyseven \nso here what were you doing computing the gradients of the loss with respect to the \nweights what will you do here \nstudent refer time threethirtythree \nit put respect to each of these pixels and then update this pixel by using what formula \nstudent refer time threethirtynine \nione thats the first pixel is equal to ione minus eta gradient ione where what is gradient ione \nactually everyone gets the intuition right you can do it now  \nrefer slide time threefiftynine \n \nso we could pose this as an optimization problem where what we want to do is given an \nimage we want to maximize the score of the output class and i also want some \nregularization because whatever i get i want it to look like an image right \nso we will see different types of regularization fo r doing this some very simple \nregularizations but this is the overall idea right so any generic loss function is always \nthe training loss plus the regularization so i have just kept both the training loss as well \nas the regularization whats my training loss the score for the class that i am interested \nin and what are the parameters of this object of this optimization problem the input \npixels right  \nso far we had already be always been doing w b but instead of w  b you now have i as \nthe parameters of your optimization problem is that fine \nrefer slide time zerofortysix \n \nand now we can just think of the entire image as a collection of parameters and we can \nnow update the weights of this matrix which is the image matrix ok \nrefer slide time fourfiftyeight \n \nso let us see how we will do it so we start with a zero image as i said set the score vector \nto all zeros and one for the class that i am interested in ok  \nnow compute the gradient of this score vector with respect to i k its i want this quantity \nto be maximized everything else to be zero so thats what my loss function is so i am \ngoing to compute the gradient of each of the pixels with this now i am going to update \nthe pixel using my gradient descent rule which i just explained brief previously \nnow i again do a forward so now instead of this zero image i have a modified image \nslightly modified image because the pixels i have moved away from zero update based on \nthe gradients now this image i will pass back through the network and what will i do \nnow again change so this is the same as the weight matrix right so you should be able \nto visualize it exactly the same way as you would have visualized this you had certain \nweights here you change them a bit again did the forward pass again did the backwa rd \npass change them a bit and keep doing this till \n student refer time fivefiftynine \ntill convergence right whatever is your definition for convergence till you are satisfied \nand instead of score of one you are at least getting a score of zeronine or zeroninetyfive or somet hing like \nthat right so we will keep doing this right till convergence at the end you will you all \nof you can imagine that this image will keep getting modified ok  \nrefer slide time sixtwenty \n \nso now let us see if we learn run this score or the run t his code for certain classes so i \nmean interested in the dumbbell class and i have ran that algorithm starting with the zero \nimage and this is the kind of image that i end up with you see a dumbbell here without \nme drawing it right if you go back and look at it you will see that there are a lot of these \ndumbbell like shapes which have actually appeared here the colour is of course very \nmuch different i dont think dumbbells are of these colours ever but you can see that its \nactually trying to produce that shapes which will cause the dumbbell output to fire  \nnow what is interesting is that its being very redundant so its not trying to generate a \nsingle dumbbell a generating a lot of dumbbells of different orientations so i just \nkeeping its basis covered so that some of this should actually fire and cause a dumbbell \noutput to be maximized ok  \nrefer slide time sevenfourteen \n \nnow let us see if we take a cup and this is like the trophy cup i believe so this is what \nis appearing here there is one mo re cup here and there is one more cup here its a \ngenerating these cups so that you cant be you would not be able to see it its different oh \nit really looks like i am manipulating it but i am not you can go back in check it those \ncups are there ok  \nrefer slide time sevenfortyone \n \nand then for dalmatian actually this at least you can see some white and black spots \nright at least thats fine \nso dalmatians are these dog which have these white and black spots so and you can \nalso see some kind of a shape  here right which with my drawing so it is actually \nproducing that doglike shape and its producing multiple of those so its being redundant \ni am trying to compute that right  \nrefer slide time eightfour \n \nand now you see right with these very arbitrary images which to you and me do not \nknow nowhere close to we will fire will classify this as dalmatian but for the machine \nand is classifying this as a dalmatian and this is bad right this is not good there is \nnothing to be impressed about this is actual ly bad because i can give it these horrible \nimages and still get away by something called as a dalmatian  \nso if i want to sell some a dalmatian on olx this is what i can do right i can upload \nthis image and a machine would trigger it and some one would  buy it ok so and this is a \nbell paper so you can go back and see you see a lot of bell papers here and similar for \nlemon and so on right \n refer slide time eightfortyone \n \n refer slide time eightfortyfive \n \nso various classes you can see that its actually trying to produce those shapes but its \nnowhere actually producing a clear image which is undoubtedly of that object right is \ngenerating something which can later on be used to fool the network right which is not a \ngood thing ok and we can actually do this fo r any arbitrary neuron so i was trying to \nactually fire this neuron which was the output layer but maybe i want something else to \nfire here so i want to actually see what is it that causes this neuron to fire so i could \nrepeat the same algorithm by setting something here as high and then again back \npropagating the gradients only from here and reconstructing the image every time so \nthat this neuron then five is right  \nrefer slide time ninetwentyfive \n \nso these are what the updated images look like which e xcite certain neurons and some \nlayers so what does this look like its actually like a pirates ship if its not very clear \nyou have these multiple layers of things and something like this ok so its some neurons \nare actually firing for this kind of a pattern there are some other neurons which are firing \nfor different kinds of patterns and so on right  \nso you can just create images which cause certain neurons to fire and all these are lot of \nfun to do so you should i would encourage you to do this i  will get more insights into \nwhat your network is"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.6 Optimization over images.wav", "duration": 587.52, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlectureninetynine \ncreate images from embeddings \nthe next thing that we will see is how do you create images from embedding so let me \nsee what that means  \nrefer slide time zeroeighteen \nso remember that each of these things can be thought of as an embedding of the image \nright because you had this original image which was two hundred and twentyseven\n\uf0b4 two hundred and twentyseven dimensional and now you \nhave a four thousand and ninetysix representation for that or a two hundred and fiftysix \n\uf0b4  seven \n\uf0b4  seven representation for that so you could \njust flatten it as an out as a vector and you could treat that as a embedding for the original \nimage right \nnow for any kind of embedding or hidden representation what do we always want from \nthat representation think auto encoders it should capture all the important \ncharacteristics of the original image and in particular i should be able to dash the original \nimage from it \nstudent refer time onethree \nwe construct the original image from it right so thats what i would want from a good \nembedding so let us see if we can do this right so find an image this is the \noptimization problem that i am interested in find an image  such that its embedding a \nsimilar to a given embedding what do i mean by that is suppose i take a monkey \nimage and pass it through all these layers and compute all these embeddings right \nnow again i start with a blank image and my optimization probl em is such that for this \nblank image i want to modify it so again this blank image is my parameter matrix and i \nwant to modify it such that the embedding that it produces should be similar to the \nembeddings that the monkey image produced so how can yo u set this as a optimization \nproblem what would your loss function be so lets call the original monkey images ione \nand let us call this as embedding of ione  \nnow can you tell me what the objective function would be for the new image that you \nare trying to create this entry the first entry in its output that let me call that e i two ok so \ne i two one and ei one one that means the first dimension of the embedding they should both be \nstudent very \nvery close so in such cases what is the error function that we will choose \nstudent refer time twotwentynine \nrefer time twothirty right so you have to get comfortable with designing these loss \nfunctions right so you have seen you have seen this loss function before we just have to \nbe able to related to the problem that you are trying to work on  \nrefer slide time twoforty \n \nso let phi zero be the embedding of the image of interest let x be a random image and we \nwill report the repeat this forward pass using x and compute phi of x right that means \nwere computing the embedding of this random image that we have started with then we \ncompute this loss function and add appropriate regularization for that and that propagate \nand update what what will you update \nstudent refer time threesix \nimage right you will update your  x matrix right and you will keep doing this till \nconvergence \nrefer slide time threethirteen \n \nand let us see what happens so its  suppose so now what i am trying to do is this is my \noriginal image and i have the convolution one embedding of it so in this i am using \nconvolution one as the embedding and then i am trying to solve this optimization \nproblem to recreate x such that its very close to the original image \nso let us see what are the different outputs that i get so this is the original image and  \non the right hand side you have the reconstructed image such that the conv one embedding \nof both the images is the same so you can see that when i am trying to do a \nreconstruction from the conv one layer i get almost the same image back now if i keep \ndoing it from different layers what do you expect it to be if i do it from conv two conv three \nconv four and so on \nstudent refer time threefiftynine \nit wont be so accurate right so let us see what happens if i try to reconstruct it from \nconv two \nrefer slide time fourfive \n \nah relu one \nrefer slide time fourseven \n \nmax pooling \nrefer slide time fournine \n \nnorm one \nrefer slide time fournine \n \nconv two \nrefer slide time oneten \n \nrelu two i am keep i am going deeper and deeper into the network so what i am trying to \ndo here is  remember that i have different choices for these embeddings so the first \nthing which i showed you was when i was trying to the first thing was when i was trying \nto set my objective function such that i am trying to map this embedding the second \nimage that i showed you was when i was trying to map this embedding and the last \nimage that i will show is when i was trying to map these to embedding so my \nobjective function was to create an image such that this embedding of the created image \nis the same  as this embedding of the original monkey image right so thats what i am \nprogressively trying to do \nrefer slide time fourfiftytwo \n \nas you can see as i keep going ahead i get more and more abstracter reconstructions and \ni dont really get the monkey back \nrefer slide time fourfiftyseven \n \nand once i go to the last fc six or f seven layers i get very weird looking reconstructions \nrefer slide time fivezero \n \nand thats expected right because by that layer they have completely abstracted it out \nright you have just probably captured there is something like a nose something like eyes \nor some for here and there but you have loss the entire shape and other characteristics of \nthe original image right from the deeper layer the construction would not be that good \nand thats kind of expected right \nrefer slide time fivetwentyone \n \nin spite of having the maximum no you could right a maximal operation is just another \nembedding which is that the compression there is much more because you have ignored \nthe four entries and just taken the  max value so it becomes harder and harder to do that \nbut mean you i wouldnt call this as a reconstruction right what you see here is not \nexcept for the conv one and conv two layers the rest of the things were not really such \naccurate reconstruction so just says that you are losing a lot of information in that \nabstraction or maybe not i will do it next time"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.7 Create images from embeddings.wav", "duration": 339.6, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred \ndeep dream \nso we will start so we were in the threerd lecture  on cnn\u2019s where we were looking at \ndifferent visualization tools for understanding what your convolutional neural network is \nlearning and we did a bunch of things and now you move on to the next module where \nwe talk about something known as deep dream very interestingly titled but i am sure \nmost of you have already seen this or read about this \nrefer slide time zerothirtyfour \nso here is the idea right so far we were seeing that if we start from a blank image then \nwe could suitably modify it by constructing an optimization problem whose parameters \nare the pixels o f the image and we can modify the image so that it starts looking like a \ncertain class of interest right but now suppose instead of starting with a blank image i \nstart with a natural image right say a sky or any image that you have in your dataset \ni start with this and then i focus on neurons in some layer of the convolutional neural \nnetwork i am focusing on these neurons say any one of these neurons i am focusing on \nand i want to change the image so that these neurons so when i say neurons i actuall y \nmean only a single neuron but for illustration i will show multiple neurons so i want to \nchange the image so that this neuron fires even more so how would we achieve this \nwhat will we do  \nso say this is the neuron which i want to fire even more so what is my optimization \nproblem first of all what are the parameters of the optimization problem \nstudent refer time onefiftyone \nthe pixels of the image that is clear now i want this to fire even more so what is the \nobjective function what you are  going to maximize lets call this neuron as h ij what \nyou are going to maximize  \nstudent refer time twoeighteen \nsorry \nstudent no refer time twotwentyone  \nno i want this neuron to fire more \nstudent refer time twotwentyseven \nmaximize hij right i mean that is i mean why so that sort of a thing ok \nbut of course we will do something so that it is a neat differentiable thing and so on so \nyou want to maximize the activation of one such neuron h ij so we could just formulate \nthe following optimization problem that i want to maximize h ijtwo ok and of course the \nparameters of the optimization are the image pixels and if i consider one such pixel in \nthe image then i essentially need to compute this gradient \nthe gradient of the loss function which is hijtwo with respect to this image pixel and i can \ndo it in these two parts the lead ability of the loss function with respect to h ij and the \nderivative of the h ij with respect to the image pixel this we have seen a million times \nwhile doing back propagation of course  you are not gone  all the way back to i mn but \nwe saw last time that it is just one more term in the chain rule and this again looks \nstraightforward right the derivative of the loss function with respect to h ij looks straight \nforward so i have a very si mple way of computing the derivative of the loss function \nwith respect to any pixel of the image  \nrefer slide time threethirtyone \n \nso now i can apply gradient descent and i can update the image so i started and now \nremember that the my original i mn was no t blank or random or zero or anything it is \nactually the sky image so maybe it was blue or cloudy or whatever pixel that i have in \nmy original image and that pixel i am changing  \nso i have started with the sky image i have changed a bit based on this grad ient update \nrule gradient descent update rule and now i feed it back to the network what will \nhappen what will happen to hij  \nstudent fire a bit \nit will fire a bit more because that is exactly how you have changed the image with \nexactly that objective function right \nand now if i keep doing this what will happen so remember what does h ij actually \ncapture now this is where so if you understand this right you will really understand and \nappreciate everything about convolutional neural network and i will be sure that you \nare actually understood the details and not just these boxed architectures right so what \nif this happens right then what does actually hij capture it captures certain \nstudent patterns  \npatterns in the image right now if hij is firing that means these patterns have started \nstudent refer time fourfiftyseven \nappearing in the image we started with a sky image \nand now hij is firing more and more that means it is now the image is suddenly \nbecoming more and more or containing more and more patterns for which h ij should \nfire does that make sense ok yeah so let us run this algorithm we will start with this \nimage and we will run this algorithm so i will run it before that i want some guesses \nwhat kind of patterns do you think will start appearing here and this is deep dream is the \ntitle right ok so fine \nso let us see so i will run this algorithm so what i am doing is i am starting with this \nimage and running exactly what i showed you that i will compute the gradient with \nrespect to one of the neurons and i will keep updating the image so that it becomes \nmore and more like the patterns that i am trying to capture so lets run this and observe \ncarefully it is almost a magic trick i hope this does not disappoint what do you see \nstudent refer time fivefiftyone \nmost of them are what \nstudent refer time fivefiftythree \nthey are dreaming so they are literally building castles in the air right so what is \nhappening why is this happening everyone sees castles right that is the first thing \notherwise  \nstudent laughter \nok good why is this happening have you seen the disney logo the castle what does it \nhave in back background how many of you find this interesting how many think this is \nok expected ok why is this happening th ink about training data think about what \nwould have happen or you missed the magic show so what is the convolutional neural \nnetwork actually trying to do \nstudent refer time sixthirtytwo \ni will give you a hint its being over enthusiastic how many of you get that ok so here \nis what is happening right should i explain it or no i am not going to ask you a quiz \nquestion i am just saying that i have some more images to show ok i will explain it first \nso this is what is happening right \nso in the trainin g data whenever the castle appears it is typically has the sky as the \nbackground ok so now the convolutional neural network started drawing these \ncorrelations so whenever it sees a sky it is trying to find a castle somewhere but \nbecause it knows that m ost of the times whenever i see a sky there is a castle in the \nforeground \nso those neurons are firing a bit and then now you are trying to fire them even more and \nmore so that keep trying to change the image till this castle actually appears on the \nimage how do you how many if you get this explanation please raise your hands ok \nso let us see some more examples right so now guess what will happen here ships ok  \nagain a generation which thinks of \nstudent refer time seventhirtyeight  \na ships is ok i shouldnt comment on that \nstudent birds  \nfishes  refer time sevenfortythree  \nstudent birds \nbirds what else but there are also mountains  \nstudent  refer time sevenfortyseven  \nice ok interesting \nstudent  refer time sevenfifty  \nnow our expectations are increase l et me just run this and see what happens oops oh \nno \nstudent refer time sevenfiftysix  \n i have my final trick ok \nstudent laughter \nthe prestige is gone ok yeah so what do you see here so actually if you go back and \nlook at it carefully right this is ve ry interesting a lot of fish eyes actually start appearing \nhere and some shapes like fishes actually start appearing here go back and look at it \ncarefully and all on the mountains and the green regions a lot of birds and animals start \nappearing right whic h is again expected because in your data set you would have seen \nbirds and animals with a green or this kind of a background right whatever you call it a \nmix of green and brown background right \nso now it is trying to find those things even though they d o not exist and as it try to \nforce it more and more it starts creating those images as you start asking to dream more \nand more right and since this is about dreams i could not let this go it has to had \ninception in that so what will happen here now \nstudent refer time eightfiftyone  \nthere is actually nothing interesting is this for my own sake that i put this unfortunately \nnothing interesting happened with this \nstudent oscars \nwow \nstudent laughter \nif only but thats the point right this is so data se t specific that it cant really generalize it \ncannot dream beyond the data set actually nothing interesting happens it is just a lot of \nthese men are wearing brown suits and in the data set unfortunately all brown things \nwere dogs laughter so this is w hat will happen we will start seeing dogs appear \neverywhere you see one here \nstudent laughter \nyou see many here actually  \nstudent refer time ninethirtyone  \nit is like a few more and this would have turned into laughter something unpleasant \nright so that is what is happening actually see a lot of dogs here here in many places \nright \nso this is its still running so what exactly is happening here the same thing that i had \ndetected right the network has been trained to detect certain patterns dogs  cats birds \netcetera which appear frequently in the imagenet data and with these backgrounds that i \nam trying to do or these textures that i have in my images it starts seeing these patterns \neven when they hardly exist and now as i start focusing on thes e neurons which are \nfiring and try to modify the image to make them fire even more it will start producing \nthese pixels or these images in the original image right \nso you can read this explanation which is from the google blog on this they have some \nreally some code and something on this so you can just read this explanation if a cloud \nlooks a little bit like a bird so that will make it look more like a bird this in turn will \nmake the network recognize the bird even more strongly on the next pass an d so forth \nuntil out of nowhere a bird actually starts appearing in the image right so that is exactly \nwhat is happening so this is deep dream"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.8 Deep Dream.wav", "duration": 638.48, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlectureone hundred and one \ndeep art \n ok now we will go to deep art now here any questions on that ok \nrefer slide time zeronineteen \nso now here is what here is a again an iq test right so what will happen ok so this is \ndeep art ok someone wanted to try this that if you take natural images or camera images \nand if you have art from various famous artists and i want to render this original image in \nthis art form and how can i do so i will explain this the bit of a leap of faith in what \nis happening here but just indulge me right so let us see \nrefer slide time zerofortyseven \n \nso to design a network which can actually do this we design we first  define two \nquantities one is the content targets so i call this image as the content image because \nthis is the content that you are interested in right i want my final content to look like \nthis for the content we would want the following thing that if i am able to create a new \nimage when i pass it through the same convolutional neural network we want these \nhidden representations to be equal right because that is the assumption here is that the \nhidden representation actually captured the essence of the image which is this face and it \nis various attributes right \nso if i create a new image in a different style still this content should be present in it \nand my way of ensuring that or rather the way of the author\u2019s way of ensuring this was \nto make sure that the embeddings that i learn for the new image and the original image \nare the same ok so i want these to be equal and i have just shown one for illustration \nbut you could have the same objective function for all the representations right \nremember that we learn multiple representations and a convolutional neural network \nso this is what my objective function would be for the content i would want that this \ntensor which is the volume ijk every pixel or every feature value in the tensor for th e \noriginal image should be the same as the generated image ok and again my optimization \nproblem is with respect to what image i am going to change the image and this is the \nloss function that i am interested in is that fine ok fine \nso i think x is m y original image and p is the new image which i am going to create \nright  \nrefer slide time twotwentyseven \n \nnow next and here is where there is a bit of leap of faith we want the style of the \ngenerated image to be the same as the style image so i gave you one content image and one \nstyle image so for content the loss function is clear now for style how do you capture \nthe style of the image so the explanation given here and i am not very sure about this \nbut maybe it comes from some traditional computer vis ion literature but i just take it on \nfaith that if you have this volume here which is say sixtyfour \n\uf0b4  two hundred and fiftysix \n\uf0b4  two hundred and fiftysix or any other \ndimension right then v t v which is a sixtyfour \n\uf0b4  sixtyfour dimensional image or matrix captures \nthe style of the image so this is what has been written in the original paper i am not \nreally dug deep but my feeling is it comes from some of the traditional literature from \ncomputer vision right so that is not important  we will just take it for granted that that \ngives the image and here is the illustration for that as you go deeper and deeper so this \nis if you plot the sixtyfour cross sixtyfour image that you got then you get different styles as you go \ndeeper and deeper you get a b etter representation of the style of the original image right \nso that is the argument made in the original paper \nnow if you assume that this is correct then can you design a loss function for the style \npart of it i want the style of the created image  to be the same as the style of the style \nimage \nrefer slide time threefiftytwo \n \nso how would i do that so this is the content image this is the actual oh sorry this is a \nstyle image correction ok so i would just want that this v t v which captures the s tyle \nand i could do it for any one of the layers or all layers depending on what i want to do i \njust want that this style should be as close to each other \nso i can have a similar matrix squared error kind of a function right so that is what this \nis trying to capture these are the style gram so this is v t v for the style image and this \nis v t v for the generated image if i pass it through the convolutional neural network i \nwant both of these to match \nrefer slide time fourthirtyseven \n \nso i want the conte nt to match i want the style to match so then what is my total \nobjective function going to be \nstudent sum of these \nsum of these right so this is what my total objective function is going to be i want the \ncontent to match and i also want the style to  match so i will use an objective function \nwhich tries to balance between these two and alpha and beta are some hyper parameters ok \nand if you do this and train the algorithm and try to modify the pixels along with some \nother bunch of tricks then you wil l get this gandalf rendered in this style that you have \ngiven right so this is again some code is available for this you can go and try it out \nand it is interesting it is in a very interesting idea that you could have taken these two \nthings and now you  could be imaginative right you could do all sorts of things with if \nyou have two different images how do you want to combine them and so on right so \nthat is the basic key idea here"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 12.9 Deep Art.wav", "duration": 308.08, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlectureone hundred and two \nfooling deep convolution neural networks \nwith that we g o on to the last module whic h i s fooling dee p convolut ional neural \nnetworks \nrefer slide time zeronineteen \nso turns out that using this idea of optimization where we are able to actually change \nthe image to suit our needs right and these needs were one was we wanted to change \nthe image so that it fires for a particu lar class the other was deep dream where we \nwanted to change the image so that it is starts seeing patterns which were otherwise not \nobserved in the image and the other was d part where we trained the image or we \noptimized over the image so that we c ould produce some artistic images and these are \nthe different optimization problems that we have seen \nbut the same idea can actually also be used to full convolutional neural networks and i \nhave already hinted at this earlier so let us see how to do that \nrefer slide time zerofiftyfive \n \nso now suppose we feed an image to a convnet and i know this is the bus image right \nbut now what i do is this is a trained convolutional neural network and what i do is \ninstead of setting the cross entropy loss to maximiz e bus i will set up the cross entropy \nloss to maximize ostrich and then i will back propagate through the network i will not \nmodify any of these weights or parameters and i am only change the image right \nso what i am trying to do is i know that this is the bus image but now i am setting the \nobjective that it should fire for the ostrich class so now i am going to back propagate \nand change this image so that the blog the likelihood of the ostrich class increases you \nget this set up its very stra ight forward ok and turns out that if you do this with very \nminimal changes to the image you can actually fool the convolutional neural network \nok \nrefer slide time onefifty \n \nso this is the change right you have the original image the second image i s actually the \namount of change you made and the third image is the original image plus this change \nnow to the human eye there is no distinction here right you would all of first would still \nthink this is a bus and in fact i do not even see that there  is a noise in the third bus that \nyou see same for some other class they have taken some bird or something like that and \nadded some noise to it and a temple and in all of these cases the network actually \npredicts that the modified image is an ostrich ri ght or some very random class from the \noriginal class so why is this happening and before asking that question let me just finish \nand it need not be that you start with an original image and then try to modify it \nrefer slide time twothirtyfour \n \nactually yo u can start with a blank image and do the same experiment where you \nmodify the image minimally so that p of robin becomes one or close to one and you will get \nsome very arbitrary noisy looking images which no in which to at least you and me do \nnot look lik e a cheetah or robin or armadillo but the network thinks that these are the \nclasses that these images belong to \nnow this is definitely a risky how many of you appreciate that it is bad ok now and \na network is not just predicting it is predicting it with a very high confidence right ninetyninesix \npercent confidence so why is this happening can even think of a reason for that \nstudent refer slide time threetwelve \nno but ok in that case i would have been fine if there are one thousand classes it should have \ngiven one one thousand probability to all the classes right but this is like worse than random \nclassifier right it is saying with ninetynine percent confidence that this is a ostrich or whatever \nclass that is so why is this happening and the interesting thing is that this in some \nsense ties back to the universal approximation theory or at least some ideas with that \ncan you think of why this is happening ok \nso let us try to see a very intuitive explanation for this so on  \nrefer slide time threefortyfive \n \nso this explanation is due to andree karpathi we need to put the acknowledgments this \nslide does not have any acknowledgments actually so remember that images are \nextremely high dimensional objects right they are two hundred and twentyseven\n\uf0b4 two hundred and twentyseven which is a very high \ndimensional object high dimensional space and no matter how much training data you \nhave you see a only a small sample of this high dimensional space right because its real \nnumbers two hundred and twentyseven\n\uf0b4 two hundred and twentyseven just imagine the number of possibilities out there no matter you have \none million samples ten million sa mples for training this is  much smaller than the actual \nnumber of samples which exist in this space of these only a few are images right \nso now think of all two hundred and twentyseven \n\uf0b4  two hundred and twentyseven matrices that you can make and how many of them are \nactually going to be natural images the probability of natural images is very small most \nof these are random things right they are just matrices which do not make any sense \nwhich actually look like these images that you see here right  \nnow using the training images we fit some decision boundaries and this is the decision \nboundaries that we fit right that this is class one the rest of the green part is class two and so \non and in fact we are doing the se decision boundaries for some one thousand classes while \ndoing so we actually end up taking decisions for a large number of points that we have \nnot seen we have not seen any points in this space but i have made a decision for them \nthat all of them belong to the green class i have not seen any point in this space but i \nhave ended up taking a decision for them that all of them belong to the red class right \nso in particular what i have done is i saw a cheetah class image from a cheetah class i \nsaw a few images from the cheetah class and i drew some boundary around it to say that \nthis is the cheetah class but my boundary also contains images like this because this is \na very high dimensional space and in that boundary a lot of points actually fall in and \nsome of these points are these random points which have no relation to cheetah right \nbut i have been so aggressive in fitting to the training data that i have drawn these \nboundaries which also include a lot of these points and now all i need to do star ting with \nthese rend random images is that go somewhere inside this boundary and then i am all \nset right it will start detecting it as cheetah because the boundaries have been drawn by \nthe classifier how many if you get this explanation good so that is the intuitive \nexplanation for why this happens so this is where we will end the discussion on \nconvolutional neural networks"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 13.1 Sequence Learning Problems.wav", "duration": 483.42, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 thirteen \nsequence learning problems recurrent neural networks backpropagation \nthrough time bptt vanishing and exploding gradients truncated bptt \nin thi s lecture we will ta lk a bout sequence learning proble ms a nd in particular some \nneural network architectures which deal with sequences so recurrent neural networks is \nwhat we are going to see so we will star t with the fir st module which is on sequence \nlearning problem \nrefer slide time zerothirty \nso what are sequence learning problem so so far we have dealt with two types of \nnetworks one is feedforward neural networks and the other is convolution neural \nnetworks and both these networks the input was always of a fixed size \nso what do i mean by that is if you take a convolution neural network you are feeding \nthirtytwo \n\uf0b4  thirtytwo images to it or two hundred and twentyseven \n\uf0b4  two hundred and twentyseven images to it and this size wil l always fixed all your \ntraining images all your test images were always scaled or cropped to this particular \nsize ok similarly when we used feedforward neural networks so one example was \nwordtwovec the size of the input was always fixed we had this in put of size two v right or k \nv in general if you are looking at the k word window right  \nso this input was not varying from one training instance to another training instance or \none training instance to the test instance or anything and secondly each input to the \nnetwork was independent to the previous or future inputs so i pass an image of an apple \ni get the prediction apple then i pass some other image to the network and i get a \ndifferent prediction it does not matter whether my previous image wa s a apple or a car \nor a mango or whatever it just reads each of these inputs independently there is no \ndependence between the inputs and the size of the inputs is fixed  \nrefer slide time onefortysix \n \nbut in many applications the input is not of a fixed siz e so and also successive inputs \nmay not be independent of each other so let us understand this with the example of auto \ncompletion that all of us are used to while typing smss or whatsapp or other things  \nso given the first character d i want to pre dict the next character which is e then once i \nhave predicted e i want to predict the next character again and so on till i get the full \nword ok this is what my task is  \nrefer slide time twotwentyone \n \nso let us notice a few things first successive input s are no longer independent if i \nknow that the previous input was d and the correct input is e then i know that only a few \nthings are possible right in particular if you know that the previous input was a z and \nthe correct input is a e then most likely the next is going to be a b right but if you \nignore the previous input which is z then after e there are many things which can appear \nright so the inputs are no longer independent of each other  \nand the second thing is the length of the input is no t fixed because words could be of \narbitrary sizes i am trying to type the word deep that is four letters or if i am trying to type \nthe learn which is five letters machine which is seven letters and so on right so the input size \nis no longer fixed and the inputs a re now dependent on each other right there is some \ndependence between so now this is very different from what we saw in convolutional \nneural networks and feedforward neural networks so how do we deal with this  \nand the third thing here is that eac h network now i am calling this as a network and i \nwill just clarify some notations also soon each network is actually performing the same \ntask it is taking as input a character and it is producing as output one character and now \njust remember that thes e networks i have drawn them vertically you are used to seeing \nthem as this so this is input this is your hidden layer and this is your output so this is \nthe green part this is the blue part and this is the orange input and this is the fully \nconnected layer right  \nso each of these boxes is actually this network ok i have just drawn it more concisely \nbecause i need to draw many such networks so everyone gets that just remember this \nmind that each of these orange blue green structures is a fully connected network like \nthis  \nrefer slide time fournine \n \nso these problems are known as sequence learning problems where you have a sequence \nof inputs and then you need to produce some outputs and each input actually \ncorresponds to one time step so this is the input at time step one time step two time step three \ntime step four and so on so let us at some more examples of such sequence learning \nproblems  \nrefer slide time fourtwentynine \n \nso one classic example is the task of predicting the part of speech tag  of every word in a \nsentence right so i am given a sentence man is a social animal and for every word i \nwant to predict whether it is a noun or an adverb or an adjective or a verb or any other \npart of speech type right and this is how it happens  \nnow notice that once we see an adjective in this case social we are almost sure that the \nnext what is going to be a noun or at least we are sure that the next word cannot be an \narticle or most likely it will not be a verb right there is a very high prior th at the next \nword is going to be a noun so that is why these inputs are actually dependent on each \nother  \nso the current output not only depends on the current input it is also actually depends on \nthe previous input right unlike the case of convolutio nal neural networks where i \nfeeded an apple it is no dependence on whether the previous input that i pass to the \nnetwork was an apple or a car or what not and the size of the input is not fixed because \nthese sentences could be of arbitrary lengths i could  have sentences as small as three to four \nwords or as long as twentyfive to thirty words right so average wikipedia sentence for example \nis twentyfive words roughly twentyfive words  \nand notice that and this case we are interested in producing an output at every time step \nbecause for  every input i want an output and each network again this orange blue \ngreen structure is performing the same task its taking as input a word and its producing \nan output what is producing as an output part of speech tag \nrefer slide time sixone \n \nso here the two examples that we saw we were having an input and every time step and \nan output at every time step but there could also be cases we are interested in producing \nthe output only at sometime steps or at the final time step so let us consider task of \npredicting the polarity of a movie review right sentiment analysis  \nso i am given a movie review and after i have read the entire review i should give a \nprediction right otherwise it would be incomplete i cannot actually look at only this \nword and give a prediction does not make sense i does not also make sense to make a \nprediction here at this point because there could have been the movie was boring but i \nstill loved it or but the action was amazing or something like that it because it coul d have \nalready flipped after that so i need to look at the entire sentence and make a prediction \nbut you are not interested and prediction as these intermediate time steps  \neven in this case you can actually assume that every network is performing the same task \nits taking a word as an input and it is producing some output it just that till the end you \ndo not care about you outputs you care about the output only produced at the final step \nyou do not care about what the outputs are at this time step rig ht that is one way of \nlooking at it  \nso again at every time step we have the same network but you are only interested in \nsome time steps of the network ok  \nrefer slide time sevenfourteen \n \nfinally it is not always necessary that sequences are composed of only words what \nother kinds of sequences are you familiar popular sequences speech is one video is \nanother \nso a video could be treated as a sequence of images and now you could have a video \nwhere some someone is performing surya namaskar and as you  can understand that i \nneed to look at the entire sequence and only then be able to make a prediction right if i \nstop at this point if i only consider this this is only namaskar no surya namaskar right \nso you have to look at the entire sequence and then decide what the output is and you do \nnot care about the intermediate outputs i do not care what is the prediction till this point \nthis of course is again some aasan but i do not care about that i care about the full \nsequence that i am dealing with it is just to motivate that sequences can be of all types  \nand i apologize to the speech people i do not really understand much of speech \nprocessing so i never give speech examples but video is something i understand so i \ncan give examples on that"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 13.2 Recurrent Neural Networks.wav", "duration": 547.46, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and four \nrecurrent neural networks \nso we have seen sequence learning problems now we are interested in the question of \nhow to model these right so we look at something known as recurrent neural networks \nrefer slide time zerotwentythree \nand our question that we are interested is how do you model tasks which involves such \nsequences ok \nrefer slide time zerotwentyseven \n \nso here is the wishlist that we have what will model will come up with should account \nfor the dependence between inputs because that is the strong case that we have made \nthat the output actually depends on multiple inputs and not just a single input you \nshould also account for variable number of inputs because a video could be three hundred seconds \nit could be twenty seconds twentyfive seconds a sentence could be of arbitrary lines and so on \nand it also makes sure that the function at each time step is the same right but every \ntime step they are trying to do the same activity ok so we will focus on each of these \nitems from our wishlist and then try to arrive at a model for dealing with such problems \nrefer slide time onefive \n \nso first let us ask this question what is the functio n being executed at each time step \nwhat is the function being executed at each time step either should come after dealing \nyou have an input your ability to go blank on me is just amazing actually you have a \nhidden representation and then you have an ou tput the first time we are seeing this \nsituation in the entire course where we have an input a hidden representation and an \noutput \nwhat is the function being executed remember the output is always a function of the \ninput lecture two or three i do not know definitely not lecture fourteen so what is the function \ncan you write y i as a function of x for my sake if not for god sake you can ok what \nis it ok first tell me what is s one u s one no nonlinearity no bias which all that plus bias \nthen no nonlinearity who cares about nonline arities ok and then what is y one ok some \noutput function ok for some reason i have written sigma here i will just call the output \nfunction as o always maybe in this case sigma would work but o is what i will call it \njust to make the this thing clear ok is that fine \nso this is the function being executed at this every time step we can just write it using \nthese two equations which are seeing for a first time and i is a time step since we want \nthe same function to be execut ed at each time step we should share the same network at \nevery time step that means what do i mean by share the same network share the same \nparameters good so this is the same as this because u v and b and c are the same ok \nso that is an easy way of taking care of the requirement that i want the same function to \nbe executed at every time step \nrefer slide time twofiftyone \n \nand this parameter also sharing also ensures that the network becomes agnostic to the \nlength of the input because now whether i have a word which has ten characters or twenty \ncharacters it does not matter because at every time step i am going to execute the same \nfunction that is why it is important that at every time step we have the same function \nso since we are going to complete t he same function the number of times it does not \nmatter and we can just create multiple copies of this network that we have and for any \narbitrary length n we can still compute the output ok still not quite there we still need \nto take care of a lot of th ings but we are just slowly addressing each item from our \nwishlist \nrefer slide time threethirty \n \nnow how do we account for dependence between inputs or rather actually the right \nway of asking this is how do we account for the case that the output actual ly depends on \nmultiple inputs and not just the current input ok how do we account for that feed in \nthe ok good \nso let us first see an infeasible way of doing this ok so you are given the first time \nstep xone you have a network which predicts one from xone you know at the same second time \nstep you also want to look at the previous in puts so why not just feed it x one and y xtwo both \nand then try to predict y two at the third time step feed  in x one x two xthree and predict ythree and so \non forever is this fine proba bly the word infeasible is there so y is so what is the \nproblem with this yeah good so i am looking in terms of the conditions that we have \non the wishlist which condition does is violet what is the function being executed at \neach time step ok so let us see \nrefer slide time fourthirtyone \n \nthe function being executed at every time step is different so yone is function of x one y two is \na function of x one  x two remember that this is not just saying that you are passing to inputs \neverything changes because you now you need to have u one and utwo here you need to have \nu one u two u three right so everything changes it is not the same function how many of you \nget this it is a different function being executed at every time step \nso now if i have a sequence of length one hundred what happens i need y one hundred which takes f x  one \nto x one hundred as inputs and has how many parameters u one to u one hundred right you could you could \nshare u one to u ninetynine for y ninetynine and y one hundred but we still need those many of that right so that \nnetwork is now sensi tive to the length of the input and on the length of the input goes \nyou will have to construct more and more functions right \nand imagine that if the training time the maximum sequence length that you had seen \nwas twentyfive and suddenly a test time you get a se ntence which is of length thirty you do not \neven know how to compute that because you have not train any parameters for doing \nthat \nrefer slide time fivethirtysix \n \nso then the final solution is actually to add a recurrent connection in the network why \ndoes this work ok before that now can you tell me what is the function being executed \nat every time step assume there is a s zero here these are a s one s two s three up to s n and there is \na s zero \nnow what is the function being executed at every time step can you writ e it down if \nyou it would help if you think in terms of y two and not in terms of y one y one is the boundary \ncase was special case the thing in terms of y two or any other of the y\u2019s and first think of \nwhat s two is from s two y is straight forward how many of you can write the function so s i \nin general s i is u into x i plus w into s i minus one plus b how many of you get this and \nthen what is y i again this has to be output functions ok but how does this solve our \nproblem does it take care of everything on the wishlist one the way we have written it \nin terms of i which is the time step definitely the same function is getting executed at \nevery time step there is no doubt about that right modulo this boundary case of s one \nwhere will assume that there is an s zero ok \nso same function being executed at every time step can you deal with inputs of \narbitrary length yes as long as you ensure that the same function is executed its fine \ndoes it ensure that the output is actually dependent on the previous inputs how \nstudent refer time sevenone s i \nthrough s i minus right so that is an interesting thing that this guy actually depends on \nthis guy which depend on the previous input and also on this guy which in turn depends \non the provision inputs so rec ursively you can see that you depend on all the previous \ninputs that you had ok that is a very neat way of ensuring that your output depends on \nall the previous inputs and you do not blow away the parameters blow of the parameters \nby sharing this recurre nt connection and that is this is a compact way of writing is that \nyour y i is now function of x i s i and has these parameters w u v and b and c so s i is \ncalled the state of the network at times step i \nand what is see here this is just for the sak e of completion this is known as a recurrent \nneural network because of this recurrent connection and s i is a state of the network at \ntime step i so as when you start working in deep learning and you are dealing with \nsequence problems state of rnn or sta te of lstm or state of grv something that you \nwill be hearing or reading often so this is what you mean by the state of the recurrent \nneural network this is the current state which kind of encode everything that is happened \nso far right it has a encoding of all the inputs that you had seen so \nthe parameters of the network are w u and v which i shared across time steps i \nobvious forget the biases and the same network is getting executed every time steps i \ndo not need to worry about whether i am comput ing y one y two y three or y one hundred right so \neveryone agrees that this solution takes care of all the things that we had on our wishlist \nhow many of you agree with that \nrefer slide time eightthirtyone \n \nand this is a more compact way of representing that that you sa y that you compute s i \nand then you are feeding it back so this is just more compact way of representing a \nrecurrent neural network \nrefer slide time eightfortyone \n \nso let us now revisit the sequence learning problems that we have seen so now just \ncorrect each of these networks so what would happen each of these things i was \nthinking of all the inputs as independent so now what will i do what is the only thing \nto be done just add the recurrent connection right \nso once you add the recurrent conne ction now you can go back and relate to all these \nproblems that one i am trying to predict the character which appears after e i also have \nthe information of d and e and same argument you can make for all the other examples \nthat you have but i am tryin g to predict this final state i have the information of all the \nprevious inputs here ok"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 13.3 Backpropagation through time.wav", "duration": 823.04, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and five \nbackpropagation through time \nso that was recurrent neural networks now whenever we propose a network what do \nwe do next training right so what we will look at  it back propagation through time \nthis is not the title of a ficti on and movie or anything this is an algorithm that we will \nsee \nrefer slide time zerotwentyseven \nso before we proceed right let us look at the dimension of the parameters that we have \nand i expect you to tell me the dimensions so i will define somethings for you which \nare very hard so x i belongs to r n so let us be clear about that s i belongs to r d that \nmeans the si is a d dimensional vector and yi belongs to rk which has k classes ok \nso now what is u what is v d \n\uf0b4  k is it d \n\uf0b4  k i am asking soham now i mean we \nhave be written it as d \n\uf0b4  k and w is \nstudent refer time onefive \nd \n\uf0b4  t sure everyone sure ok right so these are the dimensions why am i talking \nabout these dimensions whenever we talk about gradients what we talk about partial \nderivatives or gradient or something we  need to know what is the size of the parameter \nwith respect to which we are taking the gradient because that is what the size of the \ngradient matrix is going to be right that is why i am asking you to focus on this \nrefer slide time onethirtyone \n \nnow how do we train this network title of the module  \nstudent backpropagation \nbackpropagation ok how why do i have a module if i am only going to tell you about \nbackpropagation do you see any problem with this why cannot you just apply the \nstandard backpropagation algorithm ok so we will try to understand this with the help \nof a concrete example and we will go back to our example of predicting characters ok  \nrefer slide time onefiftyone \n \nso this is the auto completion task and for simplicity we will as sume that english has \nonly these three characters d e p and then a stop to indicate that the word has been \ncompleted ok this is what you are going to consider that my vocabulary size is just four \nthat means i can only predict one of these k four classes k is equal to four ok  \nand at each time say i want to predict one of these things what is the suitable output \nfunction for this task can everyone say with probability ninetyninenine percent \nstudent half max \nhalf max ok what is the suitable loss function for this tas k small pleasures in life that \nis all i get ok \nrefer slide time twothirtyfour \n \nsuppose we initialize u v w randomly and networks predicts the following \nprobabilities ok so let us understand what is happening i fed it d as the input i have \njust started  training so my u w and v are all some randomly initialized weight \nmatrices right now and so it has predicted this as my probability distribution this is the \npredictions that i have got from the network  \nand i also know what is the true probability distribution what is the true probability \ndistribution for the first time step zero one zero zero and so on right you can see it second times \nthat is also zero one zero zero third is zero zero one zero and the last one should have been zero zero so given the \nsituation and before i talk about learning algorithms what is the first thing that i need to \ndefine objective function right so what is the objective function here how many \nerrors do i have i mean i can make my errors at four places whether i making an error or \nnot is the separate case but i can have four loss functions \nso then these are the two questions that i am interested in what is the total loss made \nby the model and how do we back propagate this loss and update the parameters of the \nmodel as usual i am ignoring the biases which is w u and v so we can answer these \ntwo questions then we are done right if you can do this then we are done \nrefer slide time threefiftythree \n \nso the total loss what is the total loss actually take a guess sum of all the loss right \ngood so jus t going to be the sum of the loss over the times steps that you are i mean \nvery logical and what else would it be and we know that the loss at every time step is \nso this is the loss at time step t hence y t and what is c actually the true class at time  \nstep t right so it is would be e at first time step e at second time step then p and then \nstop ok so that what c is \nso this is we all comfortable with is this is the cross into p loss and i am going to sum at \nover all the t time setup that i have  now for back propagation what we need is we need \nto be able to compute the gradient of this loss function with respect to w u v  \nrefer slide time fourfortyeight \n \nif i give you your formula for the gradient the rest is straight forward you will just apply \ngradient as well ok so let us look at each of these parameters we will look at the easy \none first which is v so what is the derivative of the lost function with respect to v \nhave you ever done this in life \nstudent yes \nyes \nwhen \nstudent refer time fourfiftyfive \nnow i am asking the date ok so you have done this when you doing backpropagation \nthis is the gradient of the loss function with respect to the weights in the output layer \nand we know how to do that right that is very straightforward and  there is no \ncomplication there and you will see what i mean by complication later on \nso all i need to do is take this loss function and compute its gradient with respect to v it \nis very simple chain rule which i can update there apply there and i can  compute it \nseparately for all these guys and i can just sum it up right so this is the easy part this \nis very straight forward so where one parameter we are all set we know how to do that \nright we can just add up all these gradients the some los e notation here this is actually \nan addition of four matrices right each of this i hope is a matrix is that a matrix or a scalar \nor a vector or a tensor \nstudent matrix \nmatrix ok so we have already seen how to do this back propagation and this is a \nsmallest chain possible in the back propagation and we have enough confidence in doing \nthis  \nrefer slide time fivefiftysix \n \nnow let us considered the derivative of the loss function with respect to w just take a \nminute and see if it is complicated or if it is straight forward to see a lot of w\u2019s in the \nfigure ok so let us see how to do that right  \nso again the loss with respect to w or the derivative with respect to the loss derivative \nof the loss with respect to w is going to just be the sum of these four or t derivatives and \nby changed of derivatives we can just sum the derivative across all the paths which lead \nfrom the loss function to w is that fine right whenever you want to compute the \nderivative of the loss function with respect to any paramet er a recipes to look at all the \npaths which go from the loss function to that parameter and some of the gradients across \nthose paths how many if have fine with this what are the paths which are actually \nconnecting the loss function to w \nstudent refer time sixfiftyone \nthere will be t paths good so  let us see we will consider l four\n\uf071  this is the last time \nstep \nrefer slide time sixfiftyseven \n \nso lfour\n\uf071  actually depends on s four s four depends on what w and s three s three depends on what \nw and s two s two depends on what an s one depends on w and s zero always assume there is s zero \nwhat kind of a network is this what kind of a function is this what did i ask to revise \nthis is not an order derivative what kind of function is this  \nrefer slide time sevennineteen \n \nso we have an ordered network with i will give it to you and it is not be to say in an \nordered network each state is computed one at a time right so we will first compute s \none then we will compute s two because s two depend on s one there i s no other way we can \ncompute stwo then sthree s four and then finally the loss function \nso now we have the following situation that the derivative of lfour\n\uf071  with respect to w \ncan be written using this chain rule w hich is the derivative with respect to s four and then \nthe derivative of s four with respect to w and that is that looks manageable there is nothing \nfancy here or is it i see a lot if people that looks manageable right everyone is not \nrefer time eightone  \nstudent refer time eightone \neven though you have done the assignment everyone is not even though you have revise \nthe assignment everyone is not refer time eightsix so this part we have already seen \nthis is not the tricky part lfour\n\uf071  sfour is straight forward because it only depends on this v \nand so its fine that part we have seen this is same as computing the gradient of the loss \nfunction with respect to the hidden layer but now let look at the derivative of s four with \nrespect to w  \nrefer slide time eighttwentyeight \n \nwhat is s four actually \nfourws b\uf073 \uf02b  so now if i want to compute \n\uf0b6 sfour by let me just \nremove the sigma right i mean we can always get back the nonlinearity so i want to \ncompute \n\uf0b6 sfour\n\uf0b6 w so it will just be s three s three is again \nstudent depend on w \ndepend on w right so that is the problem with an ordered network in such an ordered \nnetwork you cannot compute the gradient of a s four with respect to w assuming that s three is a \nconstant s three is not a constant its again a function of w and w is the parameter with \nrespect to a computing the derivative right that is the problem here  \nso in such networks the total derivative has two parts wha t are these two parts okay \nhow many if you have revise this what are the two parts called explicit and i mean at \nleast your language model should be fine at explicit and what else can it be think on at \nleast have that much smartness either you do n ot read its fine so that is going to be \nexplicit and implicit what do we do in the explicit case if you can read the slide we \ntreat all the other inputs as constant right an implicit is summing ove r all the indirect \npaths from sfour to w so let us actually try to derive this whole thing right \nrefer slide time ninethirtyeight \n \nso this is what the total derivative looks like all of you are comfortable with this right \ni mean this is all we have done this in the assignments i will not go into the theory a nd \nall that you should be comfortable if you have not revised you have to be blamed sorry \nfor that but i cannot go into the details of that but i still derive the whole thing  \nso this is what it looks like the plus here indicates that we are going to  treat everything \nelse as a constant and just take the derivative with respect to w and then the implicit \npart would be this we are going to sum across all the paths so this is a path ok \nnow here again we have a total derivative \n\uf0b6 sthree  \n\uf0b6 w so what am i going to do for \nthat again explicit and implicit again i have this \n\uf0b6 stwo by two by \n\uf0b6 w which is again \nexplicit plus implicit again \n\uf0b6 sone  \n\uf0b6 w is that fine and then this is finite because s one does \ndepends on s zero which has no connection to w so this is what your entire formula looks \nlike now this sum slide abuse of notation here because what is each o f these actually \nscalar vector matrix \nstudent refer time tenfortyone \ns four is  \nstudent s four is vector \nvector w is \nstudent matrix \nmatrix the derivative of a vector with respect to a matrix is \nstudent tensor \ntensor you cannot do this in your head is it these three sentences is one after the other ok \nso for simplicity what i am going to do is i am going to short circuit some of these \npaths right so let us i will just tell you what i am going to short circuit so i am going \nto write just for ease of coming up with the generic formula the first term i am going to \nwrite as this and this is fair because this is just one right the second term also is fine \nthe third term i am going to short circuit this path i am just going to write as  \n\uf0b6 sfour  \n\uf0b6 stwo \nand then \n\uf0b6 stwo  \n\uf0b6 w and again i am going to short circuit these paths and just write it as   \n\uf0b6\nsfour  \n\uf0b6 sone and then this \nthe reason i am doing this then i can write it as a very simple s ummation where i have \nsfour by sk where k goes from one two three four and then i just have the explicit derivative of sk with \nrespect to w just stare at this for the minute and not a minute actually just ten second or \nsomething if you have any problems with this let me know i will use my standard trick \nif you do not understand this you will not understand anything afterwards no one is \nfalling for that ok everyone is comfortable with this ok \nso we have a formula for \n\uf0b6 sfour  \n\uf0b6 w and we have dealt with the tricky situation where \nwe have these multiple paths in an ordered network and hence we are to split into \nexplicit and implicit derivatives so we have done all that refer time twelvetwenty math and \nyou have come up with the simplified formula for this ok so finally this is what we \nhave \nrefer slide time twelvetwentythree \n \nyou noting it down right  \nstudent refer time twelvetwentyeight \nlaugher i do not see you noting it down ok so now let us look at \n\uf0b6 sfour  \n\uf0b6 w that is \nexactly what we have derived on the previous slide and that was a summation of t terms \nand for us t is e qual to four ok an d in general l t by the this was for l four so in general if i \nwant to do l t then it is going to be this which i am replaced by t and this which i have \nreplaced by this formula everyone is fine with this what were this means everyone is \nfine with this form ula right this is generic formula with respect to any time step the \nonly thing is that on the previous slide w e are derived with respect to s four now i have just \ncome up with the generic formula ok \nso this algorithm is called backpropagation through ti me because now we have taken \ncare of this ordered network and you have a way of computing this gradient once you \nhave this gradient your life is simple because now we can just supply the gradient \ndescent update ok so we have dealt with v we have dealt  with w and as the name \nsuggest who will deal with u you ok fine so you will to find out what it is for u ok \nby its going to be something very similar and i do not want to do it because that is not i \nmean going to be something very similar you can d o it on your own but i want to focus \non something which is important"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 13.4 The problem of Exploding and Vanishing Gradients.wav", "duration": 623.63, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and five \nthe problem of exploding and vanishing gradients \nand that takes us to the problem of vanishing and exploding gradients ok so you want \nto see wha t is a problem with this back propagation through time which c ould lead to \ncertain interesting situations \nrefer slide time zerotwentyfour \nso we will focus on this  \n\uf0b6 st  \n\uf0b6 sk and l et me just go back so remember that this \nformula had this \n\uf0b6 st  \n\uf0b6 sk right where st could be the last time step and sk could also be \nthe first time step because you are summing over all the time steps right \nrefer slide time zerotwentyseven \n \nso you could have a term which is s t capital t which is the last time step the first time \nstep and the derivative of the last time step with respect to the first time step right so \nthat is a situation that we are dealing with so we will consider one such generic element \nwhich is \n\uf0b6 st  \n\uf0b6 sk and we will just try to expand it so remember i have done this short \ncircuiting so i am now just going to expand it again so this is going to be t  t  one t one t \ntwo and so on up to k  one sk ok and i can write it as this generic formula everyone find \nwith this i have just replace this as a product and written it more compactly \nnow let us look at one such term here  \n\uf0b6 sj \n\uf0b6 sjone now just to confuse you guys from \nnext slide i will go over to  \n\uf0b6 sj \n\uf0b6 sj one or not confuse you i just did not pay attention to \nthis so instead of s plus one and j and i am going to do j and j minus one right it remains \nthe same does not matter \nrefer slide time onethirtythree \n \nso we are interested in this particular quantity so let us see what this derivative is and \nremember that in the final formula we have a product of these quantities so i am \nlooking at one such term in my final product so just to jog a memory a j is the pre \nactivation which is given by this and then s j is the hidden representation after activation \nafter the nonlinearity which is given by so let me just write it down as s j by s j minus one \ncan be written as this chain rule which is first compute s j with respect to a j and then a j \nwith respect to s j minus one everyone is fine that so far at this point please raise your \nhands if you find ok  \nnow let me just write down a j and s j explicitly so remember that a j is this d \ndimensional vector which are the entries a j one a j two up to a j d and s j is the \ncorresponding activation applied vector which has these entries sigma a j one a j two and so \non ok now first question what is this quantity scalar vector matrix tensor \nnumerator is a \nstudent refer time twofortyfour \ndenomitor is a \nstudent refer time twentyfivefortyfive \nthat is why it is a matrix ok so that is the matrix that i am interest ed in if i can give \nyou that matrix and we are kind of done so it help me filling in this matrix \ntell me what this matrix is going to look like even before we start filling it ok you are \nright but it does not matter because you will have u x and then you are taking the \nderivative with respect to s j minus one right so this does not matter ok so everyone gets \nthat you will have a u x j here right but that does not matter because you are taking a \nderivative with respect to s j so that is a constant \nso \n\uf0b6 sj  \n\uf0b6 aj  is what what does this matrix look like how many of you see a diagonal \nmatrix ok good so it is straightforward right what is the first entry it is going to be \n\uf0b6\nsjone \n\uf0b6 ajone what is that going to be it will be something but let us look at the second \nentry \n\uf0b6 sjone \n\uf0b6 ajone what is this going to be what this going to be \nstudent zero \nzero because it does not depend on that right so now you can see how the full matrix will \nlook like all the ofdiagonal elements are going to be zeros and diagonal elements are going \nto be sigma primes everyone fine with this ok so this matrix i am going to just ca ll it \nas diagonal sigma prime a j this is a diagonal matrix which i have and what is dou a j \nby dou s j minus one scalar vector matrix scalar \nstudent refer time fourseventeen \nmatrix which matrix \nstudent w refer time foureighteen \nw right ok so now for some reason i am interested in the magnitude of this why i am \ninterested in the magnitude of this for some reason i am interested let us see why we \nwill become clear that for some reason i am interested in \nrefer slide time fourforty \n \nand here i will write how i will write the magnitude of this right so this is the norm \nthat i am interested in so i have already said that this is actually equal to whatever is \ninside this norm so i can just write it as this norm so i have norm of c is equal to no rm \nof a b which is less than equal to \nstudent norm a norm b \nnorm a norm b ok this is fine ok now let us look at the norm of this now going to \nsay that sigma a j is actually a bounded function because we are using sigmoid or tan h \nor something so it is a bounded function ok so that mean sigma dash a j is also going \nto be bounded actually can you tell me what is the bound for the logistic function for \nsigma dash a j  \nif sigma is logistic function what sigma dash what is the bound for sigma da sh if i say one \nby four how many of you will agree with that how many of you have a problem with that \nif you do not understand this you not understand anything after that ok still do not have \na problem so for the logistic fun ction the bound is actually one four the maximum \nderivative that you can get if you have this curve so then that would be onefour ok \nwhat about the tan h function and that actually happens at this point right zerofive so zerofive \ninto zerofive is one four what about the tan h function the bound is one righ t so this is this \nclearly an upper bond on these things the derivative is going to be an upper bounded \nthing that means this magnitude is actually going to be upper bounded by something and \ni will just call it as lambda sorry as gamma so this quantit y is bounded and i am going \nto call that bound as gamma \nwhat about our weight matrix it is again bounded right we have real weights we do not \nhave like blowing we do not have very large weights it is all bounded so it is still going \nto be some upper b ound on this and i will call this magnitude as gamma right so this \nquantity on the left hand side i can say that it is less than equal to some gamma into \nlambda \nnow let us look at the product so this is a quantity that i was interested in and this is \nactually a product of various such quantities so what is it going to be now can you go \nto the next step it will be gamma into lambda raise to t minus t minus k right t minus \nit basically as t minus this product as t minus k terms right so it w ill be gamma lambda \nraise to t minus k now if gamma or lambda or rather gamma into lambda if it is greater \nthan one what will happen what will happen to the series explore if it is less than one \nstudent refer time seventwenty \nit will vanish right so you get that so that is why you have this vanishing an exploding \ngradients problem ok but why what if this vanishes what vanishes let us go back so i \nhave shown you that this quantity could vanish right if this vanishes the entire gradient \ncould vanish and if the gradient vanishes what would happen \nrefer slide time seventhirtynine \n \nstudent no updates \nno updates and you just stuck where you are if the gradient explodes what happens \nthink in terms of the wb plane you suddenly have a very large gradient what will \nhappen is just gone way far from where you are right now because your update is w is \nequal to w minus eta into this gradient and this you have got a very large value now \n it just going to move somewhere very far from where you are and that is never go where \nyour suddenly jump to a different universe ok so that is the problem in training \nrecurring neural networks you could have this problem of exploding or vanishing \ngradients and we have done a mathematical derivation of why you have this pr oblem \nok \nrefer slide time eightsixteen \n \nso one trick to do that is to avoid this is remember these are t minus k terms and the \nproblem appears when your t minus k is or rather you are t is close to capital t and k is \nclosed to one right in those cases you  will have many terms in the product you will have \nas many as t terms in the product so even if your product is even if this product is \nslightly less than one if you raise it to capital t it is going to vanish right so can you \nthink of solution for this \nand the last module in the title of this lecture was truncated back propagation can you \nthink of a solution for this so you do not back propagate through all the time steps yes \nuse an approximation that if you are at time step n we are just going to  look at n minus \nk time steps and we are not going to look all the way back right that is the common \ntrick used to avoid exploding and vanishing gradients \nwhat is the other thing that you could do to avoid exploding gradients so remember \nthat you have  some gradient right to think in terms of vectors we have some gradient \nvector w whose magnitude is very large what will you do to avoid exploding gradients \nin gradient descent your always interested in the direction so what can i do  \nstudent refer time ninethirtyfive \njust normalize it right so you can just do this so typically what is done is that you can \nit is a normalizing it you can just say that you will clip the gradient so that it is \nmagnitude is less than a certain k right so normalize it i n such a way that it is grade it \nis magnitude becomes k so this is something typical that you will see when you use \ntensor flow where you have something with says clip the gradients to a certain \nmagnitude and there are different ways of doing this so i  just give you an intuition that \nthis is what is used for magnitude but there are other things that you can use for \nmagnitude so just go back and look at that ok \nso that is a back propagation through time with exploding and vanishing gradients and \nthen the solution for that or a part for that is truncated back propagation ok we have we \nhave not yet done with this problem we will again look at other solutions for handling \nthis which will lead us to lstms which is long short term memory cells and gated \nrecurrent units so that we will do in the next lecture"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 13.5 Some Gory Details.wav", "duration": 377.59, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 one hundred and seven \nsome gory details  refer slide time zeroeleven \nbefore that i will just go into some more gory details about the math this is not to \nscare but just to make you more comfortable that we can actually deal with something \nwhich is not very straightforward or very neat as compared to what you are seen so far \nrefer slide time zerotwentyfive \n\nso i just go back to the formula which i had for computing  the gradient of the loss \nfunction with respect to w and i cannot repeat enough times that all these notations are \nactually a bit of abuse of notation because these are gradients and not partial derivatives \nso i should actually be using this notation  but for ease of explanation i use i stick to \nmy original notations ok \nso now let us look at each of these quantities here and tell me the dimensions of these \nquantities let us start with the left hand side what is the dimension of this w was what \nmatrix what is i am talking about the circle entity what is the magnitude what is the \ndimension of that k \n\uf0b4  d \nstudent refer time onenine \nd \n\uf0b4  d \nstudent refer time oneeleven \nd \n\uf0b4  d someone n \n\uf0b4  d \nstudent refer time onethirteen \nn \n\uf0b4  d and n \n\uf0b4  k are the two options which are left w is the recurrent weight so w is \nwhat dimension \nstudent refer time onetwenty \nd \n\uf0b4  d \nso what is this gradient d \n\uf0b4  d ok what about this f ast s t what the hidden \nrepresentation so that was d dimensional so what is this d \n\uf0b4 one ok what about t his \nwhy do you guys still struggle with this \nstudent refer time onefortytwo \nd \n\uf0b4  d and this d cross \nstudent refer time onefortyfour \nd cross it is very straightforward right what is the dimension of numerator what is the \ndimension of denominator thats all right \nrefer slide time onefiftythree \n \n so you see the kind of multiplication that you are doing here say of d cross done\n\uf0b4 d d \n\uf0b4  \nd and then d \n\uf0b4 d \n\uf0b4  d ok let us look at each of these quantities and see if you are actually \ncomfortable in implementing these are you comfortable with this the loss function \nwith respect to the hidden representation we have done this enough times in ba ck \npropagation what about this we just saw a formula for this right so we know how to \ncompute this quantity we have seen this in back propagation this is the derivative of a \nscalar with respect to a vector and we are very comfortable in computing this \nthis was slightly tricky but we just derive this formula on the previous slides everyone \nokay with that what about this this is a tensor how do we compute this tensor what is \nour standard recipe focus on \nstudent refer time twothirtyeight \nthe little guy one element of this tensor and then you can generalize somewhere right so \nthis is the tensor and we will just see that this just to make you all comfortable is this not \nlike just to intimated you with all these large sized tensors but i am just tryin g to show \nthat this is all easy this is not hard ok so how do we compute this all the other terms \nare covered \nthis is the only one that we do not know \nrefer slide time threethree \n \n so we will just look at one element of this tensor and it is going to be skpwq r \nso let us just see that you have s k as this vector and you have w as this matrix so i \nam considering one such weight which is w p comma q and one such element from \nhere which is s k sorry so q r and i am considering one element from this which is s k \np so i am trying to compute the derivative of one element of the vector with respect to \none element of the matrix so this is going to give me one entry in my tensor and that \nentry is going to be what p q r how many of you are fine with this ok fine \nrefer slide time threefiftythree \n \nso now recall that a k was equal to w into s k minus one plus b and s k was sigmoid of a \nk i think again i have miss that u into x k but that will not matter because that is not \nthere in the derivative ok you are fine with so far \nrefer slide time foureleven \n \nso now let us look at this because the other two terms do not matter so i just look at a k \nis equal to w into s k minus one so this is the matrix way of writing it ok now i am \nlooking at one of th ese elements which actually comes from the multiplication of a row \nand a column the highlighted row and the column everyone gets this ok \nnow so i can write it as a k p is actually equal to this summation which is nothing but \nthe dot product of this row with this column ok now s k p is just the sigmoid of that so \nnow if i want to compute s k p with respect to w q r i can just write the chain rule that \ns k p with respect to a k p which is straightforward and then a k p with respect to w q r \nand i already have a formula for a k p how many of you are fine so far please raise \nyour hands high up if you are fine ok so what is the first term going to be sigma \nstudent refer time fiveten \nsigma prime of a k p and what is the second term going to b e this is what the second \nterm is now what this is lot of terms here which of these terms would actually remain \nonly the once where only the terms where i is equal to \nstudent refer time fivetwentyeight \nr and \nstudent refer time fivethirty \np is equal to q so only that term will remain in that case it would be this right and in \nthe other cases going to be zero right so now you have one element of this tensor and you \nhave it as a very generic formula you can just fill in all the elements of the tensor right \nso what does this tensor look like it is a very \nstudent refer time fivefiftytwo \nsparse tensor right that is all i wanted to convey ok so this is again the same thing \nright is that fine so even though it is a nasty looking tensor if we just br eak it down to \none element it is going to be very easy and now from this element you can just \nreconstruct the entire tensor do not worry i am not going to ask you to implement this \nbut if someone were to maybe at some point then you should be able t o do it right that \nis where we will end today  \nso we have finished recurrent neural networks and the next thing that we are going to \nlook at is lstms and gated recurrent refer time sixtwentyfive ok \nthank you"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 14.1 Selective Read, Selective Write, Selective Forget.wav", "duration": 517.46, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 fourteen \nlong short term memory cells lstms gated recurrent units grus \nso we have been looking at a new kind of or a different kind of neural network which is \nrecurrent neural network and last class we spent some time on showing that it is how to \ntrain these recurrent neural networks because of the specific  problem of exploding  and \nvanishing gradients in particular we saw that if you want to propose if you want to back \npropagate the gradient from time step t the final time step to an arbitrary time step k then \nyou have this multiplicative term in the back propagation which could explode or vanish \nso today we are going to  try to see if we  are trying to focus  on something  which can \nhelp us solve this  problem to whatever  extent  right so  that  is why  we  will  look at \nlstms a long short term memory cells and gated recurrent units ok so let us start \nwith that  \nrefer slide time onetwo \nso first we will introduce the idea of selective read selective write and selective forget \nand then we will try to build on this intuition and see how could you could realise it by \nusing lstms and gated recurrent units and whether that help in solving the vanishing \nand exploding gradient problem \nrefer slide time oneeighteen \n \nso recording the state of an rnn records information from all the previous time steps \nthat was the whole idea that you add this recurrent connections so as you keep going on \nat this time step the cell is not only recording information from the current time step but \nit also has some kind of an accumulated history from all the previous time steps right \nbut now the issue is that this state or this blue coloured vecto r that you see is going to be \nof some finite size right we will say that it is a one hundred dimensional vector or a one thousand \ndimensional vector but whatever be the size it is going to be some finite dimension \nnow as you keep writing information to that cell you a re morphing the information that \nyou had written at the earlier time steps right do you get that so now that is the \nproblem right because on one hand you are saying that you want to record the \ninformation from all the previous time steps but the at the same you at the other hand \nyou just have a finite amount of memory to deal with so it is bound to read over ridden \nand the information will get morphed so much that it is completely impossible to say \nwhat was the original contribution at time step one or time step two once you have reach \nsome time step twenty thirty or so on right so that is the problem with recurrent neural \nnetworks and we will tie it back to the problem that we had with the vanishing and \nexploding gradients right \nrefer slide time twothirtythree \n \nso in fact the similar problem occurs when you try to when the information flows \nbackwards during back propagation right it is very hard to assign the responsibility of \nthe error caused at time step t to arbitrary time steps before it right to ver y far away time \nsteps it is very hard and that is the vanishing gradient problem right because we have \nthis multiplicative term and the gradients vanish so that that is very hard to do so both \nduring forward propagation the information vanishes and e ven during backward \npropagation the information vanishes right and we saw a formal argument of this while \ndoing vanishing gradients so this is just an illustrative diagram but we also saw that \nformally the guardians do vanish under certain conditions right \nrefer slide time threeeleven \n \nnow let us see an analogy for this and from here on we will build on some ideas which \nwill help us arrive at lstms and gated recurrent units right so the analogy is a \nwhiteboard so you have a whiteboard and it is a lways of a fixed size right i mean the \nwhiteboard is not infinite you just have some size for the whiteboard and you keep \nwriting information on that so at every time steps i keep going and writing something \non the board and i am trying to derive somet hing or just try to make a story or anything \nright i just keep trying to write information on the board \nnow since the whiteboard is fixed size at every time i am essentially morphing the \ninformation which was written at the previous time step and afte r many time steps it \nwould be impossible to find out that now whatever my state of the board is how did time \nstep one contribute to that state right because it is written all over the board i do not \nknow where i started what i did and so on and its goin g to be very hard for me to find \nand this happens right when you do these long derivations on on the whiteboard it \nbecomes very hard to track where did i start where was this variable defined and so on \nright because it is a fixed size you cannot reall y i will end of deleting some terms and so \non right \nrefer slide time fourseventeen \n \nso and we will make this more concrete with the help of an example right suppose i \nam trying to drive an expression on the board and typically what we do is we use three \nthings we use selectively write on the board selectively read the already written content \nand selectively forget or erase some content so let us see what i mean by these three ideas \nok \nrefer slide time fourthirtynine \n \nso first we look at selective write so this  is my problem this is the derivation that i \nwant to do on the board and i have a very small whiteboard which allows me to write \nonly three steps ok that is the situation in which i am operating ok so i am given the \nvalues of a b c d and i want to compute  this expression ac  bd  a  ad ok now the \nfirst thing that i am going to compute is ac right that make sense because that is the first \nterm that i need so i will write ac equal to five and the second thing i will write is bd  \nthirtythree ok so now why d id not i do the following i could have done this a equal to one c \nequal to five then ac  five then b is equal to something then b into d is equal to something  \nso what am i doing here while writing on the board why did i write only two steps \nbecause i kn ow i have a finite size for the whiteboard right so i am only trying to \nwrite information which is important right i am not writing everything because i know \nthat i am going to run out of memory right so that is why i did not write these \nintermediate steps that a  to one c  five and then a c  five and so on right i juts wrote the \nresults which are important so i am selectively writing to the board because i am \ndealing with a finite size memory and this is exactly what we do while writing in a note \nbook or a whiteboard or anywhere right we just do not write everything one because \nyou are lazy but second is also because we do not have enough space right so that is \nabout selectively writing \nrefer slide time sixtwo \n \nnow selectively reading i a lready have something stored on the whiteboard so this is \nthe state of the whiteboard at this point now at the next time step i need to read some \ninformation but i do not need to read everything that is on the whiteboard i just need to \nread some information what is the information that i need to read for the next time step \nbd i do not need to read ac right so i am just doing a selective read of the information \nor the state which is already stored on the whiteboard \nnow what is happed is i have e xhausted my space where i had just three steps that could be \nwritten on the whiteboard and i have done that now what do i do for the next thing \nnow i need to compute ac bd  a i will have to selectively erase so what will i erase \nbd right \nrefer slide time sixfortyfour \n \nso now as the whiteboard is full and i will have to selectively delete some information \nand as this obvious in this trivial example that you can get rid of bd because you have \nalready encoded the information in bd  a and now the next st ep which is ac into bd \nplus a can do on the whiteboard and this is how you keep doing at every time step you \nselectively write at every stage  \nhere again note that i am not written ac bd  a would be something like i do not know \nwhat was it five x thirtyfour  one hundred and seventy right i am not using two steps i am just writing everything in a \nsingle step because i do not have enough space right so selectively writing selectively \nreading and selectively forgetting things which are there in a constant memory is \nsomething that we do regular right \nrefer slide time seventhirty \n \nand then other ways of motivating this so you could even think of our brain as \nsomething which can store only a finite number of facts right as we keep going or if we \nwill learning more and more things we can only retain a finite number of facts \nand what happens inadvertently is that you erase some of the steps not consciously of \ncourse you do not have a delete button or anything but you erase some of these things \nthat you forget a lot of things which had happened a year back or so and also at various \ntimes if i ask you what was this which i have done in last class most of you forget to do \nthe selective read but that is what you do right you always do selective forget but that is \nwhat we typically do in our brain also right any time when you are dealing the finite \nsize memory you will always have this three operations either they are explicit or implicit \nbut the intuition is that you end up doing this ok \nso now since the rnn also has a finite state size can we do something like this \nselective read write and forget so that one during forward pass even if the information \ngets morphed it gets morphed in a principle manner right so even in the whiteboard \nexample i was morphing the information i wa s deleting the information written at time \nstep one time step two but i was being a bit smart about that i was retaining some good \ninformation and only deleting what was not required so can we do this in analogy so \nthis analogy really sets it up but now  the solution is not going to live up to the \nexpectation but it will have some it will be something in this direction ok"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 14.2 Long Short Term Memory(LSTM) and Gated Recurrent Units(GRUs).wav", "duration": 1839.14, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science engineering \nindian institute of technology madras \nlecture \u2013 one hundred and nine \nlong short term memory lstm and gated recurrent units grus \nrefer slide time zeroeleven \nso with that motivation let u s go to the next module where we will talk about long \nshort term memory and gated recurrent units ok \nrefer slide time zerotwentyone \n\nso now all this was fine in terms of ok i gave you a derivation on the board and say \nthat this is not required but can i g ive you a concrete example where rnns also need \nto selectively read write and forget right only then you will be convinced that this kind \nof morphing is bad in the case of rnns so i will start with that example and then once \nwe agree that we need sele ctive read write and forget how do we convert this into \nsome mathematical equations right because conceptually it is fine but you have to \nwrite some equations so that the rnn can do some computations where you have \nselective read write and forget ri ght so that is what we are going to do over the rest of \nthe lecture \nrefer slide time zerofiftyfive \n \nso first let me start with the concrete example where you want to predict the sentiment \nof a review using an rnn so this is the rnn structure we have do ne this in the past \nthat you have a sentence one word at a time is your every time step you will feed this to \nthe rnn and at the last time step you will make a prediction and as i said the rnn \nneeds a document from left to right and by the time it reache s the end the information \nobtained from the first few words is completely lost right because it is a long document \nand you are continuously writing to the same cell state  \nso you will lose the information that you had gained at the previous time step  but \nideally we want to do the following we want to forget the information added by stop \nwords like a an the these do not contribute to the sentiment of the i can ignore these \nwords and still figure out the sentiment of the document \ni want to selectiv ely read the information added by previous sentiment bearing words \nso when i have reach the last time step i should be able to read everything else which \nhad some sentiments before it and focus on those words just i want to selectively read \nfrom these sentiment bearing words and also i want to selectively write the new \ninformation so i have read the word performance now i want to selectively write it to \nthe memory whether i should write it completely or should i only write parts of it or not \nthat is what i need to decide so that is fair this is a typical example where rnn also \nwhen it is dealing with long documents it needs to understand what is the important \ninformation in the document that needs to be retained and then selectively read write and \nforget ok \nso i am spending a lot of time on this analogy because you need to really understand \nthat this is important and this is where rnn suffer right if you are using them for very \nvery long documents if we have document of the size one thousand words  which is not comm \nwhich is not uncommon right because wikipedia pages have much more than that per \ndocument so it is going to be very hard to encode the entire document using an rnn \nnot that it is going to become significantly easier with lstm or grus  but to certain \nextent it will become easier ok \nrefer slide time twofortyseven \n \n refer slide time twofiftythree \n \nnow the next part is how do we convert this intuition into some mathematical equations \nright so let us look at that so in this diagram recall that the blue colored vector is \ncalled the state of the rnn it has a finite size so now i will just call it as s t belongs to \nsome r n and the state is analogous to the whiteboard and sooner or later it will get \noverloaded with information and we need to take  care of this right so now our wish list \nis selectively read write and forget ok so let us start with that \nrefer slide time threefifteen \n \nso what we want to do is that and this is the problem definition now that we have \ncomputed the state of the rnn t his is a blue colored vector although it is not blue but \nthis the blue colored vector from the previous diagram where the state of the rnn was \ncomputed i know what the state is at time step s t minus one now i want from here to here \ngo from here to here  that means from s t minus one i want to compute the new state of \nthe rnn right so i had something written on the whiteboard i want to write something \nnew i want to change the state of the whiteboard and this is the new information that has \ncoming to me right the x t is the new information at time step t \nand while doing this i want to make sure that i use selectively write read and forget so \nthese three operations have to come in somewhere in the between so that i am true or \nfaithful to the analogy which i have been giving right that is the this is the our problem \ndefinition now going from s t minus one to s t and introducing these three operations along the \nway that is what we are interested in doing \nrefer slide time fourtwentyone \n \ni will go one by one we will implement each of these three items right so we will start \nwith selective write so recall that in rnns this is what happens at every time step t \nyou take the previous time step previous cell state you take the current input do you \nrecognize the op eration here how many of you recognize the operation here raise \nyour hands ok so this is nothing but the following operation and as usual i have \nignored the bias ok is that fine so that is what i am representing it as \nbut now so one way of looki ng at it is that when i am computing s t i am readin g or i \nam taking the whole of s tone so once i have computed stone i am writing this to my \nwhiteboard and then whole of it would be used to compute the next time step ok but \ninstead of doing this what i  want is i want to read only selective portions of s t minus one \nor rather i want to write only selective portions of stone once i have computed s t minus one \ni do not want to write the whole of it because then the whole of it will be used to \ncompute the ne xt cell state i do not want that i just want to selectively write some \nportions of it ok \nnow in the strictest case since i know that s t minus one belongs to r n it is an n \ndimensional vector in the strictest case what i could have done is i could have  used a \nbinary decision that of all these n entries i am going to read some entries and ignore the \nothers so all the other entries i am going to set to zero fine that is the strictest thing that \nyou could have done now for any of these strictest things  what is the soft solution so \nfor binary what is the soft solution binary zero to one so what is the soft solution for that \nbetween \nstudent refer time sixfive \nzero to one so read some fraction of each of these dimensions right so let us try to \nunderstand what i am trying to do here ok so and the third bullet some of these entries \nshould have gone to zero right ok \nso instead of doing this what we want to do is we have this vector which has n entries \nthis is the cell state at t minus one i do not want to write the entire vector onto the final cell \nstate what i want to do is i will take some fractions of it is say zerotwo of this zerothree of this zerofour \nof these and then write only that do you see the operation that i am trying to do right i \nwant to take some fractions and write only those to the cell and as i said this is the softer \nversion of the hard decision which would have been zero for this one for this again zero for this \nand so on right \nrefer slide time sevenseven \n \nhow to do this why to do this all that is not clear i am just telling you the intuition \nhow and why will become clear later is that fine ok so we want to be able to take stone \nand write only selective portions of it or pass only selective portions of it to s t \nrefer slide time seventwentythree \n \nso whenever we compute st we do not want to write the whole of s t minus one just want \nto use selective portions of that so what we do is we introduce something known as a \ngate and so this gate is otone ok we take the original cell state stonedo an element wis e \nproduct with a gate which is known as the output gate and then write that product to a \nnew vector which is stoneok so initially this will look confusing but it will become clear \nby the end of this lecture ok so is that fine this is what i am tryi ng to do again how to \ndo this is not clear but this still matches the intuition which i have been trying to build \nthat i want to write only selective portion of the data which i already have is that fine \nok so each element of otone gets multiplied by  the corresponding element of stone and it \ndecides what fraction is going to be copied and this o t minus one is going to be between \nzero to one \nbut how do i compute o t minus one how does the rnn know what fraction of the cell \nstate to get to the next state ho w will it do it we need to learn something whenever \nyou want to learn something what do we introduce everyone \nstudent refer time eighttwentysix  \nparameter sorry what did you guys say back propagation back propagation will do \nwhat it will work in the air or propagate to what \nstudent refer time eightthirtyseven \nwhenever you want to do some kind of a learning i want to learn some function what \ndo i introduce \nstudent refer time eightfortytwo parameter \nparameter right so that is what we are going to do we are now  going to introduce a \nparametric form for otone right and remember this throughout in machine learning \nwhenever you want to learn something always introduce a parametric form of that \nquantity and then learn the parameters of that function do you get thi s how many of you \nget the statement ok this is what we have been saying day from right from class two or \nclass three right \nrefer slide time ninethirteen \n \nalways introduce a parametric function for your input and output and learn the \nparameters of this function s o that is exactly what i am going to do i am going to say \nthat otone is actually this function i am just giving some time to digest this so this is at \ntime step t  one so it depends on the input at time step t minus one it also depends on the \noutput at output means whatever comes out of this right so the same operation what \nhave happened at time step t two so whatever was the output at that state it will also \ndepend on this \nyou just take a while to digest this equation you will see at least six mo re equations of \nthis form in this lecture so if you are comfortable with one all of them would be clear \nso try to connect the whole story i have stone i do not want to pass it as on pass it on as it \nis to st so i am computing some intermediate valu e where i will only selectively write \nsome portions of stone and selectively write in the strictest case it should be binary but \nthat is not what we are interested in we introduce fractions if the fraction has to learn \nbinary let it learn but we will make it fractional that means we will make it between zero \nto one  \nhence the sigmoid function right remember in one of these lectures we had said that \nsigmoids are still used because in rnns and lstms remember in we had said that \nsigmoids are bad use sta nage refer time tenthirtytwo or use relu but we had ended with \nsigmoids are still used in the case of recurrent neural networks and lstms so this is \nwhere they are used ok how many should get that connection ok good fine \nso we use sigmoids because we want the fraction to be between zero to one and we also want \nsome parametrization right and this is the particular form that we have chosen there are \nvarious equations possible various things you could have done here in fact there are ten \nto fifteen different var iants of lstms i am covering the most popular one which uses the \nfollowing equation right so it is says that this is how you will compute the output gate \nand that gate will regulate how much of the cell state should be passed on from t minus \none to the next state ok everyone clear with this ok so now if you are clear with this \ngive me an equation for h t minus one \nstudent refer time eleventwentytwo \nloudly everyone s t minus one is that fine right so this is the equation that we will have \nso we have don e selective writing and these parameters are no special they will be \nlearned along with the other parameters of the network ok so let us spare some thought \non that you got a certain loss at the output ok earlier you just had these parameters w \nu v which were the parameters of rnn which you are adjusting to learn this loss now \nin addition you also have the flexibility to adjust these parameters  \nso that if the lost could improve by selectively writing something then these parameters \nshould be updated accordingly right may be you are being over aggressive and making o \nt minus one to be all ones that means you are passing everything to the next state right \nnow it has the chance because they have introduce parameters if it helps the overall \nloss it better make these fractions more appropriate so that only selective information \nis passed to the next state how many you get this intuition so that is why anytime you \nintroduce parameters you have more flexibility in learning whatever you intend to learn \nthere is remember one clear difference here right and that is where i said that while i \nwas giving the analogy i was really setting up things but here there is one distinction \nwhat is the distinction that is there ideally what would i have wanted suppose i take \nthe example of the review ok and the review was say the movie was long but really \namazing ok now which is the word here which is actually trying to mislead so overall \nsentiment is positive right everyone agrees with that  but which is the word which is \nmisleading \nstudent long \nlong right that means i need to do what with that word \nstudent refer time thirteeneleven \nforget that would right now ideally i would have wanted someone telling me retain \nretain retain forget  retain retain retain i would have a label for each of these words \nand then i could have a loss function which tells me whether my gates were actually \nathering to this decisions or not so remember my gates are learning some distribution \notone which tells me what fraction to retain and at this particular time step i would have \nwanted o t minus one to be all zero\u2019s ok i would have wanted to forget but this kind of not \njust o t minus one this will become more clear when i do all the other gates also so what \ni am trying to say is that you should have had some supervision which tells you which \ninformation to retain and which information to forget but you do not have this \nsupervision right no one is telling whether these are the important words these ar e not \nthe important words \nso that is the difference between the whiteboard analogy there you knew exactly which \nstep is important and which step is not important here you do not know that all you \nknow is that you have a final loss function which depends on plus or minus whether the \nthis prediction is close to positive or close to negative and what is the loss and that loss is \nwhat is being back propagated but the difference now is that you have introduced a \nmodel which can learn to forget some thi ngs right earlier you did not have a model \nwhich could learn to write or read or forget selectively now you have introduced a \nmodel this is a better modeling choice right so the same as we have had arguments that \nyou could do y is equal to w transpos e x or you could do y is equal to deep neural \nnetwork of x right you are making different modeling choices here and with the hope \nthat one modeling choice is better than the other choice \nso just as rnn was one modeling choice now you are using a different modeling choice \nwhere again with the help of these gates and all you can definitely write a function of y is \na function of the input and that function is going to be lstm function which we will see \nin detail so this one part of that function and whil e doing this you are just making a \nbetter modeling choice which allows you to learn more parameters and along the way if \nimportant do selective write read and forget is that clear right so you would see the \ndifference what would have been the ideal c ase and what is it that you have the ideal \ncase would have been explicit supervision for what to forget read and write you will \nnever have that but you are still making a modeling choice which allow you to do that \nso if it required to model while ba ck propagation should be able to learn these \nparameters so get you are able to do that \nrefer slide time fifteenthirtynine \n \ni know i am repeating myself but it is very important that you understand this situation \nhow many of you get this now and as i said these parameters will be learned along with \nother parameters and o t is called the output gate because it decides what to output to the \nnext cell state ok still you see that there is a lot of gap here we have not reached s t yet \nwe are still at stone we have computed some intermediate value but we have not reached s \nt yet and along the way we had three things selective write read and forget we have only \ntaking care of selective write so far ok \nrefer slide time sixteenfive \n \nnow let us look at selective read so what this selective read do you are going to get \nnew information at time step t which is x t right and now instead of this original cell \nstate you have used the selectively written cell state because that is what you have \nwritten now so that is what you should use \nnow using a combination of these two i am going to compute some intermediate value \nok and just stare at this equation this equation form is very similar to the rnn \nequation form right only thing is that instead of s t minus one i am  using h t minus one and \nfor good reason because i know that h t minus one contains only selectively written values \nfrom htone is that fine and x t is the new input still there is some  gap here i have not \nreached st yet i am still at an intermediate value so this is the new input which i have \nreceived now what should i do with this new input selectively read this input i do not \nwant to take all of this input because may be the input which i have got now is a stop \nword and i do not want to read all of it right do you get that \nrefer slide time seventeenfifteen \n \nso now it captures all the information from the previous state as well as the current input \nand we want to selectively read this so now what would you do to selectively read \nagain the same situation  that you have a \ns\n the answer is already here you have s tilde \nand you do not want to pass it on as it is to s t this is \ns\n  st is somewhere here which you \ndo not know how to get to but you know that yo u do not want to pass on all the input \nthat you have read you want to selectively pass it on so what will you do now again \nintroduce a \nstudent gate \ngate and this gate will be called \nstudent read gate \ninput gate or the read gate right ok \nrefer slide time seventeenfiftyfive \n \nso now what can you give me an equation for the gate i t is equal to sigma of that is \ngood because sigmoid is what we need it is going to be a fractional thing let me add \nthe easy part w into \nstudent htone \nhtone that is telling y ou what ha s happened so far and u times x t you see the same \nequation same form the parameters have changed so these we will call as w i ui and vi \nand they are depending on the input as well as the previous state previous temporary \nstay that we had computed ok so that is exactly what your input gate is going to be and \nnow this operation is the selectively reading operation how many you are fine at this \npoint ok and then this product is going to use to be it is will help us to read selectively \nfrom this temporary value that we have constructed or the input that we have taken ok \nrefer slide time eighteenfiftyone \n  \nso far what do we have we have the following we have the previous state which was s \nt minus one then we have an output gate which was o t min us one using these two we have \ndone selective write right we have taken the previous state and the gate and then a \nselective write is that fine ok we need to check if the sigmoid should come here \nbecause sigmoid is already there in the computation of s t minus one right oh it is not \nthere so this already has one sigmoid right yeah so then again a sigmoid on that is it \nthere ok we will figure it out just check the equation right  \nso there may or may not be the sigmoid the sigmoid already applied to s t minus one but \nwe can figure that out ok so this is the selective write portion then you compute the \ncurrent temporary state ok and just look at the similarity between these equations then \nyou have an input gate and using these two we have done a sel ective read ok so you have \ntaken care of selective write and selective read but you are still not reached s t i still do \nnot have an arrow here i still need to f igure out how to compute the s t finally ok so \nwhat is the operation which is remaining now selective \nstudent forget \nforget ok so what do you think should we forget we want to find new st so let us see \nwhat we will forget right \nrefer slide time twentyseven \n \nso the question now is that you had this s t minus one and now you have a temp orary state \ns\nt which is here how do we combine these two to get the new cell state ok so the \nsimplest way would be that you say  that s t is equal to whatever was there in s tone plus \nselectively reading from the current input is that fine this is the one way of doing it ok \nbut now what am i doing here what is the problem here i am reading i am taking stone as \nit is right so what should i do i should forget some parts of stone so what should i do \nfor that introduce a what gate \nstudent forget gate \nforget gate right so we may not want to use stone as it is but we are want to forget so \nthere is at this point all of you should get some confusion if you do not then i would be \nworried if you are getting some confusion good right you should all get confused at this \npoint why are you confused because you already did selective write and now again \nyou are doing a selective forget also right but there is a difference because the selective \nwrite was then used to compute  how to read the information right but now once you \nhave read the new information you want to see how to assimilate it back with the old \ninformation that you had right so that is why you introduce a separate gate so think of \nit as this way that you are keeping these functions separate input output and forget so \nthey can separately learn things ok \nso whatever you want to selective write let it be a separate function these h t minus one \nis not going back to s t right let us just by use so that yo u can compute these temporary \nstates so that is what is being passed to the next temporary state let i t only decide \nhow much of this input should be read ok and then when you want to combine these two \njust use a separate gate and this exact idea which  is confusing all of you why have a \nseparate write gate and a separate forget gate led to something known as gated \nrecurrent units where they merged these to gates we will get back to that ok  \nso at this point it is fine i am just telling you the ori ginal equations for lstm and this \nwas the motivation that they had so as i said there are at least fifteen to twenty different variants \nof lstm which use different equations they tie some of these weights so one thing \ncould be that forget is the same as one minus remember right or output could be same as \none minus input right you could have tied these gates instead of learning separate \nparameters for that \n refer slide time twentytwofiftyseven \n \nso in the most parameterized form you have a separate parameter for all of t hese ok so \nwe introduce the forget gate again can you tell me the form for this forget gate ft is equal \nto first term \nstudent wf \nwf second term u f what wil l be there in the second term x t and the first term ok so \nthis is what it will look like ok  so if you remember one of these equations you will be \nable to write all of these not that i am going to ask you to write them in quiz or \nsomething but why take a chance so and then once you have completed the forget gate \ninstead of this equation can you tell me what is the equation and you are going to use \nwhat is the first term going to be it is stone here what is it going to be now  \nstudent ft into \nf t into \nstudent stone \nstone that fine ok \nrefer slide time twentyfourfortyseven \n \nso now we have the fu ll set of equations for lstm we have certain gates and certain \nstates what are the gates output gate \nstudent input gate \ninput gate \nstudent forget gate \nforget gate why do you guys has this momentary amnesia like suddenly you forget \neverything ok so output gate input gate and forget gate all of these are the same form \nwith different parameters ok what about the states which are the states th at we have \ncompleted one was st the other was ht and the third one was \ns\n t ok\ns\n t from \ns\n t we get st \nand from st we compute ht ok \nrefer slide time twentyfourthirtyseven \n \nso in the diagram that you see here at the top tell me which are the computations \nwhich are happening at one time step at  time step t which are the computations which \nare happening is it i will give you the options right is it this or is it this let us call this one \nlet us call this two or this three or this four which are the computations happening at one time \nstep and you see the order also here this should be straight forward right why \nstudent refer time twentyfivesixteen \nhow many of you say four that is the one right because you start with selective reading \nright and you can just go by this right these are all indexed by t right is that fine ok so \nthese are the computations which happen at time step t and these are exactly the \ncomputations which were written right so we have the three gates which you need to \ncompute at every time step and you have the three states which you need to co mpute at \nevery time step is that fine and this s t minus one is not being computed it is just taken \nfrom the previous time step is that fine so you have these six computations which \nhappened at every time step and the output final output of an lstm so when you use \ntensor flow or something the output of an lstm would give y ou two things it will give \nyou ht st because these both the states that are being computed one is the running state \nand another one is the current state which is being computed ok \nand i choose the notation s because that is what we have been using for rnns but in \nlstm in all the literature instead of s you will find it to be ct because it is called th e \ncell state so that is why s t ok so all these equations wherever you see a n s when you \nare reading some standard blogs or things like that you will see c instead of s so you \njust do this mapping in your head ok \nrefer slide time twentysixfortyone \n \nso lstm actually has many variants with include different number of gates and also \ndifferent arrangement of the gates so as i was saying that you could say that input is one \nminus output or input is one minus forget or things like that and also why this particular \nparametric form right why not make w zero into s tone instead of h tone and so on so the all \npoints of things that you could do or all of these are valid these are all valid variants of \nlstms \nso there is this paper called lstm a search space odyssey so you can go and look at i \nthink we link it in the in the reading material right ah  so you can see that there are \nactually many variants of lstms but this is the most standard and default variant \nwhich you will find in most platforms on tensor flow or pytorch refer time twentyseventwentysix \nform \nrefer slide time twentyseventhirtythree \n \nand there is another v ery popular variant of lstms which is called gated recurrent \nunits so we will just see gated recurrent unit so i will just give you the full set of \nequations for grus so you have gates but unlike lstms you have only two gates you \nhave an output gate and you have an input gate you do not have the forget gate ok \nso what am i going to do for the forget gate so this is what i am going to do you see \nthe last equation so instead of forget gate i am just saying that this is what you are \ngoing to se lectively input from the current temporary state so the rest of you rest of it \nyou take from the previous state right so i have just tied the input gate and the forget \ngate any other changes do you see in this so earlier we had s t minus one everywher e \nright now we have s t minus one itself is that fine ok so the basic idea these equations are \nmany and you could think of your own equations you could say that i will not really use \nthis input information at all or i will choose to use it differently or what not right there \nare several things that you could do at a very abstract level this is what you need to \nwhat is this  \nstudent refer time twentyeightthirtyeight \nso these parameters could then make a difference right they could adjust it accordingly \nand so on right so that is what i was coming so the there are various ways of realizing \nthis right at the abstract level you need to understand that the original problem was \ntrying to store all the information from time step one to time step t capital t right which is \nnot feasible because of this finite size that you have so along the way we built this \nintuition that it should be good to have these operations which allow you to selectively \nread write and forget right how do you mathematically realize these op erations there \nare various choices for doing that and we saw a few choices for doing that right there are \nmany others you could have done but this is largely what whenever you say that i have \nused an lstm most likely you are using the set of equations which i saw which we saw \non the previous slide and whenever you are using a gru these are the set of equations \nthat you will be using ok \nand again remember this that there is no explicit supervision here it is just that we have \na better modeling choice we are just introduce more parameters so that if required these \nparameters could be adjusted to do a selectively read write and forget right so it is \noften it is often valuable if you are doing some task with rnns or lstms you should \nvisualize these gates right you should see that at time step t if you thought that it should \nhave forgotten everything that it has learnt so far because suppose you had this the \nmovie was long but i really loved it because the direction was superb and so on \nnow this word but actually changes everything right because it whatever was written \nbefore it does not matter anymore right so is it really learning those kind of gates \nwhere everything before but was forgotten right so it would be helpful to visuali ze \nthese output gates and see what kind of values they are learning what kind of things \nthey are remembering forgetting and selectively reading and so on right so as i said i \nwill just again summarize the key thing here is the intuition and then the r ealization in \nthe form of equations there are multiple choices we have seen a few of those right that \nis what i will end with and in particular in grus there is no explicit forget gate and \ninstead of htone you use stone everywhere"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 14.3 (Part-2) How LSTMs avoid the problem of vanishing gradients (Contd.).wav", "duration": 1392.86, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and ten \nhow lstms avoid the problem of vanishing gradients \nso that was lstm and grus \nrefer slide time zerofifteen \nnow the issue is that i have given you a very explanation that why you selectively read \nwrite and forget should work but you have not actually formally proven or even given \nan intuition for with these sets of equations how are we sure that the gradients will f low \nback right we introduced a bunch of equations remember in the case of lstms sorry in \nthe case of rnns the problem was because of the recurrent connections right because \nyou had these recurrent connections this w which was the recurrent parameter ri ght \nwhich was connecting cell state stone to cell state st  \nthis was repeatedly appearing in your gradients right and that was causing the problem \nbecause when you had this multiplicative factor lambda into w and then if you compute \nthe and this was some  raise to t so then if you compute this magnitude then if the \nmagnitude of w blows up then the whole thing will explode if the magnitude of w \nvanishes then the whole thing will vanish right that is the problem that we had \nrefer slide time onetwentynine \n \nso this was because of the recurrent connections do we have recurrent connections in \nlstms or grus for that matter do you have recurrent connections yes or no \nstudent yes \nyes so then that problem could still occur right i mean if you had that the cr ux of the \nproblem for the vanishing gradient was this recurrent connection which is getting \nmultiplied and hence reading to problem but we still have recurrent connections the \ncase of lstms also and why should things become any easier in this case how m any if \nyou get the question how many if you can give me the answer selectively that is a good \nanswer so can you think of what is happening here so first thing that we going to do \nnow so i will go on to the next module  \nrefer slide time twofive \n \nwhen i going to give you intuition for what is happening and then we will do slightly in \nfact a rigorous proof of why it actually solve the vanishing and exploding gradient \nproblem ok so let us look at the intuition first how lstms avoid the problem of \nvanishing gradients i am only focusing on vanishing gradients exploding gradients are \nactually easier to deal with why \nstudent refer time twotwentysix \nwhat can you do what are we interested in when we compute a gradient direction so \nif the magnitude i s very large what can we do just normalize it and restricted to be a \ncertain magnitude so that is known as gradient clipping so exploding gradients in that \nsense is still not a big problem but vanishing gradients is because if it vanishes you \ncannot do anything because you could think of it that you already have a learning rate \nwhich is getting multiplied with the vector the gradient now in addition to the learning \nrate which was anyways clipping the norm of the gradient right so you are doing an \nexpressive clipping also \nso it just like a additional learning rate inductions right ok \nrefer slide time threesix \n \nso here the intuition and then will go to the more rigorous stuff not in this class \nprobably so during forward propagation the gates control the flow of information right \nthe gate decides how much of stone should be pass to st ok and they prevent any relevant \ninformation from being returned to the next state similarly during back propagation the \ngates will regulate the flow of infor mation so what i mean by that is that if at a cer tain \nstate you have computed st  f t stone  it\ns\n t right \nso this gate is actually deciding how much information flows in the positive direction \nok and suppose this gate value  was zerofive so the  zerofive of this information from s t has been \ncarried on stone ok now during back propagation what is the derivative of s t with respect \nto stone going to be partial derivative i s going to be a f t think of s t and stone as single \nvariables like you know n dimen sion variables then the just f t of course you are \nforgetting that what kind of a network is this ordered network right so you cannot \nread as till de t as a constant \nwhere s tilde t also somewhere depends on s t minus ok but just th is assume that \nmaybe this vanishes and that is the worst case assumption right because i do not want it \nto vanish but i am assuming that the second term vanishes but even then with the first \nterm i will have a gradient which is proportional to the gat e why is that fine so \nremember that i am not making a easy assumption i am making a worst case \nassumption this is not favourable to me i am saying that the second term vanishes and i \ndont want it to vanish but i am just trying to prove that even in the worst case by the \nsecond term vanishes you still have this gradient f t from the first term right \nand why is that good why is that ok because f t decides how much flew in the forward \ndirection and it is also deciding how much goes back in the backward direction so it is \na fair regulator with says that if i passed on only this much information in the forward \ndirection then during backward pass also i should only make a responsible by this \nmuch ok now let us look at a situation where you had f one f two f three upto f t and all of \nthese gates were zerofive now zerofive seems a reasonable value but when we have zerofive raise to t \nand t is a large value what is going to happen this quantity is going to vanish \nso what is happening is that sone contribution to st i n the forward direction itself had \nsone\u2019s contribution to st in the forward direction itself was had already vanished right \nbecause it was continuously getting multiplied by zerofive zerofive zerofive so it is like this chinese \nvespring problems right so this guy said something whereas next guy added noise the \nnext guy again added noise and so on till the time it reach the t th guy this information \nwas completely lost so in the forward pass if sone did not contribute to s t in the \nbackward pass should i make it responsible for the crimes of s t no \nso what is happening in the backward pass again the gradients are getting regulated by \nthe same forget gates so again in the backward pass will have a situation that by the \ntime the gradient reach is sone it would be zero five raise to t and that is fine it is going to \nvanish but that is because even in the forward pass it vanished so let it vanish in the \nbackward pass also so this kind of vanishing is ok \nrefer slide time sixfortyone \n \nso this is just the same thing writte n in words so if the stated time t minus one did not \ncontribute much to  the state at time t because f t was tending to zero right then during \nbackpropagation the gradients flowing into s tone minus one will also vanish because again \nduring backpropagation the gradients will get multiplied by ft and they will vanish  \nbut this kind of vanishing gradient is fine this is fair because if we did not contribute in \nthe forward direction why should i help you hold your responsible in the backward \ndirection right so  that is fair so the key difference from rnns is that the flow of \ngradients is now controlled by gates which give the same regulation in the forward pass \nas well as the backward pass right so only if you contributed to something you will be \nheld responsible if your contribution vanished your responsibility in the backward pass \nwill also vanish right so that is the intuition  \nrefer slide time seventwentyseven \n \nand will next see an proof for this a proof actually it s as based on the intuition but i \njust make it more formal in terms of introducing the notations and so on so that problem \nwe will do it in the next class ok"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 14.3 How LSTMs avoid the problem of vanishing gradients.wav", "duration": 450.06, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 one hundred and eleven \nhow lstms avoid the problem of vanishing gradients contd \n refer slide time zerotwelve \nok so will start from where  we left off so in the last class we started with this \nmotivation that recurrent neural networks have this problem of vanishing and grade \nexploding gradients and we wanted to arrive with some principle way of avoiding this \nso you have first started wi th this intuition that in many real life situations like for \nexample the human brain or the whiteboard we tend to these to these three operations \ncalled selective read selective write and selective forget and they essentially help us in \ndealing with t hese finite sized memories right or whether it is a whiteboard which is \nfinite sized or your brain or whatever it is right \nso can we is it possible to kind of improve rnn\u2019s which also suffer from this problem \nthat they have this finite sized memory and hence if you are trying to capture \neverything from time step one then by the time you reach time step t where say t is thirty or \nforty or so on its quite natural that whatever you have learned earlier will get move off to \nan extent that it just is not recognisable anymore right \nso you wanted to deal with this problem and with that we motivated selective read write \nand forget and then we introduced some equations or converted this into a model and \nthis is the diagram that you see is the model actually that is t he lstm cell yeah and it \nhas these three gates output gate input gate and forget gate and which perform these \nthree functions of selective read write and forget so intuitively all these was fine but \nwe need to be more technical in terms of you tryin g to deal with a problem of vanishing \nand grade exploding gradients \nso how does it solve that problem all that makes or the story seems fine but how does \nthis actually relate to the math so we saw some intuition for that and the intuition \nhinsed on th is observation that during forward pass the gates control how much of \ninformation passes from one state to another and in particular if you have the situation \nthat from one time step to another say the forget gate tells you that keep forgetting point \nfive of the previous state then by the time you reach say the one hundred state you would have \nforgotten zerofive raise to one hundred of the first state \nso that means even during forward pass the information from state one vanishes so if it \nvanishes during backward path that is also fine because state one did not contribute to state \none hundred and that was the intuition that all this hinsed on now we are not going to do much \ndifferent from this intuition we just going to see the corresponding equations for these \nintuitions and just make a more i would not call it rigorous but more mathematical proof \non why lstm solve the problem of vanishing gradients \nand we are also sure that they actually do not solve the problem of exploding gradients \nand then we will see a simple trick of dealing  with exploding gradient that is what we \nwill do in the remainder of this particular lecture and then will move on to the next \nlecture in this lecture \nrefer slide time threetwo \n \nso we will now see an illustrative proof of how the gates control the flow  of gradients \nright \nrefer slide time threeeleven \n \nso we call that this is the control this is the flow diagram or the dependency diagram \nthat you had for rnn\u2019s and in particular because you are dealing with an ordered \nnetwork we add this explicit and impl icit derivatives and finally you came up with this \nmultiplicative form and this term here is actually a matrix because it is a derivative of a \nvector with respect to a vector \nand then this same matrix was getting multiple times and then we did this proo f it \nshowed that this term is actually lambda gamma ok it is actually proportional to this \nterm right and as if lambda into gamma is greater than one then this will explode if it is \nless than one then it will vanish given sufficient times that is \nrefer slide time threefiftysix \n \nnow in particular what is happening here is the following that you have this loss at time \nstep t you have the time step is four now what if this loss or this error occurred because \nw was not good enough to compute a good value for sone ri ght so w was at a certain \nconfiguration based on that you computed sone and that sone was not good enough which \neventually led to the error at time step four all of you if you can imagine this situation that \nyou mean you not being not be able to do something w ell at sone now this needs to be \ntold to w so that it can improve right and that information has to come through sone that \ninformation is already going from here but this information is about how badly it \nperformed in computing sfour \nthis is not how badly it  can perform in computing sone so that information has to travel \nto w all the way through sone and that was not happening because this path do not look at \nthe bullets this path was actually vanishing and that is what this multiplicative term says \nthat as the number of times that increased that time that path would vanish ok so that is \nthe actual problem that we are trying to deal \nrefer slide time fivefive \n \nso now what is the general situation here right the general principle is that the gradient \nof l t heta at particular time step say here we are considering lfour so i will just call it lt \nwith respect to any parameter theta i the parameters at w u v b and c with respect to \nany parameter it would vanish if all the paths leading to that parameter if it vani shes so \nwith respect to this particular path so that is the only path which leads to w through sone \nif there were multiple paths if there was say one such direct path right if we had you \nsome other kind of connection which gave us this direct path then it  would still have \nbeen fine \nbut there was only one path leading to w through sone at the gradient vanishes along that \npath then the gradient will vanish ok if there were multiple paths then only if the \ngradient varnishes across all the paths then the gradi ent would vanish is it fine what is \nthe corresponding rule for exploding gradients if there are multiple paths the gradient \nwould explode if \nstudent refer time sixsix \nif it vanishes through any one if it explodes though any one of the paths ok \nrefer slide time sixeleven \n \nso these are the two things that we need to consider ok so to prove that in the case of l \nst m this does not happen for the first case will have to show that there are at least one \npath through which it does not vanish and for the second case because we are going to \nshow that it explodes we just have to show that there is at least one path through which it \ncan explode ok so these are the two things that we need to prove and the first thing that \nwe are going to focus on is the vanishing gradient problem \nrefer slide time sixthirtyeight \n \nso will start with the dependency graph for lstm\u2019s that means i want to draw \nsomething similar for lstm\u2019s involving all the different elements in lstm so what \nare these different elements th e two rhyming things one being gates states ok so gates \nand states that the two things that we care about so let us look at all these so starting \nwith states at time step k  one at time step k one you have this two states s k one and hkone ok \nusing hkone you are going to compute the output gate at time step k and it is als o depends \non these parameters wo uo and bo right which is obvious from the equation \njust to make sure that this diagram remains tractable i am going to get rid of the \nparameters and i will come back to them later so right now will just focus on the states \nand the gates ok and then you have these other intermediate states and the other gates \nright so you had f k you had i k so add these three gates the temporary state and then \nwhat else what are the other two things at time step k so we saw this diagram about all \nthe computations which happen at time step k right how many computations happen \nthree states and three gates right \nso you seen the three gates and this one temporary state so which are the other two \nthings there is no selective forget with you guys is early everything forget hint look at \nthe grey cells and change the time step what will you get away are you all i mean we \ndid lstm\u2019s two days right i mean are you al l with that or should i we need to revise \nsomething mean i do not need to revise it but we going to is it fine ok so sk and the \nother thing h k remember that s k also depends on h k just stare at this for thirty seconds and \nmake sure that you are with it righ t all the equations are there these are the see six \nequations that or the six computations which happen at time step k \nthere are three gates and three states and the dependency graph is obvious from these \nequations except for the fact that i have ignored th e parameters how many if you are \ncomfortable with the equations and the graph corresponding graph please raise your \nhands high so i think it should be right we have these six equations and we have this \ndependency graph \nrefer slide time eightfifty \n \nnow starting so what happened in the graph is we started from s kone and h kone and we \nreached sk and hk which were the outputs at the next state now what will happen from \nhere we were looking at recurrent neural networks recursion is the answer what will \nhappen now \nthe same graph will keep recusing right for the next time step and up to the last time \nstep right does that makes sense ok this looks much more complicated than the \ndependency graph that we had for rnn\u2019s right by just because there are so many we in \nrnn we just had this one state and no gates so here but we have these three states and \nthree gates that is why this so many paths ok now for simplicity what i will do is i will \nnot draw separate nodes for the parameters all the in the case of the r nn dependency \ngraph i had drawn them separately what i am going to do is i am just going to put the \nparameters on the corresponding edges right \nso fk actually depends on wf it also depends on uf and it also depends on that bias but \ni am just going t o take a small set of parameter i am only going to focus on the w\u2019s not \nthe u\u2019s and the biases ok there is only for illustration for no other reason right and \nwhatever arguments or proof that we are going to see it holds for all the parameters but \nwe jus t need to prove it with respect to one parameter and the same story repeats for \neverything ok so this is the dependency graph and these are the parameters now what i \nam interested in knowing is that there was some loss at time step t and maybe that los s \nhappened because wf was not good enough to compute sk \nof course wf computes fk and then fk helps in computing sk but maybe wf was not i am \njust short it short circuiting it and saying that w f was not good enough to compute s k \nright and that is why i w ant the gradient to reach to w f through this s k that is what i \nwant do ok \nrefer slide time tenfortytwo \n \nand this exactly what i said i am interested in knowing that if this loss can reach w f \nthrough sk right so all the three highlighted things that what i  am interested in i  am \ninterested in the path to w f through sk of course there are many other paths to w f but \nthey do not account for the problem in sk is that fine everyone is clear the setup ok \nnow and we can ask similar questions about all the oth er parameters the w\u2019s the us the \nthe input gate parameters the output gate parameters and so on right there is nothing so \nspecial about w f the same question holds for all these other parameters also ok now \nhow does lstm ensure that this does not vanish so let us see that \nrefer slide time eleventwenty \n \nas i argued earlier it is sufficient to show that this gradient does not vanish ok if i can \nshow that this gradient does not vanish then i am pity sure there is only there is no \nrecursive connection here be cause it just a single connection so there is no recursive \nconnection here so if i can show that the gradient reaches up to this point then after that \ni can be sure that it is going to reach wf everyone buys that set up right that is what i \nneed to show \nso to prove that the gradient reaches w f i just need to show that it reaches s k that is the \nonly thing that i need to show and the first thing i am going to observe is that there are \nmultiple paths to reach to sk which  are these paths one through s kone because s k \ncontributes to skone the other through \nstudent hk \nhk which is visible but now also notice that how many paths are there to reach h k itself  \nfour not four actually that is going to be combinatorial because there four outgoing edges \nfrom here but then again there will be four next stage and four next stage and so on \nright so let us not count the number of paths but let us just convince ourselves that \nthere are many paths to reach to s k from lt\n\uf071  everyone is convinced about that we are \nnot counting the exact number of paths that is not very hard to do but all we are saying \nis that we know that there is one path through s kone one path through h k and h k itself \nseems to have many incoming path during back propagation \nso there are many paths which are reaching from lt\n\uf071  to sk everyone is convinced \nabout that anyone who has a problem with that now to show that the gradient does not \nvanish what do i need to show of all the paths the set th ere exist at least one path through \nwhich the gradient can flow that is what i need to show ok even if i vanishes across all \nthe other paths i am still fine with it ok \nrefer slide time thirteenfive \n \nso now consider one such path which is this highlighted path that is a valid path to reach \nto sk now let us denote the gradient along this path to be t naught and the total gradient \nis going to be a sum of many such paths right so i am calling this path as t naught and \nthis is what the gradient look like ok so this is simple just this red path the next red path \nand then the series of problematic multiplications right you have this recursive \nmultiplications again so everyone agrees that red is good the red path there is no \nrecursion the gradient will flow right we just need to focus on the blue path everyone is \nconvinced about that right ok \nso that is good the first term is fine as i said because it directly connected to l t there is \nno recursive or no other intermediate nodes so the gradient will just flow through that \nthere is not a problem there and now we look at the other terms which is first is dh t  st \nand the other is this ok \nrefer slide time fourteenthree \n \nso let us look at  \n\uf0b6 ht  st what is this going to be tensor vecto r matrix scalar at this \npoint in the course i want a unanimous answer \nstudent matrix \nmatrix right and recall that in particular the equation was of this form ok so what is the \nderivative going to look like even without computing can you tell me someth ing \nprofound about it it will be a dash matrix big matrix how many if you say diagonal \nmatrix how many if you do not think it is a diagonal matrix please raise your hands \ntotal sum is never one so remember that ht is equal to htone httwo up to htd and you have ot \nequal to otone ottwo otd and st equal to stone sttwo std so httwo depends only on ottwo and sttwo right it does \nnot depend on in particular does not depend on any of the other st\u2019s \nso we have already seen this before in such cases whats the i j th entry of this matrix of \nthe gradient matrix derivative of hti with respect to stj which of these terms are going to \nbe zero wherever \nstudent i not equal to zero \ni is not equal to zero that means it results in a \nstudent diagonal matrix \ndiagonal matrix \nrefer slide time fifteenthirtyone \n \nso that is exactly what is written here and the diagonal elements are going to be this is \nthat fine everyone with this ok so now this diagonal matrix which contains this on \nthe diagonal i am going to represent it by the following notation is that ok  fine so \nthis is a diagonal matrix where every element is i mean this is actually a vector right \neveryone agrees this is a vector so this diagonal is this vector is along the diagonal of \nthis matrix how many if you get this nota tion if you do not get this you will not \nunderstand anything else \nrefer slide time sixteenseven \n \nnow let us consider \n\uf0b6 st  \n\uf0b6 stone ok this is what st is equal to so what is the derivative \nof \n\uf0b6 st  \n\uf0b6 stone  ft right ft right what else why no why are you rebelling what the i mean \nst only right if it is can you treat this as a constant no why because this is a dash \nnetwork \nstudent refer time sixteenthirtyfour \nso in an ordered network the derivative will have \ntwo terms which are those \nstudent explicit \nexplicit and implicit in the explicit term what you assume the other terms to be a \nconstant right fine so s t i mean s tilde t also depends on s tone so we cannot treated as a \nconstant so once again this derivative is going to contain an explicit term and an implicit \nterm now i am going to make a worst case assumption i making this assumption that \nactually the implicit term vanishes notice that this not favourable to me i am trying to \nprove that the gradient does not vanish the gradient is a sum of two terms i am saying it \nlet the worst case be that one of these terms vanishes ok \nso this is not a favourable assumption this is a unfavourable assumption w hich i am \nmaking so let us fine so i making the assumption that the implicit term vanishes so \nwhat is the explicit term actually \nstudent ft \nft and what kind of a matrix is that \nstudent diagonal matrix \nif you agree that it is a matrix first of a ll it is a diagonal matrix again and what is the \ndiagonal \nstudent ft \nft right so i am going to represent it as d of ft is that fine  \nrefer slide time seventeenfortysix \n \nso remember that the original equation had three terms all of these the last blue onc e \nfor all identical so this is not problematic because this is a directly the last layer this \nwe have already derived a form this is sum diagonal and now for each of these we have \na form do you get that these are the three paths that we have done so f ar so let me just \nsubstitute them this is what it looks like ok now this is a product of diagonal matrices \nwhat will the product look like \nstudent diagonal matrix \na diagonal matrix and each element would be each element on the diagonal would be a \nstudent product \na product of all those things right so is it fair if i write it as this right which i can write \nit as this ok now just stare at this equation and tie it back to the intuition that we \ndeveloped something about the gates regulating the fl ow of information you have a \nmultiplicative term here right whenever there is a multiplicative term we have a \nproblem because remember these gates are between zero to one \nso there is a chance of vanishing everyone sees that you are multiplying t terms al l of \nwhich are between zero to one so there is a chance of vanishing but i am going to end this \nproof by saying that the gradient does not vanish so by what am i going to do now ok  \nmake the statement the gradient could vanish but this kind of vanishing is fair what do \nyou mean by that now when will the gradient vanish \nstudent product \nat this product of the forget gates vanishes but if the product of the forget gate vanishes \nthat means what would have happened during the forward pass that informati on was not \ncarried all the way back all the way front two times step t right do you see that ok so \nthat is the main reason here right so the red term does not vanish the red term time \nzone vanish the blue term can vanish but it will vanish only if d uring the forward pass \nalso this multiplicative term at cause the information to vanish by the time you are reach \nthe time step t how many if you get this and this exactly what i meant earlier by \nsaying that \nrefer slide time nineteenfortyfive \n \nif during the forward pass st did not contribute much to s tone because the forget gate was \ntending to zero then during backward pass there is no need to pass this information back to \nst right because during forward pass you did not contribute so during backward pass \nwhy should i hold you responsible right and this is absolutely fine to do this and this is \nexactly what the equation tells us that they gauze the gradient will vanish only if things \nvanished in the forward pass ok and the gates are doing the same regulation in  the \nforward pass as they will do in the backward pass so everything is fair is that ok \nand does there exist one path along which the gradients will not vanish when they do not \nneed to vanish so if during forward pass all the gates were on that means  the \ninformation from state one was actually carried all the way up to state t then during \nbackward pass what will happen the information will go all the way back right is that \nfine so the gradients flow back only when required as regulated by the forge t gates and \nthis is fair because if you are regulating the same thing in the forward as well as the \nbackward direction then you are not doing anything wrong ok \nrefer slide time twentyfiftyone \n \nnow that is a proof for lstm solve the vanishing gradient problems or in other words \nthe gradients vanish only when required and not unnecessarily or arbitrary as is to \nhappen in the case rnn\u2019s now we will show there exist one path along which the \ngradients can explode right so let us show that path so consider this path ok a this \npath is again also active so i if we consider the path to h k there is going to be active for \nall the gates and all the states right so in whatever gates or states you are considering \nthis paths would be there \nand this is what this path looks like you have the derivative with respect to the last layer \nand then you have these guys ok these pairs h t by ot ot by htoneand so on everyone fine \nwith this so far w hat is the derivative of h t with respect to ot we do not remember the \nequations so i will just tell you directly so based on whatever we have done so far just \ntrust me that this is what each of the terms in the bracket looks like we can go back and \ncheck this this is just comes directly from the equations now what is happening he re \ndoes this look very similar to the situation that we had with rnn\u2019s \nwe had a diagonal matrix and a weight matrix and a repeated multiplication of these \nright and again the diagonal matrix is bounded the weight matrix is bounded so now \nthe repeated multiplication could explode is that fine so it does not solve the problem of \nexploding gradients but it solves the problem of vanishing gradients but now still this is \nbad for us right whether the gradients explode or vanish our training is going to g et \nmessed up so how do we deal with this for exploding gradients what will you do \nstudent refer time twentytwotwentyseven clipping \nwe will do \nstudent clipping \nclipping right \nrefer slide time twentytwothirtytwo \n \nso in practice the way of dealing with this is gradient c lipping if the norm of the \ngradient exceeds the certain value then we are going to just clip it to a certain threshold \nand this is fine because we care about the gradients only for the direction and not for the \nmagnitude anyways when we introduce a learning rate we are doing some kind of scale \ndown for the gradient magnitude so this is just being more explicit and being careful \nthat if the gradients are non manageable in terms of their magnitude \nthen we just going to keep them to some manageable val ue while being faithful to the \ndirection and the direction is what matters so that is why exploding gradients is easy \nbut in the case of vanishing gradients you do not have direction also because the entire \ngradient becomes zero so there is no direction there right so that is why vanishing \ngradients is more serious than exploding gradients and as long as lstm solve that they \nare fine with it is that fine ok so that is the end of this lecture"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.1 Introduction to Encoder Decoder Models.wav", "duration": 1274.9, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 fifteen \nencoder decoder models attention and mechanism \nand in this  lecture we are going to talk  about encoder decoder models and attention \nmechanism so this is a very interesting lecture at least interesting to me because this is \nvery put all  these pieces  that we  have  learnt  so far  right we  have  learnt  three types  of \nnetworks feedforward  networks recurrent  neural networks  and  convolutional neural \nnetworks and we have seen independent applications of each of these  word to vec  and \nimage classification and so on now  today what we are going to see  is how  do we  do \ndifferent combinations of these networks and come up with a wide range of applications \nlike apply them to a wide range of applications ok so let me start by an introduction to \nencoder decoder models and then we do various applications of encoder decoder models \nrefer slide time zerofiftythree\n \n\nso what we are going to do is we will start by revisiting the problem of language \nmodeling so the problem of language modeling was that you are given some t  one words \nor characters and you want to predict the t th word or character right this is like auto \ncomplete in short right whenev er we are typing something you have type four words you \nwant to predict the fifth word or you have typed four characters and you want to predict \nthe fifth character ok  \nso more formerly this is what we are interested in how many of you get this equation \nthis expression so we are given a sequence of t  one words and you want to find out \nwhat the value of y would be at time step t and we want to find out that value which \nmaximizes this property that is what this argmax equation means and now we will try \nto see how to model this using a rnn  \nso let us see we are going to start with go that is that we want to start generating a \nsentence and then we will produce the first word which is i ok and what is it that we are \npredicting at this point what is th e network supposed to predict what is the output \nsupposed to predict actually \nit is supposed to predict a dash over the vocabulary a broadly distribution over the \nvocabulary right so this is what is happening we will of course come back to this on \nthe next few slides but you have say words wone wtwo up to w v in your vocabulary at \nevery time step you want to find a distribution over these words and then pick the word \nwhich had the maximum probability at that time step right that is exactly what thi s \nquantity is that is what we want the rnn in to model and then we want to keep doing \nthis till we reach the end of the sentence ok so that is the language modeling problem \nand as we had made a case for it earlier the word produce the time step t depen ds on a \nfew previous words how does a recurrent neural network ensure that at any time step \ni am going to give it only one word as the input  \nso how does that ensures that it depends on all the previous words also through the \nrecurrent connections and the gate and sorry it is not the gate the state st ok fine \nrefer slide time threethree \n \nso we will see this of course in more detail and we will write down the model equations \nand what is happening so we are interested in this quantity which is the probability of \nthe word at the time at the t th time step where this j belongs to vocabulary v and see a \nvocabulary of tenk words or twentyk words for english it is actually much higher but say you \nare considering only tenk to twentyk words then we want to predic t a distribution over this \nvocabulary so using an rnn what are you going to do at the output layer is the \nfollowing is this correct how many if you understand this equation not many why \nwhat does this equation compute first of all softmax softmax means \nstudent probability distribution \nprobability distribution ok what does it take as input at every time step the state \nright what does it do with the state a linear transformation right and then a bias ok so \nwhat is this quantity scalar vector matrix vector of size  \nstudents refer time fourzero \nthe refer time fourone what is the gth element of the that \nstudents probability of the gth word \nthe probability of the g th word right so i just have to explain it in that many words \neveryone gets it now everyone gets it if you do not get it you will not understand the \nrest of the lecture i am very serious everyone gets it  \nso in other words what we do this entire y one to ytone which we were conditioning on we \nare just using st as a surrogate for that and that is fair because st has actually captured all \nthe previous information that we had now just using st as a state which captures \neverything that happened so far \nso that is actually how we are modeling this and the recurrent connecti ons ensures that \nst captures everything which has happened so far \nrefer slide time fourfortythree \n \n so now let us look at the five things that we have in a typical supervised machine learning \nset up which are those data model \nstudents parameters \nparameters \nstudents objective function \nobjective function \nstudent cross refer time fourfiftyfour \nvery good no someone said objective function and then loss function learning \nalgorithm right ok so whats the model here \nstudents refer time fiveone \nyou know wha t you are trying to model which is a property distribution what is the \nactual so here y is the probability distribution and your x is the input given to you can \nyou tell me what\u2019s and we have already set always said in this course that whatever be \nthe y whatever the be the x we are interested in this function x sorry function f and we \nshould be actually expressively we able to write this function so what is the function \nhere can you actually write down the set of equation just think of what the o utput is \nhow you are going to reach the output given this network what is yt going to be what\u2019s \nthe equation for yt  \nand then try to go back all the way back to xt so yt depends on something that \nsomething might depend on xt so how do you go all th e way back right that is the \nthing which i expect you to do ok how many of you get it now please raise your \nhands ok so let us see at every time step what am i interested in predicting \nstudents probability distribution \na probability distribution that means i will have to compute which function \nstudents softmax \nsoftmax so the green vector is what i am going to focus on so what\u2019s the equation for \nthe green vector is this fine now what does this contain apart from the parameters st \nhow do i get st is it fine you can write now you have written this output y as a \nfunction of x because x appears here or other yt as a function of xt is not it is straight \nforward right once i show you the answer is should be how many of you get it now \nplease raise your hands high up above okay what are the parameters b and c right \nso these are the parameters what\u2019s the objective function cross entropy or dash of \ncross entropies \nstudents sum of cross entropies \nsum of cross entropies right so the loss is going to be over all the time steps at every \ntime step is the cross entropy loss right everyone gets this ok what\u2019s the learning \nalgorithm back propagation \nstudents true time \ntrue time fine so that is what it is going to be right eve ryone is clear so you can see \nthat we have written the final output as a function of the input right and this is end to end \ntrainable that means the gradients can flow modulo this vanishing exploding radiant \nproblem and we have a way of handling that we can replace rns by lstms that is all \nright  so that is what it is now this just make sure you understand this properly so \nthat we are going to do various instantiations of this for different problems ok \nrefer slide time seventhirtyone \n \nnow here is one q uestion we all smartly wrote this xt but why is the input at every time \nstep when i am predicting home the input was at but how did i get at that is what i \ndash at the previous time step predicted at the previous time step right \nso this is what th e input looks like so at time step one i predicted i as the output at the \nnext time step i am going to feed that has the input does it make sense so just see if \nyou are doing auto complete you would select that i am fine with the word i at this time \nstep so it is going to take that as the input and then try to predict the next word that is \nwhat exactly is happening here and now you are predicted am at the next time step you \nare going to feed am as the input and continue this chain throughout ok \nso the input at every time step is going to be the word that you have predicted the \nprevious time step and i am just going to represent it by a one hot vector right it is the \nindex of the jth word only that could be hot everything else would be zero and all of yo u are \nfine with this no so at training time this is the inference time at training time we will \nhave the real inputs no that is at inference time at training time we will just use that \nthrough because training time you know what the inputs are right  you know the true \nsentence you have the wikipedia sentence right and you know what the true sentence is \ngoing to be \nwhat i am talking about how will you generated at test time at training time you know \nall these things right no about training time h ow will you do that you will know what \nthe next input is right so now ok so i said that the input is going to be a one hot vector \nis everyone fine with that one hot vectors are ok what else could you use \nstudents word representation \nthe word representation for that right so assume that you have already done the word \nto vec assignment and you have completed all the word representations and you have \nthem with you now but instead of feeding the one hot representation of the input you \ncan just feed t he word representation of it does that make sense one hot representation \nis just one of the many representations possible for the world so why just do that you \ncould do s v d you could do one word vec or whatever you want right so that is in \npractice what we will feed is the word to vec representation \nso everyone gets this what is happening at every time step  \nrefer slide time ninefortyfour \n \nnow one more thing that you need to notice that s zero which is the input at time step one \nthe previous so s oneone so that we do not know what it is so we just keep it as a \nparameter we say that s zero is also weight vector and you are going to learn it along with \nall the other parameters in the network does not make sense because you do not know \nwhat szero means is a semantics of it is not clear like  what was generated at the zero th time \nstep we do not really know right so will just make it a learnable parameter and that \nwould be trained along with all the other parameters of the network \nrefer slide time tenseventeen \n \nso before we move on what we are going to do is we are going to see a very compact \nrepresentations for rnns grus and lstms so remember rnn is the following \nequation rnn is defined by the following equation the s t is a recursive function of s tone \nand xt right so i am just going to write it as that s t is equal to rnn of s tone xt instead of \nwriting all these parameters and sigma\u2019s and all that i am just going to write it \ncompactly as this now this is what what is this gru so how may going to write it as \nstudents gru \ngru of \nstudents stone xt \nstone xt \nwhat is this \nstudents lstm \nlstm how may going to write it lstm of when the output of the lstm is both h tone \nand stoneright fine so in some sometimes i will just say st sometimes i will s ay both ht \nminus ht and st as per whatever i needed right so this is i am not going to write these \nequations and parameters again i will just say that lstm of this assume that is a \nfunction which does this calculation and gives you back ok ok \nrefer slide time eleventwentytwo \n \nso far what you have done is we have seen how to model the conditional probability \ndistribution given the previous t minus one words now let me give you a different \napplication right what if we want to generate a sentence given an i mage so this is what \ni am interested in doing i am giving an image and i want to generate a sentence can we \njust think of it formally what is it that you want to do so we saw that in this case \nformally we were interested in this conditional distrib ution in this case what is it that \nwe are formally interest in \nif i were to write it as something formal what would i write it as ok i will give you a \nhint what kind of a distribution is this a conditional distribution right given the \nprevious sequ ences previous s equence of words generate the t th word now in this \nsituation can you stated an similar words given the \nstudents image \nimage generate the \nstudents sentence \nsentence or given the image and the description that are generated so far be cause i am \ngoing to write the description one word at a time given the image and the description that \nhave written so far generate the next word in the mission so what kind of conditi onal \ndistribution is that pyt given \nstudents y one to t minus one \nyone to t minus one \nstudents comma \ncomma \nstudents image \nimage does that make sense everyone gets that ok so what so this is what we want \nright so here now we are interested in this quantity as a post to this quantity does that \nmake sense ok and this is again a conditional distribution  \nrefer slide time thirteentwo \n \nso earlier how did we model this we just modeled it as the following we said that the \nwhole context of yone to yt minus one is just contained in that blue vector which is st right \nso remove this variable and replace it by a vector does that make sense ok \nnow you have the image also so how are you going to model this so what are you \ngoing to write on the right hand side ok let me give you a hint we all agreed that this is \nthe quan tity that we are interested in right we also agreed that the following is fine \nreplacing yone to t  one by st is fine now what about the image what do you mean by \nobjection in the image you will supply the words which are there the object names that \nman fine that is fair enough well if want to make it more abstract more neural so what \nyou are saying is that whatever information is contained in the image should be passed \nhere \nwhatever information is contained in the image should be passed here how do  you \nwhat\u2019s the way that you have learnt of computing the information in the image a dash \nneural network \nstudents refer time fourteenfifteen \na \nstudents convolutional neural network \nfeedforward neural network \nstudents convolutional neural network \nconvolutional neural network ok so but what from a convolutional neural network \nhow many representations that is a convolutional neural network learn how many does \nv g g network learn v g g sixteen the last layer is a softmax layer fifteen right so which one \nwill you give now one before the last one that is called the \nstudents refer time fourteenthirtysix \ndash layer  dash dash layer fully dash layer \nstudents fully connected layer \nfully connected layer ok at least your language moral works fine ok so that is the f ully \nconnected layer remember that all the layers in the convolutional neural network learn \nan abstract representation of the image and as she was trying to say that this abstract \nrepresentation contains or at least you believe it contains all the infor mation that is there \nin the original image just as st contains all the information that was there in the sequence \nyone to t minus one this abstract representation that we will get from a cnn contains all the \nrepresentation all the information that is there in the image we all believe that ok \nand we also believe that any of these representation is fine in practice the convention is \nto use the fully connected layer that is called as f c seven the seventh fully connected layer \nright and it is seven because you al so start the numbering from the convolution layer one two three \nfour five and then the seventh layer ok so that is what you will take ok so does this make \nsense and it is a very simple extension from what we were doing earlier this is what i \nhave circles is what we were doing earlier right where we only had st now i am saying \nis just as you believe that st and codes all the information in the previous sequence \n i am just asking you to stretch that a bit more and say that f c seven of the image contains all \nthe information that was there in the image is it fine  \nrefer slide time sixteenone \n \nok but still there are some issues and there are other ways of making this condition on f \nc seven in particular what you could have done as she was trying to suggest initial is t hat \nmaybe you have a vocabulary of all the objects that are possible in your image right so \nmaybe in your image there is man woman there is flying desk frisbee or there is dog \ncat and all these things right \nso you do an object detection first get o ut all the object which are there and then make \nthe distribution conditional on these objects right so you can say that i will allow for a \nten words to describe the image so there word one is equal to man because i have \ndetected the object man in the ima ge word two is equal to frisbee because i have detected \nthe object frisbee in the image that is all that is another way of doing it ok so i just \nwant to make it clear that there are different ways of making the conditional distribution \nconditional on the image itself we are choosing to make it conditional on f c seven of i right \nthat is the neural way of doing it ok \nrefer slide time sixteenfiftynine \n \nso let us see two such options the first thing that we coul d do is we could set s zero to fcseven of \nthe image what is s  zero the first thing that was passed to the language model ok so \nremember we had this go symbol and we had this s zero which was mysterious we did not \nknow how it comes but now we know it that s zero could just be the image that is what my \nstarting point is so take this image and now start generating the representation \ngenerating a description does that make sense ok so this is what the network looks \nlike \nso what do you saying is that these things are of dimension d the cns output was say \nof dimension four thousand and ninetysix so this has to be converted to size d right that means what will you \nhow will you do that we have a four thousand and ninetysix dimensional vector and you want to convert it to a \nd dimensional vector w belonging to \nstudents refer time seventeenfiftyone \nfour thousand and ninetysix was d fine in g eneral any two vectors if you want to make them compatible this is \nwhat you will do you will project so that they are of the same dimensions x zero will be \nthe go symbol go is the special word in your vocabulary which says star generating the \nsentence right so whatever vocabularies you will add two special words right one is go \nand other is stop so whenever you generate stop you stop generating after that fine \nwhat is the other way of so here now what happens is so this is what is happening \ntechnically and that is why that is what i wanted you to understand this now s one depends \non szero ok what we are interested in is the following that yt should be conditional on yone to \nt minus one comma image ok \nthey have make sure that i is s zero and this quantity is st you have to find now since the \nfirst time step depended on the image all subsequent time steps will depend on the \nimage is that ok what is the other way of doing this what now in this looks slightly \ninefficient what\u2019s the other option that you cou ld have used just feed the image at \nevery time right so that is the one constant thing that this is the image now whatever \nyou have generated so far considered that but in the addition to that also consider the \nimage \nrefer slide time nineteenfourteen \n \nso what would the diagram look like just passing the input to every stage of the decoder \nok \ni have already started using terminology which have not introduced but i will just \nintroduce it shortly \nrefer slide time nineteentwentyone \n \nso let us look at what the fu ll architecture looks like there is something known as the \nencoder which takes your input encodes it and gives you a representation right then \nyou have something known as a decoder because given this input you want to decode \nwhat the output is right s o remember general terminology would be whatever input is \ngiven to you you want to encode it and whatever is the output that needs to be decoded \nright it is you could think of it that this is the image now i am trying to decode the \ndescription for the re is that fine ok and then you have an rnn which is used to \ndecode the sentence from this input \nso such architectures are known as encoder decoder architecture and these are become \nextremely popular and we will see why they are so popular and why the y have led to the \npopularity of deep learning in general ok so everyone understands this diagram anyone \nwho does not see a problem with this diagram there is actually no problem but i want \nyou i want you to see beyond the diagram and to look at the equations what do you \nmean by that what do i have here as the input what is my x so this this looks fine i \nhave taken one box and connected it to another box and everything is fine right but that is \nnot what i am interested in \nwhat am i interested in  can you write the input as a the output as the function of the \ninput in this case is it possible to do that so that is what we need to make sure that we \nare able to do right so we look at various applications suggest lepted criptical here but \ni am going to come back to it so i just the emphasis that look beyond the diagram the \ndiagram looks very nice i hope it does thanks to the ta\u2019s but it does and but we need \nto understand what is the what is the set of equations being conveyed through thi s \ndiagram right what is the function that we are trying to learn we are going to write y \nas a function of x are we able to write that function because now we are suddenly \nthrown in a convolution neural network at some place we have an recurrent neura l \nnetwork then we have the feed forward layer at the output which is the green vectors \nso does all this combined together right"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.2 Applications of Encoder Decoder models.wav", "duration": 1013.14, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 one hundred and thirteen \napplications of encoder decoder models \n so we are going to see a lot of applications of the encoder decoder models \nrefer slide time zeroseventeen \nand for all these applications we are trying to answer the following questions what \nkind of network can we used to encode the input in the previous application what do we \nuse cnn what kind of network can be used to decode the output what did we use \nstudent rnn \nrnn what are the parameters of the model we will see that and what is an appropriate \nloss function right \nrefer slide time zerothirtyseven \n \nso let us again go back to this task which was image captioning what is the input what \nis the trai ning data given to you what is x what is y x is the image y is the \ndescription right so this is what is given to you given n such training pairs where x i is \nthe image and y i is the description and y i in itself is a sequence right so you have y  i \none to y i capital t everyone gets the input and output \nnow what is the next thing model can you write down the model equations i want an \nequation which starts from x and goes all the way up to y and since we have several time \nsteps i want an equation for y t this is generic for every time step right say can you write \nthat equation and feel free to use shortcuts so you do not need to write the entire rnn \nequation just say rnn of something dont even try to write the vgg sixteen cnn equations \njust say cnn ok so we will go ahead \nthe first thing that i am going to do is i am going to write the equation for the encoder \nso the encoder gives me cnn of x i whatever is the input given to me x i is the i th \ntraining image given to me so i will just pass it through cnn i will get a representation \nfor that and i am just being  cryptic here it could be the f cseven representation or the con five \nrepresentation or the max fool five representation or whatever you want right and it is \ngoing to denote all of this as cnn of x i run this cnn take whichever representation \nyou want to take \nnow what is the decoder going to be decoder is the following rnn rem ember the \nequation of rnn was s tone comma x t what is the input of the t th time step whatever we \nare predicted at the previous time step just the embedding of that so e means \nembedding if you want take one odd embedding if you want take word  to vec \nembedding is that fine ok \n and then what is the output it is the soft max function of the following how many of \nyou get this now please raise your hands how many of you can say that y can be written \nas a function of x is that pretty straight forward ok so you have an encoder you have a \ndecoder and remember that this final y is a composite function of the original input x ok \njust that you are doing too many computations along the way but there is a path which \nexists ok \nwhat is the loss function everyone at this point should be able to say it \nstudent sum of cross entropies \nsum of cross entropies they just wai t for me to say two more sentences what are the \nparameters u v w b c \nstudent refer time threetwelve \na b c d e f laugher everything right what is that all the parameters of the \nstudent refer time threeseventeen \nconvolutional neural network that means all the filters that you have all the parameters \nof the rnn which is w u and the parameters of the output layer which is v right all of \nthese is that fine i am i may have missed some biases but ok the objective function as \nyou said is a sum of cross entropies where l t is the true character at time step true word \nat time step t and what is the algorithm that you are going use back propagation through \ntime and with back propagate all the way through the cnn also which is an end to end \nthing in pract ice of course you do not do that yes you could just said both to be the \nsame do you get that question is that ok ok \nrefer slide time fourzero \n \nnow let us look at another task we look at the task of textual entailment what textual \nentailment does is that i give you a input or premise the premises that it is raining outside \nand you need to tell me a hypothesis the hypothesis that the ground it is wet ok with \nbasically means that it is raining outside implies that the ground is wet ok now what is \nthe encoder decoder architecture that you will use for this problem what is the input \nhere  \nstudent a sequence \na sequence what is the output sequence it is all the hint that i am going to give you \nso what will you do what is the encoder equation going to be \nstudent rnn \nrnn what is the decoder equation going to be \nstudent rnn \nrnn how will it become end to end by setting what to what \nstudent refer time fourfortynine \nszero of the decoder to to what of the encoder \nstudent refer time fourfortynine \nlast time step of the encoder how many if you get that really we are on the same page \nfirst time in i do not know how many lectures by finally it happened so here is what \ntraining data is right it is a collection of premises and hypothesis and you have n of \nthese there are two options for the model the first option is that you encode the input \nusing an rnn feel free to replace and by an lstm if you want then you have the \ndecoder where and you set the zero time step to whatever you got from the encoder \nthen every time step you computed using the rnn where remember the input at every \ntime step is whatever you predicted at the previous time step and then the output is just \nthe soft max function is that fine and what is the loss function going to be los s \nfunction \nstudent refer time fivefortyfour \nsum of cross entropies training algorithm \nstudent back propagation \nback propagation through time and really it is through time right all the way back ok so \nwe will see that let me see if i had any other question ha ok i will ask it parameters i am \nnot going to bother about ok now this was option one i have just clearly written what is \noption two what is the set of equations look like for option two which of these equations \nwill change and how remember option two  was maybe pass the input at every time step \nwhich equation will change \n st what will it become but s t can take only i mean rnn take only two inputs right s \nt minus when you need to gave embedding you need to gave so how will you fit in the \nthird i nput this animation has it is own mind so this is how back propagation will \nhappen right so let us it is finish that so will actually back propagate all the way back \nthrough time fine really all the way back through time \nrefer slide time sixforty \n \nand same task textually entailment i want model two option two so this is what will happen \nwe will just concatenate h capital t which is this guy along with the input at every time \nstep right how many if you get this the rnn is still taking just two inp uts one is the \nprevious state the other is the concatenation of the current input as well as input that we \ngot from the encoder everyone get this ok so this is model two i am going forward i am \nnot going to do both model one and model two it is model two is j ust a very simple variation of \nmodel one a parameters loss function training algorithm everything remains the same ok \nrefer slide time seventwenty \n \n  \nlet us look at machine translation what is the input an english sentence what is the \noutput a hindi sentence what is the encoder going to be \nstudent rnn \nrnn what is the decoder going to be \nstudent rnnsixrnn what is the loss function going to be \nstudent refer time seventhirtyseven \nsoft max who said soft max \nstudent refer time sevenfortytwo \nwhat is the loss function going to be \nstudent sum of \nsum of cross entropies training algorithm all the way through time right ok so let us \ncan you draw can you write the equations just copy it from the previous slide right \nactually copy it from the previous slide right if you have the rnn you have the rnn as \na decoder again in option when you will set s zero to h t you have the loss function the \nparameters and your training algorithm ok and for option two it is back propagation will \nfine and for option two what will happen option two what will happen \nstudent refer time eightseventeen \nthis will change right so just focus on that we just passed in the last time say belong \nwith that  \nrefer slide time eighttwentyseven \n \nnow transliteration what is transliteration what is transliteration if you do not know it \nat least see it from the example what is it \nstudent refer time eightthirtyfour \nwriting the same word in another language right so this is typically done for named \nentities very when you are when you are translating from one language to another you \ndo not often for thomas you do not come up with an indian translation right you just say \nthomas in devanagari right you just right thomas in the devanagari right so for \nnames you typically just do a transliteration that means from the english script you just \nwrite it in the native language script ok what is the input one word the input is the \nword right what is it a sequence of \nstudent characters \ncharacters what is the output \nstudent sequence of characters \na sequence of characters what will you use for the input \nstudent rnn \nrnn what for the output \nstudent rnn  \nrnn so i will becoming too easy right can you write the equations for this yes you \nwill copy it from the previous slide yes ok everything remain s the same right so you \nsee why this framework has become so powerful you do not see it still maybe let us look \nat something else  \nrefer slide time ninethirtythree \n \nimage question answering tell me what is the data here what is the input image and \nstudent question \nquestion and what is the answer what is the output answer so for simplicity we are \ngoing to assume that the answer here is a finite vocabulary we are not generating \ndescriptive answers we are not being overly dramatic let us going to sa y one word what \nis the colour white we are not going to write i think the colour of the image is white \nnow ok just white so all these outputs are going to be single words and we have v \npossibilities and we are going to predict one of those v possibilities ok \nnow give me a model for this now things are getting slightly complicated you have \none image as the input one sequence as an input and a dash as the output oh god now \nthink why would you generate the sequence of characters as the answer i sai d that the \nanswer is going to be come from a finite vocabulary that means you need a \nstudent probability distribution \na distribution probability distribution is here enough said now tell me what is the \nmodel a model should connect the input to the  output you have two inputs here i see \nsome people doing this laugher i do not know what that means but let us just do it let \nus make a train simple formula simple recipe and whatever input you are given just \nencode it depending on the type of input  you know what is the encoding is going to be \nfor images what is encoding sequences \nstudent refer time tenfiftyfive \nnow what do you do with these two separate things \nstudent refer time tenfiftyeight \nconcatenate them ok and then \nstudent refer time elevenone \nafter that \nstudent refer time eleventwo \ncan you think of all the equations can imagine all the equations along the way \nstudent yes \nof course yes laugher right i mean imagination laugher you can always do that \nlaughter now just think about it can you write the output as a function of the input \nwhere the input is actually a pair now it is image comma question what is the model \ngoing to look like let us see so model will first have an encoder for the image let us call \nthat as \n\u02c6h i it is going to have an encoding of the question let us call it as it as \nh\n  i am \ngoing to concatenate these two as someone rightly gestured and then what am i going to \ndo after that pass it through a \nstudent feed forward network \nfeed forward network and predict a probability distribution what are the parameters of \nthis network parameters of the feed forward network the parameters of the recurrent \nneural networks and the parameters of the cnn right so everyt hing that we have done \nso fine because ok how do you train it back propagate through time and space ok also \ngo back to the image also fine is that ok ok \nrefer slide time twelvenine \n \ndocument summarisation what is the input sequence what is the ou tput rnn rnn \neverywhere fine i will not even bother to ask you  \nrefer slide time twelveeighteen \n \nvideo captioning sequence of images i want to hear the choice of phrasing that you use \ni just want to hear that i love hearing that every time \nstudent refer time twelvethirty \nrnn of cnns whatever that means what rnn of cnn every time i do this everyone \nsays rnn of cnn laugher i do not know what that means but it is the right answer \nwhat does it mean what is the video it is a sequence what is the sequence of \nstudent images \nimages so what will you do encode every image and then pass it through a \nstudent rnn \nrnn can you imagine the equations ok let us see ok and in this case what is the output \nagain the sequence so what is the decoder going to be \nstudent rnn \nso here is the model so first what you do is for every time step you compute the cnn \nencoding of the frame then you pass it through an rnn to get the final time step t and \nthen you feed it to a decoder and generate one word at a tim e is that fine so this thing \napparently is called rnn of cnns ok and so that is and loss function would again be \nthe same sum of cross entropies and back propagation through time and space ok good \nplease do not quote me on this thing also this is getting a recorded but ok \nrefer slide time thirteenfortyone \n \nthe next one video classification what is the decoder decoder is probability \ndistribution okay what is the decoder \nstudent feed forward neural network \nfeed forward neural network  \nrefer slide time thirteenfiftyfour \n \nok this one dialogue how are you i am not fine ok input \nstudent rnn \noutput \nstudent rnn \nright so you see this why this has become so popular we took a wide range of \nproblems different modalities right we took images we took video s we took sequences \na combination of these right image question answering has a combination of images \nand sentences and the output you have a probability distribution all of this could be \nmodel by this unified end to end network all of the components  are neural network \nbased components whether it is a convolutional network or a feed forward network or a \nrecurrent neural network right \nnow let me stretch this right what if you have video question answering what is the \ninput going to be sequence of images and sequence of \nstudent words \nwords the output is \nstudent refer time fourteenfortysix \nno just a word right we will pick from a fixed vocabulary how are you going to model \nthe input \nstudent refer time fourteenfiftythree \nrnn for the question rnn of cnn for the video then \nstudent concatenate \nconcatenate and \nstudent feed \nfeed it to your feed forward network right so all of these become to what extent that \nwork is a separate question but all it was not even possible to model all of this as an end \nto e nd network right but now i just because possible to model it as an end to end \nnetwork with all the components being neural components right and this is this story \nstill not complete everything is not as easy as it looks they still and very crucial \ncomponent that we have missing in the architectures that we have seen so far and we will \ntalk about that soon ok \nrefer slide time fifteenthirtythree \n \nnow let us just continue that i challenge you to do this pick up any problem there are \nstudent from relevant different departments pickup any problem do not say that i want to \ndesign some gear for certain aeroplane and all that and i want to use neural network to do \nso no something which involves machine learning right not problem is does not \ninvolve machine learn ing and see if you can model it using the encoder decoder frame \nwork just try to do this \ntake problem from biotech right for example they given a sequence of genes and you \nwant to predict whether this person is susceptible to a certain disease what wil l you do \nconduct a blood test ok do not do not laugher do not go and do neural networks for \nthat but if you had to do that this is what you will do it will take a sequence you will \ntreat the sequence of sequence at the given dna as a sequence of genes  and then you \nwill try to predict something as the output try to predict a probability distribution over \ndisease it a possible right \nso you can think of many applications from many domains and all of that you could \nproblems involving machine learning wi th potentially model than using the neural \nencoder decoder architecture but there is a very important part missing from this whole \nstory which is attention which is a very important idea and we will spend some time on \nthat in the remainder of the lecture s so we will first motivate why do we need attention \nand from there we will see that how do you make how do you integrate attention with all \nthese encoder decoder architectures that you have seen so far"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.3 Attention Mechanism.wav", "duration": 1617.42, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 one hundred and fourteen \nattention mechanism \nso let us go on to the next module which is attention mechanism \nrefer slide time zerofourteen \nso let us motivate not the task of attention let us motivate attention mechanisms with \nthe help of machine translation ok so what is happening in the models that we have \nseen so far the current model that we saw for machine translation by the way all t he \nmodels that i have shown you so far are wrong or rather incomplete we will complete all \nof them right and that is where attention fits in ok that was for the camera \na encoder reads the sentence and its computes the encoding once right we read the \nentire sentence and be encoded it and then we have these two options either the pass the \nencoding at the zero time step or pass this encoding at every time step is this how humans \ntranslate a sentence what is the human analogy for this you have read the se ntence \nonce done and now we are going to remember this entire thing throughout and then \ntranslate imagine if you doing this for sentences which have twentyfive words which is a typical \nwikipedia sentence what is wrong with this we have read the input ones and w e have \nencoded it what is likely to happen you will forget something you are going to lose \ninformation not just that is the entire sentence important that every time step \nstudent refer time onetwentyeight \nonly certain words are important you see this conce ptually something wrong that we \nare doing here is is saying ok i have encoded the sentence and then start decoding from \nthere ok thats the conceptual error that we are making so let us see how humans \nactually try translate it right \nrefer slide time onethirtynine \n \nso when producing one word in the output suppose my input is the hindi sentence and i \nhave the output sentence when i am trying to produce the first word i actually compute \nthis probability distribution which tells me which of the input word s that i need to focus \non at this point it is ok if i dont know what is the translation for ghar or ja or raha or \nhoon as long as i know the translation for mai i am done because that is the word which \ni first need to produce there right  \nso i am going to say that at this point i only need to pay attention with the first word in \nthe input and i can ignore everything else what about the second time step i just need \nto focus on the last word what about the third time step is it always going to be tha t i \nonly need to focus on one word at a time \nstudent no \nno what about the third time step \nstudent fl \ni am sorry i am assuming everyone understands hindi but i think that is this is small \nsentence i can assume that what will you focus on \nstudent ja rahi \nja and rahi right you want to focus on both these things and not an anything else and \nwhat about the next one hoon right so just on ghar and not an anything else is this \nwhat the model encoder decoder model is doing what is it doing actu ally the every \ntime step is focusing on the entire sentence because that is the encoding that your feeding \nto every time step that is the problem that we need to correct we need to learn to pay \nattention to certain important parts of the sentence is th e setup clear to all of you is the \nmotivation fine not your motivation layers is the motivation for this fine or not ok \nthe distribution actually tells us how much attention to pay to each input word at each \ntime step and ideally at each time step we  should face pay attention to only certain \nwords in the input \nrefer slide time threetwentytwo \n \nso let us revisit the decoder that we have seen so far this is what the decoder looks \nlike in fact i also have the encoder there now suppose sorry so currently what we are \ndoing is we are either feeding s zero at the i mean we have either feeding the input \nembedding or the encoder embedding at time step zero or at every time step the suppose \nthere was an oracle which told us exactly which are the words important at  time step t \nright so in our example at time step three suppose it told us that the word going is \nimportant actually we need to flip the input and output here also right but you can still \nunderstand right \nso i am saying at time step three certain words ar e important and suppose a oracle actually \ntold us that these are the words which are important what would you do assume that \nyou have already run the encoder what will you do now and say someone told you that \nonly this word is important word why weight ed i am just saying binary weigths right \nonly this word is important what would you do ideally \nstudent refer time fourtwentyfour \njust feed this blue vector to the decoder and do not feed everything else does not make \nsense suppose i told you that two word s were important send those two words but how \nconcatenate but now at certain time steps four words will be important and you cannot \nconcatenate four words right because then the dimensions will change so what do you do \nstudent refer time fourfortyseven \na weighted \nstudent refer time fourfortynine \nweighted some of the important inputs is that make sense at time stamp three we saw that \nja was zerofive important and rahi was zerofive important just a weighted combination of those \ntwo blue vectors and feed that to the decoder so you are not changing the dimensions at \neach time stamp because the blue vectors have the same dimension i am just taking a \nweighted combination of those i am going to give you the same dimension does that \nmake sense ok \nrefer slide time fivesixteen \n \nso in fact what i am saying is that i could just take a weighted combination of all the \nblue vectors that i have at the encoder and the weights of this weighted combination \nright now i am assuming that someone oracle has given me is that ok if i had his \nweights does this makes more sense then having the vanilla encoder decoder model \neveryone agrees with that ok \nnow the question of course us who is going to give us these weights we will come \nback to that later but at least given the weights this mak e sense so at every time step \nthey just going to focus on the words which are actually important just take a weighted \ncombination of those words and we will just feed that to the decoder and intuitively this \nshould work better because unlike before whe re we were overloading the decoder with \nthe entire sentence remember twentyfive words thirty words entire sentence was being passed to \nthe decoder now you are just overloading it with the amount of information that it \nactually needs to produce that particular word hence intuitively this should work better \nright ok \nnow how do you convert this intuition into a model \nrefer slide time sixnineteen \n \nin practice of course there is no angel who is going to coming give as these weights is \nno oracle the machine will have to learn this from the data whenever you need to learn \nsomething you need to \nstudent refer time sixthirtyfour \nintroduce parameters so i am going to now introduce a parametric form for the from \nthe figure which thing for those of we cannot see these are alpha one alpha two and so on \nso now from the figure we are going to introduce a parametric form for \nstudent alphas \nfor the alphas ok so i am going to introduce i will come to alpha but and what you \nthink this weight should depend on what i am tryi ng to say is that at the tth time step  of \nthe decoder so this is e jt at the t th time step of the decoder i want to find out how \nimportant is the jth word in the input that is exactly what i am interested in at every time \nstep of the decoder of all the in put words i want to see which of them is the most \nimportant right so this is the quantity that i am interested is how important is the j th \ninput word at the tth time step this should depend on what what should it be a function \nof \nfor one it should d epend on what that word is right the other is should depend on what \nhas happened in the decoder so far what is the decoder produce so what is the input \nand what is the decoder state at so far right so as the decoder has already decoded the \nword ghar or home it does not need to look back at home right that is why need to \nknow what is the state of the decoder what captures the state o f the decoder at time step \nt ht and what is the state of all the words that we have it is captured by what th e \nhj\u2019s right this is h one h two h three h four does that make sense how many of you have fine at \nthis point please raise your hands high above ok how many of you have questions \nplease ask specific questions if you have a question \nall i am saying is a couple of things one is at every time step instead of the oracle giving \nme these weights i want to learn these weights whenever i want to learn something i \nhave to introduce a parametric form and then i learn those parameters ok now what is \nthe quantity that i am interested in i am interested in this for all the input words for the \njth input word i am interested in knowing how important it is for the t th time step there \nare several ways i could write this function i am saying that the two things that are \nimportant is one what is this jth word which is captured by hj right and what is the state of \nthe decoder up to this time step which is captured by stone \nyou could think of various other equations at this point i am fine if you by the intuition \nthat this quantity should indeed depend on these two terms it should depend on what has \nhappened in the decoder so far and what is my current word actually look like how \nmany of your fine with that please raise your hands up and high ok now also the other \nthing that i want is that across all the input words this should actually sum to one right i \njust want a weighted combination i do not want arbitrary weights it just like taking a \nprobability distribution over what which word is important by how much  so if i ha ve \nthis ejt how will i convert it to a probability distribution \nstudent softmax \nsoftmax so i will just compute the alpha j\u2019s as a softmax of e jt e j\u2019s is that fine did not \nget this ok now we have still not seen what the exact form of attention is  of the f \nattention function is \nrefer slide time tenthree \n \nso this is what the equation for the \n\uf061 jt is that we had an alpha j actually denotes the \nprobability of focusing on the j th word at the tth time step ok now we are n ow trying to \nlearn these alphas instead of an oracle telling us what these alphas are so learning is \nalways going to involve some parameters so let us define a parametric form for alphas \nand just a couple of notations \nrefer slide time tentwentyfive \n \nso from now on we would not change this we are going to refer to the decoder state as \nst and the encoder state as shj ok so these blue vectors are s s and these blue vectors are \nhs ok \ngiven these new notations one among many possible choices for f attent ion is the \nfollowing i wanted it to depend on the current decoder state i am making a dependent on \nthe current decoder state but i am also adding a parameter in front of it right i also \nwanted to make a dependent on h j i am making a dependent on that i  am also adding a \nparameter here  \nwhy do i need this parameter what is the dimension of this let us assume this is also \nwhat is the dimension of this remember after multiplying with u attention and after \nmultiplying with w attention the two vectors sho uld be addable is that fine something \ncross d what about this same thing cross d good ok so let us call that same thing as d \none what is this output then the tanh output is vector scalar matrix vector of size \nstudent refer time eleventhirtyeight \nyou said matrix or scalar it is \nask r raise to d one what is the quantity on the left hand side vector scalar matrix \nvector even though it has two indices it is a vector what is this quantity capturing at \ntime step t what is the importance of the jth input that is a i will keep asking till everyone \nreplies that is a \nstudent scalar \nscalar ok now you have scalar equ al to something multiplied by r done so why do you \nneed this something \nstudent refer time twelvesixteen \nso what is the dimension of that going to be \nstudent rdone \nrdone so that is the dot product so you see why we have these parameters ok so what \nwe have done is made it dependent on s tone and hj and also made sure that the output is a \nscalar that is what these three parameters are doing ok and these parameters will be learned \nalong with all the other parameters of the encoder and the decoder \nrefer slide time twelvethirtynine \n \nso now this is all fine you would actually someone had given me the true alpha j\u2019s and \ni had predicted this alpha j\u2019s to that  fancy equation which i just showed you and then i \nadded a dash function softmax that is the laughter safest choice in this course i want \nto learn something so what do i what should i add \nstudent loss function \nloss function what would the loss function be \nstudent refer time thirteenfour \nsay a squared error loss and then i want to adjust the parameters to minimize this \nsquared error loss then all of this makes a lot of sense right because then you can \nimagine that your u attention w attention an d v attention will get tuned in a way that \nthe predicted weights are very close to the true weights this we all understand \ngiven an objective function we understand that the weights will get adjusted so that you \nare there to the objective function but th e whole premise was that we do not have the \ntrue alphas because in the case of translation no one is going to tell you that the k th word \ncan come came from the j th word or the set of j words do you all agree with that so if \nwe had the true alphas this makes a lot of sense because then we could have added a loss \nfunction which takes the loss of alpha true with respect to alpha prediction and then an \naddition to our lost theta which was the sum of the cross entropy errors and then we \ncould have jointly m inimize this and we could have hope that the attention parameters \nwould have been learnt accordingly \nrefer slide time fourteensix \n \nin practice we will not have this in our translation example we would want someone to \nmanually annotate for every word in the output which is the set of input word from is this \nwhich it came is not going to be possible this we cannot collect so much annotated data \nso what do we do why should this model then work they does not have any \nsupervision why should this model work in the absence of such data \nhow many of you get the meaning of the question how many of you see the problem \nplease raise your hands we are not given the true alphas and that is what a problem is \nthen why should this work better this works better becaus e this is a better dash better \ndash choice language model better dash choice what is the possibility is there better \nmodeling choice why ok i give you the answer this was better because this is a \nbetter modeling choice why so so i will give you analogy and the reinforcement \nlearning fans will cringe but they can just go out \nso suppose i trying to learn a bicycle how to ride a bicycle that\u2019s why i said they will \ncringe i already see some of you can see as if you guy have a copyright on bicycles  ok \nso suppose you trying to learn a bicycle and for some reason you in your infinite \nwisdom you decide that you can learn how to ride it without holding the handle and you \nstart trying to do it its conceivable that you know few years or decades you will actually \nknow how to ride the bicycle right even if you are not holding the handle right people \ndo that and people can ride it before without that \nnow the only thing that i do is i come and tell you that instead of doing this why not \nyou try to ho ld the handle and then try to ride the bicycle right that is all i am telling \nyou i am not giving you any other supervision i have just given you a better model i \nhave said that instead of just trying to adjust the parameters with respect your feet and \nthe pedal and your back position why not you also introduce this additional parameters \nwhere you are holding the bicycle which your hands and now try to figure out what kind \nof weights you need to put on your left hand right hand and so on \ni am not giving you any supervision for that that you need to still discover on your own \nthat you will start riding it you might fall on one side you might fall on the other side \nbut you will eventually figure out what these weights need to be right because a second \nmodel where you hold the handle is a better model than the first model where you are not \nholding the hand \nin the second model you have additional parameters where you could adjust these \nparameters so that you could learn to drive better that some more natural way more \nclose to human way of learning how to ride a bicycle the same thing is happening here \nthe second model where you have a way of learning these attention on the weights even \nthough i am not all i am telling you that look maybe if you decid ed every time step \nwhich were to pay attention on you might be able to do better than feeding the \ninformation from the entire sentence at every time step \nthat is all the information that i am giving you which is very similar to saying just hold \nthe hand that is not going to teach you how to ride a bicycle right you still have to do \nthe extra work of learning these parameters but now you are given a chance you are \ngiving the model a chance to learn these parameters they are telling it that this is a be tter \nway of modeling it with this you should be able to learn better right \nrefer slide time seventeenseventeen \n \nso there is a hope of doing better because now the model is actually making a more \ninformed choice right it is a more informed way of learning how t o do translation by \nfocusing on certain words at every time step \nand now these parameters how will they get adjusted they will get adjusted because at \nevery at a given time step you produced a wrong output you did that maybe because this \nparameter was wr ong which is the v parameter or maybe because these recurrent \nconnections were wrong or maybe because your attention weights are not proper so \nnow adjust the attention weights and that should given sufficient data it should be able \nto learn which words to focus on just as humans learn how to do translation right \neven when we are doing learning how to translate or when we learn translating from one \nlanguage to another we are not given this word by word supervision right we just do a \nlot of translatio ns or read a lot of translations and somehow understand that while \ntranslating i need to focus on certain words and at every time step this is the word that i \nneed to focus on so given enough words it should be able to learn that at least someone \ngets the joke good so that is the hope and in practice indeed these models work better \nas compared to vanilla encode you do not know where the statement comes from \nrefer slide time eighteenthirtyfour \n \nso now let us what we will do is so this entire thing hints on h ope only right that is all \nthat is all i am saying but it does makes sense right because you have these additional \nparameters which you can learn and you can back propagate through them i will just \nnot stop there will actually prove what happens not pr oved by demonstrate what happens \nin practice right \nso with this attention model in mind let us look back at the encoder decoder model that \nwe had for machine translation integrate the attention mechanism with it and then let us \nsee the end to end equation that we get ok \nrefer slide time nineteenfive \n \nso this is what the diagram looks like the input and output still remains the same we \nhave just given the source sentence and the target sentence in particular you are not given \nwhich words to pay attention  to every time step right that is not given so remember \nthat my input is not changing it still the same source sentence and the target sentence \nwhat is the encoder \nnow try to work out the equations i wanted to write the equation for y t which is goin g \nto be some composite function of x where x is a vector it is a x one x two x three all the words \nin the input and somewhere along the line it also going to have this attention equation \nit is going to take a while but at least try to imagine it there is som e hints in the diagram \nitself you could take a look at it i am just asking you to convert the diagram to a set of \nequations  \nso encoder part is fine i have computed the representation of each word at time step t \nso this is a contextual representation the word because it is aware of what the \nneighboring words are right now what is the decoder going to be what is the first \nthing at time step one or a time step t in the decoder what is the first thing that i am going \nto compute the dash weights the last time step of the decoder of the encoder sorry \nwhat is the first thing that we need to compute a time step t t attention weights speak \nup please what is the first thing that you need to compute at every time step how what \nkind of a combination i t ake off the inputs or rather which are the words that i need to \nfocus on from the input who tells us this \nstudent attention weights \nthe attention weights how will you compute the attention weights using this fancy \nequation that we have seen earlier is this enough i need to convert this to a \nstudent probability distribution \nprobability distribution right that is just to ensure that everything is a neat combination \nonce i know the attention weights what do i need to feed the decoder a dash \ncombination of the inputs a weighted combination how do i take a weighted \ncombination of the inputs summation i is equal to one to capital t \nstudent \n\uf061 jt \n\uf061\njt into h j right no j t is the decoder time step  j is the input word so at the t th time \nstep of the decoder i am taking a weighted combination of all my inputs the index over \nthe inputs is going from j equal to one to t by the way did that answer your question that is \nwhat you are asking right ok is that fine ok \nnow what next now i want to produce a word at the output what is the decoder going \nto be first thing that i am going to do is decoder is a dash rnn ok what is the input \nto the rnn every time step the previous predicted word as well a s the weighted \ncombination input that you have given it does that make sense ok and then finally \nhow do i get the probability distribution is that fine yeah i think this should be a \ndistribution right l t does not make sense l t is r max of this rig ht and what is the loss \nfunction \nstudent cross entropy \ncross entropy there is no change in the loss function right loss and the algorithm \nremains the same say seen these set of equations now how many of your confident of \ngoing back and modifying all the wrong encoder decoder modules that we have covered \nin the initial part of lecture modifying them to add the attention equations in it how \nmany of you can do that please raise your hands i am not going to ask you just do it so \nthat i feel happy you can do right any questions at this point very good ok \nrefer slide time twentythreefive \n \nso you can go back and try adding attention mechanisms to all the models that we have \nseen before right see how will you compute so remember the only purpose of  ok \nwhat kind of a network is the attention network it is a single feed forward neural \nnetwork right this is just transforming a simple linear transformation of the inputs and \nin a nonlinearity on top of that and then just again one more transformation right \nit is a simple feed forward neural network only these three equation somehow need to be \nfitted in all the other models that we have seen so far right this is a very generic \nframework just as the encoder decoder framework or the very generic frame work the \nencode attend decode framework is also very generic framework you can go back and \nmodel all the applications that we saw and you can change them change them to at the \nother case ok try to answer the same set of questions what\u2019s the data what\u2019s the \nencoder what\u2019s the decoder what\u2019s the loss what\u2019s the training function \nand in particular remember that in when you go back represent all the applications that \nyou have done the data is not going to change no one is going to give us the supervis ion \nfor the alphas that is one thing which is not going to change ok \nrefer slide time twentyfourtwelve \n \nso here is one more thing so this probably tie to this question like how do we be sure \nthat the alphas actually learn something meaningful now what do  i mean by this if i \nhave to convince you that alphas are actually learning something meaningful and let us \ntake the context of machine translation what do i need to show you suppose the model \nhas generated an output for a given input sentence it has generated a translation what do \ni need to show you to convince that it is learn some kind of weights at every time step \nwhat should i show you \nstudent refer time twentyfourfortysix \nwhat does the attention weights look like right so let us see \nrefer slide time twentyfourfifty \n \nthis is a common trick or not a trick actually it is a probably a trick only but this is the \ncommon thing which is used in several papers and that is why i call it a trick because it is \na trick to get a paper accepted that you actually show w hat the attention weights actually \nlook like right so on this is the input document and this is the summary that you want \nto generate ok \nand what you see here is that at different time step so look at the last time step terrorism \nit paid maximum attention to the word terrorism in the input right so we can draw this \nmatrix suppose you had capital l time steps in the output and capital t time steps in the \ninput so you could draw this l cross t time step or t matrix which tells you what was \nthe attention paid to every input word at every output time step do you get that \nyou see what is matrix is this heat map is essentially a matrix of size l cross t and every \ncell here tells you how much attention you paid to a particular word at that time step an d \nthe darker the cell that means more the attention that you paid everyone get this ok so \nwhat this is saying is that probably see when you wanted to generate russia the \nmaximum attention was paid to russian and maybe some other words also sometimes it \ndoes not work very well but sometimes it does right  \nso for calls the maximum attention was paid to called and then similarly for front with \nthe maximum attention was paid to front and so on you see some meaningful patterns \nthat it is learning here \nand here is another example for machine translation so roughly to quickly understand \nwhat this figure is right so this is i think english to french is it french  yeah its \nfrench or french to english translation which is largely monotonic right t hat means at \nthe fourth english word you would end up paying attention to the fourth french word right \nthat means you are almost doing a word by word translation  \nand that is exactly what you see the most of the attention is along the diagonal right so \nit is learning some meaningful attention weights as always helpful if you are using if \nyou are using an attention mechanism to plot the sense see if it is actually learning any \nmeaningful attentions or attention weights or not right so that is a common t rick which \npeople use"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.3 (Part-2) Attention Mechanism (Contd.).wav", "duration": 117.3, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture \u2013 one hundred and fifteen \nattention mechanism \nrefer slide time zeroeleven \nso let us start s last lecture we are looking at encoder decoder mo dels and we saw that \na bunch of problems from different domains and different modalities images text videos \nand so on and even this cross modal or multi modal applications where you are taking \na video and trying to describe it so video is one modality  description texts is another \nmodality and so on \nwe were able to propose modals for all of these using this encoder \u2013 decoder \narchitecture and then we motivated this attention mechanism where we said that \nencoder decoder is trying to do this silly thin g where it tries to encode the entire input \nonce and that is what how humans do it he do this back and forth thing where at every \ntime step if we are trying to produce a translation or a single word in the translation we \njust focus on certain words in the input sentence and kind of ignore the other \nso the attention mechanism which is this bunch of equations that you see here that \nallowed you a neural way of modelling attention and the key thing to note here is a there \nwas a supervision for the attention  no one actually tells us that this is the portion of the \ntext which is important at time step t but they still works better because this is the better \nmodelling choice and i give you that bicycle analogy and also it is a better modelling \nchoice we are ab le to no one has given you these supervisions but you are still have \nmore parameters in the model to learn this kind of a behaviour \nrefer slide time onethirtyseven \n \nand then we also saw that we could actually visualise these attention based and from \nsome exp eriments on some papers we saw that actually learn some meaningful \nattentions in the particular case on the figure on the on the right hand side so the one \nthat clearly shows that for a monotonic kind of a translation scenario between english \nand french most of the attentions weights are along a diagram and that is exactly what \nyou would expect right \nso that is where we end it"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.4 Attention over images.wav", "duration": 650.56, "text": "deep learning \nprof sudarshan iyengar\ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and sixteen \nattention over images \nand so now in thi s lecture we will g o on to the nex t modu le whic h is talking about \nattention over images so let us first motivate why is it so different and wha t could be \ndone there right \nrefer slide time zerotwentythree \nso the question is how do we model an attention mechanism for images \nrefer slide time zerotwentyseven \n \nso in the case of text we have a representation for every location of the input sequence \nright so every location in the input sequence in the case of text was a word and then \nyou are looking at this problem of transliteration every location was a character and \nwhether it is a character or a word for everything it was discrete \nso we could just know that this is the important time step t and then we know that along \nall the inputs at different time steps you want to pay attention to certain time steps right \nso that the definition they was very straightforward right \nrefer slide time zerofiftyeight \n \nnow for images what do we do so for images we typically take the representation from \na cnn right it could be fc seven or any of the convolution layers or max pooling layers \nright now there is no concept of time step there right because the entire image is given \nto you at one go \nso now how do you decide where to pay attention to but if you think about it does \nmakes sense at least the motivation is very clear so for example for this figu re if i am \ntrying to generate the description as man throwing a frisbee in a garden or in a park or \nsomething like that right \nso when i am generating a word man i would want to focus only on the man and not \nfocus on any other part of the image simila rly when i am generating the word frisbee i \nwould like to focus on this may be when i am saying throwing i would like to focus on \nhis hand action or something like that and in a park i would like to focus on the \nbackground and so on so it does make se nse that each word in the description is \ncomplete covering from coming from a different space in the image or different position \nin the image \nbut the representation that we use say the fc seven representation that doesnot contain any \nlocation information it just a flat and vector that we had so now how do we do this \nhow do we get attention on locations is is a motivation and the problem clear and \nmotivation is straight forward the problem is that we are using fc seven representation is \njust a flat vector remember that was the fully connected vector and does not have any \nlocation based encoding \nso if for example if the fully connected vector is of size five hundred and twelve i cannot say that the first twelve \nor first twentyfour of these five hundred and twelve dimensions correspond to this set of pixels  the next twentyfour \ncorresponds to this set of pixels and so on right so that is the problem how do i what \ndo i attend to how do i decide where to attend to because that is what i am saying that \nthe vector the elements of the vector or the dimension of the vectors do not have any \nsemantic right that is not that the first dimension corresponds to first location \nwhat you want an attention on this locations in the image right but the first dimension \nin the vector fc seven vector does not correspond to any specif ic location in the image you \nknow that was a fully connected vector right so it corresponds to everything in the \nimage \nso what something simpler than that why do i say something simpler than that object \ndetection is itself is a in itself is a another  convolution neural network which does this \nand so on right and we saw this past or seen in past class seen in problems \nnow let us solve the problem at hand the problem at hand is that i want i will just \nrephrase a problem definition so that the answe r becomes obvious i want a \nrepresentation which allows to give which allows me to get some location information \nno but the fully connected layer if you back track it was fully connected by definition \nright the answer is really straight forward the pro blem only arise at the fully connected \nlayer right because that is fully connected but what about the outputs on the \nconvolution layers do they have position information  \nstudent yeah sir yes \nrefer slide time threefiftysix \n \nwe saw that suppose this is vg g whatever it is sixteen i guess and this is what i am saying \nas a problem because this was fully connected so you do not know that this dimension \ncorresponds to location one location two and so on but if you look at the convolution layers \nwe know that everything here actually goes back to some location in the image and if i \nlearn to pay attention to this guy maybe i am paying attention to some equivalent portion \nin the image does that make sense right  \nnow can you build on this intuition and tell me i want  this as a solution right so in the \ncase of words these are the word vectors that i had at every location and i was learning to \npay attention to them let us learning these alphas for each of these now what is the \ncorresponding diagram for images what is each of these box is going to be so that we \nlearn the attention weights what do you mean by pass \nstudent refer time fourfortynine \nno so maybe i am not understanding your answer what is the equivalent of this this \nbox which i have highlighted there  between the image and some attention weight so \nyou are directly going to operate on the image remember attention weights are never \ngiven to us no one will mark that man is this frisbee is this and park is this or \nwhatever there is no supervision same size as a convolution what now what is the size \nof a convolution \nlet us take the last one right so what do you mean by size you mean five hundred and twelve or you \nmean fourteen or you mean the other fourteen that is the size of the output five hundred and twelve cross fourteen cross fourteen \nso what do y ou mean by the size five hundred and twelve or fourteen or fourteen or five hundred and twelve cross fourteen cross fourteen channel \ndoes not make sense because channels capture i mean you do not want to focus on the \nred part or the green part or the blue part in some cases you might that is correct ok  \nstudent refer time fivethirtynine \nthese are all partially correct answers going in the right direction let just think a bit \nmore and see so probably between these two they gave some part of the answer now \ncan you think of it so you want a representation for the i mage first of all of us are clear \nthat we do not want to work with the raw image right that is all of us are clear with that \nthe second part is we are going to work with the convolution neural network we want \nto pick up a representation for the convoluti on neural network which gives us location \ninformation right \nand we agree that the fully connected layer does not give us the convolution layers give \nthis ok now i am asking you to focus on one of the convolution layers which is five hundred and twelve \ncross fourteen cross fourteen so let us see from there how we will be try to get these attentions ok \nso the output of the fiveth convolution layer or five c this is i think this whole thing is five and \nthis is five a five b and five c right these this is how the code or the general architecture is \nnumbered \nso this guy has fourteen cross fourteen locations right the five hundred and twelve cross fourteen cross fourteen output so it is five hundred and twelve \nchannels but the number of locations is fourteen cross fourteen and we have seen that each of these \nfourteen cross fourteen locations corresponds to certain portion in the image right \nrefer slide time sixfiftynine \n \nnow for each of these fourteen cross fourteen locations that means i have one hundred and ninetysix such locations and \nfor each of this how many dimensional representation do i have i want everyone to say \nthis \nstudent five hundred and twelve \nfive hundred and twelve because we are rea ding it from the figure no what you are taking is the one \ntaking this pixel can you see what i am highlighting i am taking it across the depth \nright so that is why five hundred and twelve dimensional representation of one pixel in your output volume \nand how many such pixels do you have \nstudent five hundred and twelve \none hundred and ninetysix and each of these pixels corresponds to some real location in your image that \nmeans it has space information right ok so now these are the one hundred and ninetysix locations that you \nhave now this looks very much similar to that diagr am that we had for words so now \nyou can think about that you have one hundred and ninetysix items in your sequence and now what will you \ndo we will learn to pay  \nstudent refer time sevenfortynine \nso what would that look like \nrefer slide time sevenfiftytwo \n \nso at every time step we will have an equation for alphas right let us try to write an \nabstract equation right first of all give me the indices of alpha what does alpha \ncompute the importance of the of the j th location at the p th time step fine is that ok \nwhat should this be a function of second part is kind of obvious so let us c all these as \nhone to hone hundred and ninetysix so these are the j\u2019s or the t\u2019s j\u2019s or the t\u2019s \nstudent j\u2019s \nj\u2019s so what would the second parameter be \nstudent h j \nh j and what is the first parameter be what is the decoder in this case we are trying to \ngenerate a caption that means we are trying to generate a sequence that means what \nwill be the decoder be \nstudent rnn \nrnn so what should i depend on st or stone \nstudent stone \nstone s t is the current thing right that we do not know yet so it will depend on st minus one \ncomma h j of course you can make it depended on several other things also but at the \nminimum you will see these two things right because you are trying to understand the \nimportance of the se guys so these better participate in the function and you are trying \nto compute the importance at current time step so you better know what has happened \ntill time step t  one it is not very different from the attention equation that we had written \nin fact it is a same actually \nand what was one form of this attention that we had seen does anyone remember that \nwe had carefully analysed the parameters and the dimensions of that form what should \nthe output of this function be scalar vector matrix scalar right \nso what is the form that we had seen for this function v t tan h of something you \nshould get comfortable in writing these equations right because that is what you will do \nif you are proposing your models and so on right so you will see ok ay in the previous \nmodel this depended on the following two quantities i think it should depend on four \nmore quantities so i will write a new equation just think about it there are two inputs s t \nminus one and h j what will be doing with each of these inputs do a introduce some \nstudent parameters \nparameters so what will you do \nstudent w stone \nw stoneplus \nstudent plus u \nv into sorry u into \nstudent h j \nplus some bias right so you just comfortable with this right i mean that is all i mean \nwhatever we have seen so first of all remember that the attention is a feed forward \nneural network the moment i tell you that you should know that it will have a linear \ntransformation followed by a non linearity right so that should have been very cle ar \nthat it would have a linear transformation followed by a nonlinearity ok \nand this is the non linearity and then you have this other constraint that you want the \noutput to be a scalar that is why you had this guy which was a vector multiplied by a \nvector which gives you a scalar everyone is comfortable with this right so we see at \nthe attention over images is not very different from attention over sequences it is more \nor less the same once you figure out what is the correct representation to you s o that you \nget the space information after that it is straightforward right ok so that ends the \nmodule"}
{"audio_filepath": "/Users/mqureshi/Documents/Speech-to-Text/preprocessed_data/preprocessed_audios/Deep Learning(CS7015)\uff1a Lec 15.5 Hierarchical Attention.wav", "duration": 1191.21, "text": "deep learning \nprof sudarshan iyengar \ndepartment of computer science and engineering \nindian institute of technology madras \nlecture  one hundred and seventeen \nhierarchical attention \nso we will g o on to the next one whic h is hierarchical attention so again something \nvery popular in nowadays become very commo n for various things so aga in not very \ndifficult idea to understand \nrefer slide time zerotwentyfour \nso let us first look at the motivation for this and then we will look at the solution so \nconsider a dialog right today everyone is interested building chatbots every second \nstart up wants to build their own chatbot and every second startup out of that they wants \nto build for the agriculture domain or the banking domain or the healthcare domain so \nhere is what a typic al dialog looks like right this is of course not for any profound \npurpose this is but you can see this is an important dialog right very relevant and very \nimportant so this is what a dialog looks like \nso let us try to break it down into the kind of entities that we deal with so can you tell \nme about a dialog what is a dialog it is a dash think in terms of things that we have \ndiscussed so far  \nstudent refer time oneseven \nsequence good right again the safest answer is sequence from now on no  it is only \nfor one lecture it is a is it a just a sequence or sequence of sequences right \nso it contains a sequence of utterances so each of these lines here is an utterance and \neach utterance in turn is a sequence of words right ok so what we hav e here is a \nsequence of sequence as input and this is very common in many many applications right \nso can you think of an encoder for such a sequence of sequence rnn of rnn\u2019s good \nthat is the answer right \nrefer slide time oneforty \n \nso we think of a tw o level hierarchical rnn encoder so first leveller will encode the \nutterances ok let me ask you few questions is there is a mistake in a diagram should \nthis be connected yes no maybe do not care ok second question i will write some \nparameters here right what is our notation w u this is u right and this is w right is it \nfine if i have a dialog which contains one hundred utterances what will happen that is a \npractical problem \nbut more conceptually what is wrong here what is each rnn trying to do  i encode a \nsentence encode an utterance so why should it be different for the first utterance \nsecond utterance third utterance and so on right all these rnn\u2019s should be the same \ndoes that make sense the u one is equal to u two is equal to u three and w one  is equal to w two is \nequal to w three ok is that fine everyone agrees with that so now can you tell me if there \nis a correction should be there or not \nconceptually what is each rnn doing encoding  \nstudent refer time twofortyseven  \none sentence right then why should it be connected to the previous sentence but then if \nyou do not know all these sentences then how will you predict the utterance the next \nresponse rather what is missing here what kind of a what was the title of this module \nso what is missing where is the hierarchy right \nrefer slide time threefour \n \nso what we will have is this right so each of these green guys is presentation for one \nutterance in your input in fact the red red guys sorry the red guys are the \nrepresentations for the utterances in your dialog and then the green guy is a sequence of \nutterances right so remember you have the sequence of sequence of words \nso the red guys are the sequence of words and the green guys are the sequence of \nutterances does that make sense  ok how many of you get this please raise your \nhands ok good so and now what would the decoder be you have a hierarchical \nencoder what could the decoder be what is the decoder have to do it has to produce a \neveryone  \nstudent sequence of words \nsequence of words so what kind of a decoder will you use just an rnn not a \nhierarchical rnn the input is hierarchical why should not the output be hierarchical \nhow it would look unbalanced right the diagram will not look very neat right if you \nrefer time threefiftyseven \ndo you need a hierarchical and decoder no right i mean just a simple decoder because \nthe decoders has to produce a sequence of words so it will take something from the \nencoder what will it take from the encoder in the normal encoder dec oder paradigm not \nthe attention based paradigm what will be the input to the decoder \nthe last dash vector your options are state that is very safe answer here the last state \nvector what will that means your options are orange blue green and red  \nstudent green \ngreen the last green vector that is what the input is going to be right so this is what is \ngoing to look like this is option one what is option two feed it to every time step that is \nwhat option two is going to be ok so that is you have yo ur hierarchical encoder decoder \nnetwork \nrefer slide time fourthirtysix \n \nwhat is missing here attention ok so let us look at another example consider the task \nof document classification or summarisation what is the difference between two in \nclassification what the what were the decoder be feed forward neural network with the \nsoftmax what would be decoder be in the case of summarisation \nstudents rnn \nrnn good what is the document sequence of sequence not sequence of sequence of \nsequence it can be a sequence of sequence of sequence also right i did not think of that \nthen it could be a sequence of sequence of sequence of sequence ok let us look at the \nnot so funny case which is sequence of sequence what is the sequence of sequence of \nit is the se quence of sentences which in turn is a sequence of words which in turn is a \nsequence of character we will not go there we will just keep it till words so it is a \nsequence of sequence so again you need some kind of a hierarchical encoder here right \nso will encode each sentence then you will treat the sentence sequence as a sequence \nencode that and then you will pass it to the classifier \nnow think of this problem right now if we want to do document classification how \nwould you go about it actual ly you want to classify whether this is a politics or sports \nor health or whatever refer time fivefiftyseven i think in terms of attention what would you \ndo actually first we will find the important words in the sentence to find the important \nwords you will have to read all the \nstudents refer time sixten \nif you want to find the important words you will have to read all the sentences so what \nwill you do first find the \nstudents word in sentences \nword in sentences and then important words within the  sentence so what kind of an \nattention mechanism do you need \nstudent hierarchy \nhierarchical right \nrefer slide time sixtwentyfive \n  \nso let us look at this so first let us look at the our data model paradigm so what is the \ndata given to you i given a  document and the class table for the document and your \nfirst thing is the word level encoder which looks like this can i will not explain what this \nis i will just expect you to know what this is why do i have two indices for h what is i \nand what is j i is the dash id sentence id and so it is the j th word or the ith sentence right \nthat is what i am encoding and how many sentences am i encoding the number of \nsentences in the document ok \nand what is the second encoder so diagram is absolutely clea r but the equations are \nnot the diagram is absolutely clear right there is no just need to map the red blue \norange green guys to the equations ok so  let me ask you this what is h ij no in the \ndiagram blue red orange \nstudent refer time seventwentythree \nwhat is w ij how did you write the rnn equation time step t is equal to rnn of t \nminus one and the input at time step t right what is the input at time step t here \nstudent word \nword right probably this was not a good choice maybe we should ma ke it xij w i think \nwe might get a confused with the weight\u2019s we should no t but they are so fine so w ij is \nactually the input word what is h i j minus one now tell me what colour is w i j it is like \nan iq test at which colour map it w i j maps to which colour \nstudents orange \norange good and h i j \nstudents blue \nblue but what about the red that is which colour i mean sorry not which variable \nstudents refer time eightfifteen \n hi ti that is the last state of every sequence right t i is a length of the ith sentence right \nok now what about hitwo the green guys right and what is this htwok  \nstudents refer time eighttwentynine  \nthe last green guy this guy ok is it fine \nrefer slide time eightthirtythree \n \nand then the decoder is just a softmax we do not need to go with that and loss and \neverything is fine so this again whatever it is we should always be comfortable and \nwriting the end to end equations from x to y right and you can write it in this case \nrefer slide time eightfortyfive \n \nnow let us make it a bit inte resting how would you model attention in such a \nhierarchical encoder decoder model how many attention functions would you need \ntwo one for attention over sentences the other for attention over  \nstudents works \nworks ok can you think of these equati ons not a very big stretch from what we have \ndone already right i mean at level one it should be straight forward at level two just ignore \nlevel one \nhow many if you can imagine the equations it is not very hard i am not joking i am i \nmean just think about i t and the level one should be straight forward because that is just \nthe same as ok so first we need to attend to the most important words in a sentence and \nthen we need to attend to the most important sentence in a document ok \nrefer slide time ninethirtyfive \n \nlet us have see how to model this so we have document again the same input then \nyou have the word level rnn ok now what be the word level attention equation look \nlike \nrefer slide time ninefortynine \n \ni am looking for the attention equation for words what  are the indices going to be j i j \nt ok what is i j t that is i have put as superscript in w this is the wor d level attention \nso at the tth time step i want the importance of the \nstudent refer time tenfour \njth good right what would that equation depend on  \nstudents the word  \nthe word should be straightforward right it should depend on oh but oh ok sorry this is \nonly for this guy right ok so you have focusing on one of these so you trying to find \nthe importance of these three words right which are there in the first sentence \nso you have computed h i j that means you have computed all the representation for \neach of these word that means you have computed the first three blue vectors that you \nsee ok and then you are computing the a ttention no so this is oh so instead of having \nit here this is how we have been writing at right \nstudent refer time tenfortynine \nok i so sorry i should have check this so read this as u i j is equal to or let me just \nexplain it so remember this i s a vector and we wanted to do this operation to make it a \nscalar right so u w is that parameter which was getting multiplied earlier so we had \nthis attention equation as u w transpose tan h of something right so now that u w has \nbeen removed from here in equation two and has been added as exponent to equation to the \nalpha equation does is that ok \nhow many of you completely confused please raise your hands how many of you \nunderstand this completely once can the sum the one ok aa shit what did i d o ok let us \njust see if i can still salvage this ok let us go one by one so what is let me just delete \nsome of these things let us try to write it on our own right so this is what we are trying \nto do \nso i will i am ignoring the sentence id rights so this is sentence one two and three so let us just \nfocus on one sentence and the same equations we hold for the other sentences also ok so \nfirst of all the attention weight would depend on what it would depend what are we \ntrying to pay attention to  \nstudent words \nwords so it should have word in the input right what else can you at have in the input \nstudent previous \nprevious state unfortunately for this problem do we have a previous state  \nstudent refer time twelvetwentyseven \nno we just doing one predict ion right there is no rnn here we just the feed forward \nnetwork do you have any s t minus one in the output that we have put here so t his was \nthe importance of the j th word at the tth time step so j belongs to the input and t belongs \nto the decoder ri ght in this case does decoder have multiple time steps there is only \none time step of the decoder right that is the problem which we have run into \nbut let us assume instead of classification we are trying to do summarisation that means \nwe are given th is document and we were trying to generate summary of this and let us \nsay the summary was the following ok so this is the summary that you are trying to \ngenerate from this document ok and now this summary has three or four time steps if you \ncount exclamation as the last time step now how is it going to be what is the decoder \ngoing to be in that case rnn right and the decoder will have some k time steps ok \nnow at every time step at a given time step t what am i trying to do  \nin next assignment try to develop a better eraser for this ok \nrefer slide time thirteenfortyone \n \nso we want to compute when you compute the attention for a word the j th word in \nparticular at that tth time step right and we have for a minute understood that we do not \nhave a feed forwa rd network at the decoder we have a recurrent neural network at the \ndecoder because we are trying to generate a summary ok we are trying to generate a \nsequence at the output so at the t th time step we are interested in understanding which \nof the docum ent was to pay attention to ok so now that is going to be a function of \nwhat and i finded a bit irritating for the want of a better word that at least one input to \nthis function should be straight forward right what is it \nstudent the word \nthe word that you are trying to learn how much attention to pay to right so that should \nbe very straightforward so that should be w one j because i am considering the first \nsentence right now ok is that fine for the first sentence i am trying to find out which \nare the words which are important what is the second input that you could put in it \nshould depend on the index t right so it should be the previous state of the decoder ok \nand then of course i will have a this is again actually not alpha but e right and then \nyou get how do you get alpha from there how do you get the alphas \nstudent softmax \nsoftmax is that fine ok so alpha will be some softmax of the e\u2019s is that ok fine so \nthis is for the word level now the equations that are written on the slide are slightly so \nthe equation has slices slightly differently written so let me just go back and write our \nown equation and so we want to write an equation for this what was our equation \nrefer slide time fifteentwentysix \n \nignore the equations on the slide it was something like this v transpose tan h of w \nstudent s t \nstone uwonej  \nstudent b \nb ok now imagine that your decoder is a feed forward neural network what will be \nmissing in that case there is no s t minus one there is no previous state o f a decoder \nbecause we just want to make a prediction once by paying attention to all the important \nwords and sentences in your document right so which part will go away wstone ok fine \nnow the other thing that you are doing is alpha was actually or ot her alpha i j is actually \nexponent of e i j divided by summation of other k sorry alpha j t e j t e k t is that fine \nit is just the softmax equation is that ok right now the only thing that you see different \nand these two equations here is first yo u do not have the w s t minus one because the \ndecoder only has one time step and second we have taken out this v transpose from here \nand instead we have added it here is that ok does that make sense this is just \ndifferent way of writing it so again you  write the attention equation for the words now \nok now what about the sentence now first of all earlier what were we using for the \ngreen guy what was the representation for the green vector it was the what was the \ngreen vector \nin the absence of atte ntion what was the green vector h t i right the last time step of \nsentence one is it fine everyone with me please raise your hands if you are with me ok \nnow what would it be it would be a dash sum of w vectors a weighted sum attention \nweighted sum r ight so that is exactly what this equation is capturing so what did you \nsaying is there is representation of sentence i is a weighted sum of the representations of \nall the words in that sentence is it ok  \nso that we will get a representation for s one s two up to s capital k all the sentences that \nyou have now what do you want to do for the second level what do we want we want \nto compute the import ance of that sentence for the t th time step right so let us call that \nbeta so i am interested in beta if  we need to really read out this i said again alpha is \nbeing used in both the places right \nso you want to find out the importance of the j th sentence at the tth time step what is \nit going to be a function of one is a sentence representation what is th e sentence \nrepresentation given by  \nstudent s j \ns j and what else the decoder state at the previous time step right does the decoder \nhave a previous time step state here no so it will just depend on s j and that is exactly \nwhat this equation is cap turing right and again the same trick that i have added this \nextra parameter to the exponent is that fine and the final representation being fed to \nthe feed forward network is a weighted sum of the sentence representations right so this \nagain has to be si\n\uf061 si ok  \ni really sorry about this but i am pretty sure that once we correct the slides and then you \ngo back and look at it should be clear right it just two sets of equation one set of \nequation sorry os sorry that is corre ct sorry so this the idea is there are two sets of \nattention mechanisms for each you will have your own set of equations the basic form \nif you can work out what the f attention would depend on the actual form would depend \nfrom would differ from paper to paper or the toolbox to toolbox that does not matter so \nmuch you just need to know that you have these as the input you are going to add some \nparameters to every input that means you are going to do linear transformation and \nthen you just need to make sure that alphas eventually turn out to be scalars right that is \nwhy you will have this additional vector getting multiplied at one point"}
